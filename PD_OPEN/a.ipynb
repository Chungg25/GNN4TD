{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ea32c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH D·ªÆ LI·ªÜU BIKE\n",
      "\n",
      "============================================================\n",
      "PH√ÇN T√çCH FILE: data/bike.npz\n",
      "============================================================\n",
      "üìÅ K√≠ch th∆∞·ªõc file: 35,174,604 bytes (33.55 MB)\n",
      "üîç S·ªë l∆∞·ª£ng arrays trong file: 11\n",
      "üìù Danh s√°ch keys: ['train_x', 'train_target', 'train_timestamp', 'val_x', 'val_target', 'val_timestamp', 'test_x', 'test_target', 'test_timestamp', 'mean', 'std']\n",
      "\n",
      "üî∏ Key: 'train_x'\n",
      "   - Shape: (3001, 250, 2, 12)\n",
      "   - Dtype: float64\n",
      "   - Size: 18,006,000 elements\n",
      "   - Memory: 144,048,000 bytes (137.37 MB)\n",
      "   - Min: -1.000000\n",
      "   - Max: 1.000000\n",
      "   - Mean: -0.937762\n",
      "   - Std: 0.092912\n",
      "   - Sample (first 3x3):\n",
      "[[[[-0.97916667 -1.         -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.        ]\n",
      "   [-1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.        ]]\n",
      "\n",
      "  [[-1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -0.97916667]\n",
      "   [-1.         -1.         -0.97183099 -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.        ]]\n",
      "\n",
      "  [[-1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.        ]\n",
      "   [-1.         -0.97183099 -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.        ]]]\n",
      "\n",
      "\n",
      " [[[-1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.        ]\n",
      "   [-1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.        ]]\n",
      "\n",
      "  [[-1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -0.97916667 -1.        ]\n",
      "   [-1.         -0.97183099 -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.        ]]\n",
      "\n",
      "  [[-1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.        ]\n",
      "   [-0.97183099 -1.         -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.        ]]]\n",
      "\n",
      "\n",
      " [[[-1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -0.97916667]\n",
      "   [-1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -0.94366197]]\n",
      "\n",
      "  [[-1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -0.97916667\n",
      "    -1.         -1.        ]\n",
      "   [-0.97183099 -1.         -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.        ]]\n",
      "\n",
      "  [[-1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.        ]\n",
      "   [-1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.         -1.         -1.         -1.\n",
      "    -1.         -1.        ]]]]\n",
      "\n",
      "üî∏ Key: 'train_target'\n",
      "   - Shape: (3001, 250, 2, 12)\n",
      "   - Dtype: int64\n",
      "   - Size: 18,006,000 elements\n",
      "   - Memory: 144,048,000 bytes (137.37 MB)\n",
      "   - Min: 0.000000\n",
      "   - Max: 96.000000\n",
      "   - Mean: 2.559994\n",
      "   - Std: 3.773492\n",
      "   - Sample (first 3x3):\n",
      "[[[[0 1 1 0 1 0 0 2 0 2 0 0]\n",
      "   [0 2 0 1 0 2 1 0 1 0 0 1]]\n",
      "\n",
      "  [[0 0 1 1 4 2 5 3 3 2 5 5]\n",
      "   [0 0 0 0 1 1 1 0 3 1 1 2]]\n",
      "\n",
      "  [[0 0 0 0 0 0 4 4 1 3 1 3]\n",
      "   [0 0 0 1 2 0 4 5 1 2 2 1]]]\n",
      "\n",
      "\n",
      " [[[1 1 0 1 0 0 2 0 2 0 0 1]\n",
      "   [2 0 1 0 2 1 0 1 0 0 1 0]]\n",
      "\n",
      "  [[0 1 1 4 2 5 3 3 2 5 5 3]\n",
      "   [0 0 0 1 1 1 0 3 1 1 2 0]]\n",
      "\n",
      "  [[0 0 0 0 0 4 4 1 3 1 3 3]\n",
      "   [0 0 1 2 0 4 5 1 2 2 1 4]]]\n",
      "\n",
      "\n",
      " [[[1 0 1 0 0 2 0 2 0 0 1 1]\n",
      "   [0 1 0 2 1 0 1 0 0 1 0 1]]\n",
      "\n",
      "  [[1 1 4 2 5 3 3 2 5 5 3 0]\n",
      "   [0 0 1 1 1 0 3 1 1 2 0 0]]\n",
      "\n",
      "  [[0 0 0 0 4 4 1 3 1 3 3 1]\n",
      "   [0 1 2 0 4 5 1 2 2 1 4 0]]]]\n",
      "\n",
      "üî∏ Key: 'train_timestamp'\n",
      "   - Shape: (3001, 1)\n",
      "   - Dtype: int64\n",
      "   - Size: 3,001 elements\n",
      "   - Memory: 24,008 bytes (0.02 MB)\n",
      "   - Min: 12.000000\n",
      "   - Max: 3012.000000\n",
      "   - Mean: 1512.000000\n",
      "   - Std: 866.314031\n",
      "   - Sample (first 3x3):\n",
      "[[12]\n",
      " [13]]\n",
      "\n",
      "üî∏ Key: 'val_x'\n",
      "   - Shape: (672, 250, 2, 12)\n",
      "   - Dtype: float64\n",
      "   - Size: 4,032,000 elements\n",
      "   - Memory: 32,256,000 bytes (30.76 MB)\n",
      "   - Min: -1.000000\n",
      "   - Max: 1.250000\n",
      "   - Mean: -0.923259\n",
      "   - Std: 0.107932\n",
      "   - Sample (first 3x3):\n",
      "[[[[-1.         -0.91666667 -1.         -0.9375     -0.95833333\n",
      "    -0.97916667 -0.97916667 -0.97916667 -0.97916667 -0.97916667\n",
      "    -0.95833333 -1.        ]\n",
      "   [-0.83098592 -0.97183099 -0.88732394 -0.94366197 -0.97183099\n",
      "    -1.         -1.         -0.91549296 -1.         -0.88732394\n",
      "    -0.88732394 -0.97183099]]\n",
      "\n",
      "  [[-0.95833333 -1.         -0.97916667 -1.         -0.9375\n",
      "    -0.97916667 -0.95833333 -0.91666667 -0.95833333 -0.91666667\n",
      "    -0.91666667 -0.89583333]\n",
      "   [-0.94366197 -0.94366197 -0.94366197 -0.94366197 -0.85915493\n",
      "    -1.         -0.88732394 -0.97183099 -0.94366197 -0.8028169\n",
      "    -0.69014085 -0.66197183]]\n",
      "\n",
      "  [[-0.91666667 -0.95833333 -0.9375     -1.         -0.9375\n",
      "    -0.97916667 -0.91666667 -0.875      -0.9375     -0.9375\n",
      "    -0.72916667 -0.85416667]\n",
      "   [-0.88732394 -0.97183099 -0.94366197 -1.         -0.91549296\n",
      "    -0.85915493 -0.85915493 -0.91549296 -0.91549296 -0.77464789\n",
      "    -0.77464789 -0.77464789]]]\n",
      "\n",
      "\n",
      " [[[-0.91666667 -1.         -0.9375     -0.95833333 -0.97916667\n",
      "    -0.97916667 -0.97916667 -0.97916667 -0.97916667 -0.95833333\n",
      "    -1.         -0.91666667]\n",
      "   [-0.97183099 -0.88732394 -0.94366197 -0.97183099 -1.\n",
      "    -1.         -0.91549296 -1.         -0.88732394 -0.88732394\n",
      "    -0.97183099 -0.77464789]]\n",
      "\n",
      "  [[-1.         -0.97916667 -1.         -0.9375     -0.97916667\n",
      "    -0.95833333 -0.91666667 -0.95833333 -0.91666667 -0.91666667\n",
      "    -0.89583333 -0.9375    ]\n",
      "   [-0.94366197 -0.94366197 -0.94366197 -0.85915493 -1.\n",
      "    -0.88732394 -0.97183099 -0.94366197 -0.8028169  -0.69014085\n",
      "    -0.66197183 -0.66197183]]\n",
      "\n",
      "  [[-0.95833333 -0.9375     -1.         -0.9375     -0.97916667\n",
      "    -0.91666667 -0.875      -0.9375     -0.9375     -0.72916667\n",
      "    -0.85416667 -0.77083333]\n",
      "   [-0.97183099 -0.94366197 -1.         -0.91549296 -0.85915493\n",
      "    -0.85915493 -0.91549296 -0.91549296 -0.77464789 -0.77464789\n",
      "    -0.77464789 -0.85915493]]]\n",
      "\n",
      "\n",
      " [[[-1.         -0.9375     -0.95833333 -0.97916667 -0.97916667\n",
      "    -0.97916667 -0.97916667 -0.97916667 -0.95833333 -1.\n",
      "    -0.91666667 -0.79166667]\n",
      "   [-0.88732394 -0.94366197 -0.97183099 -1.         -1.\n",
      "    -0.91549296 -1.         -0.88732394 -0.88732394 -0.97183099\n",
      "    -0.77464789 -0.97183099]]\n",
      "\n",
      "  [[-0.97916667 -1.         -0.9375     -0.97916667 -0.95833333\n",
      "    -0.91666667 -0.95833333 -0.91666667 -0.91666667 -0.89583333\n",
      "    -0.9375     -0.95833333]\n",
      "   [-0.94366197 -0.94366197 -0.85915493 -1.         -0.88732394\n",
      "    -0.97183099 -0.94366197 -0.8028169  -0.69014085 -0.66197183\n",
      "    -0.66197183 -0.69014085]]\n",
      "\n",
      "  [[-0.9375     -1.         -0.9375     -0.97916667 -0.91666667\n",
      "    -0.875      -0.9375     -0.9375     -0.72916667 -0.85416667\n",
      "    -0.77083333 -0.9375    ]\n",
      "   [-0.94366197 -1.         -0.91549296 -0.85915493 -0.85915493\n",
      "    -0.91549296 -0.91549296 -0.77464789 -0.77464789 -0.77464789\n",
      "    -0.85915493 -0.91549296]]]]\n",
      "\n",
      "üî∏ Key: 'val_target'\n",
      "   - Shape: (672, 250, 2, 12)\n",
      "   - Dtype: int64\n",
      "   - Size: 4,032,000 elements\n",
      "   - Memory: 32,256,000 bytes (30.76 MB)\n",
      "   - Min: 0.000000\n",
      "   - Max: 108.000000\n",
      "   - Mean: 3.122193\n",
      "   - Std: 4.346253\n",
      "   - Sample (first 3x3):\n",
      "[[[[ 4 10  1  3  1  2  0  1  0  5  2  2]\n",
      "   [ 8  1  4  4  5  4  0  0  2  1  0  0]]\n",
      "\n",
      "  [[ 3  2  5  4  0  3  2  0  0  0  1  0]\n",
      "   [12 11  5  3  2  0  0  1  0  0  4  0]]\n",
      "\n",
      "  [[11  3  4  2  1  3  0  0  1  0  0  1]\n",
      "   [ 5  3  4  1  2  2  0  1  0  0  0  2]]]\n",
      "\n",
      "\n",
      " [[[10  1  3  1  2  0  1  0  5  2  2  0]\n",
      "   [ 1  4  4  5  4  0  0  2  1  0  0  0]]\n",
      "\n",
      "  [[ 2  5  4  0  3  2  0  0  0  1  0  1]\n",
      "   [11  5  3  2  0  0  1  0  0  4  0  0]]\n",
      "\n",
      "  [[ 3  4  2  1  3  0  0  1  0  0  1  0]\n",
      "   [ 3  4  1  2  2  0  1  0  0  0  2  0]]]\n",
      "\n",
      "\n",
      " [[[ 1  3  1  2  0  1  0  5  2  2  0  0]\n",
      "   [ 4  4  5  4  0  0  2  1  0  0  0  0]]\n",
      "\n",
      "  [[ 5  4  0  3  2  0  0  0  1  0  1  0]\n",
      "   [ 5  3  2  0  0  1  0  0  4  0  0  0]]\n",
      "\n",
      "  [[ 4  2  1  3  0  0  1  0  0  1  0  0]\n",
      "   [ 4  1  2  2  0  1  0  0  0  2  0  1]]]]\n",
      "\n",
      "üî∏ Key: 'val_timestamp'\n",
      "   - Shape: (672, 1)\n",
      "   - Dtype: int64\n",
      "   - Size: 672 elements\n",
      "   - Memory: 5,376 bytes (0.01 MB)\n",
      "   - Min: 3013.000000\n",
      "   - Max: 3684.000000\n",
      "   - Mean: 3348.500000\n",
      "   - Std: 193.989476\n",
      "   - Sample (first 3x3):\n",
      "[[3013]\n",
      " [3014]]\n",
      "\n",
      "üî∏ Key: 'test_x'\n",
      "   - Shape: (672, 250, 2, 12)\n",
      "   - Dtype: float64\n",
      "   - Size: 4,032,000 elements\n",
      "   - Memory: 32,256,000 bytes (30.76 MB)\n",
      "   - Min: -1.000000\n",
      "   - Max: 1.309859\n",
      "   - Mean: -0.915911\n",
      "   - Std: 0.113205\n",
      "   - Sample (first 3x3):\n",
      "[[[[-1.         -1.         -1.         -0.9375     -0.9375\n",
      "    -1.         -0.97916667 -0.95833333 -0.97916667 -0.95833333\n",
      "    -0.9375     -0.91666667]\n",
      "   [-0.97183099 -0.97183099 -0.88732394 -0.83098592 -1.\n",
      "    -1.         -0.88732394 -0.91549296 -1.         -0.91549296\n",
      "    -0.88732394 -0.97183099]]\n",
      "\n",
      "  [[-0.97916667 -0.97916667 -0.97916667 -1.         -1.\n",
      "    -1.         -0.95833333 -0.97916667 -0.97916667 -0.95833333\n",
      "    -1.         -0.95833333]\n",
      "   [-0.97183099 -0.94366197 -1.         -0.94366197 -0.97183099\n",
      "    -0.94366197 -0.91549296 -0.97183099 -0.91549296 -0.97183099\n",
      "    -0.85915493 -0.71830986]]\n",
      "\n",
      "  [[-0.9375     -0.95833333 -0.91666667 -1.         -0.97916667\n",
      "    -0.97916667 -0.91666667 -0.91666667 -0.89583333 -0.79166667\n",
      "    -0.83333333 -0.8125    ]\n",
      "   [-0.97183099 -0.97183099 -0.88732394 -1.         -0.94366197\n",
      "    -0.88732394 -0.94366197 -0.94366197 -0.91549296 -0.8028169\n",
      "    -0.88732394 -0.88732394]]]\n",
      "\n",
      "\n",
      " [[[-1.         -1.         -0.9375     -0.9375     -1.\n",
      "    -0.97916667 -0.95833333 -0.97916667 -0.95833333 -0.9375\n",
      "    -0.91666667 -0.95833333]\n",
      "   [-0.97183099 -0.88732394 -0.83098592 -1.         -1.\n",
      "    -0.88732394 -0.91549296 -1.         -0.91549296 -0.88732394\n",
      "    -0.97183099 -0.88732394]]\n",
      "\n",
      "  [[-0.97916667 -0.97916667 -1.         -1.         -1.\n",
      "    -0.95833333 -0.97916667 -0.97916667 -0.95833333 -1.\n",
      "    -0.95833333 -0.89583333]\n",
      "   [-0.94366197 -1.         -0.94366197 -0.97183099 -0.94366197\n",
      "    -0.91549296 -0.97183099 -0.91549296 -0.97183099 -0.85915493\n",
      "    -0.71830986 -0.74647887]]\n",
      "\n",
      "  [[-0.95833333 -0.91666667 -1.         -0.97916667 -0.97916667\n",
      "    -0.91666667 -0.91666667 -0.89583333 -0.79166667 -0.83333333\n",
      "    -0.8125     -0.9375    ]\n",
      "   [-0.97183099 -0.88732394 -1.         -0.94366197 -0.88732394\n",
      "    -0.94366197 -0.94366197 -0.91549296 -0.8028169  -0.88732394\n",
      "    -0.88732394 -0.91549296]]]\n",
      "\n",
      "\n",
      " [[[-1.         -0.9375     -0.9375     -1.         -0.97916667\n",
      "    -0.95833333 -0.97916667 -0.95833333 -0.9375     -0.91666667\n",
      "    -0.95833333 -0.97916667]\n",
      "   [-0.88732394 -0.83098592 -1.         -1.         -0.88732394\n",
      "    -0.91549296 -1.         -0.91549296 -0.88732394 -0.97183099\n",
      "    -0.88732394 -0.88732394]]\n",
      "\n",
      "  [[-0.97916667 -1.         -1.         -1.         -0.95833333\n",
      "    -0.97916667 -0.97916667 -0.95833333 -1.         -0.95833333\n",
      "    -0.89583333 -0.95833333]\n",
      "   [-1.         -0.94366197 -0.97183099 -0.94366197 -0.91549296\n",
      "    -0.97183099 -0.91549296 -0.97183099 -0.85915493 -0.71830986\n",
      "    -0.74647887 -0.8028169 ]]\n",
      "\n",
      "  [[-0.91666667 -1.         -0.97916667 -0.97916667 -0.91666667\n",
      "    -0.91666667 -0.89583333 -0.79166667 -0.83333333 -0.8125\n",
      "    -0.9375     -0.91666667]\n",
      "   [-0.88732394 -1.         -0.94366197 -0.88732394 -0.94366197\n",
      "    -0.94366197 -0.91549296 -0.8028169  -0.88732394 -0.88732394\n",
      "    -0.91549296 -0.88732394]]]]\n",
      "\n",
      "üî∏ Key: 'test_target'\n",
      "   - Shape: (672, 250, 2, 12)\n",
      "   - Dtype: int64\n",
      "   - Size: 4,032,000 elements\n",
      "   - Memory: 32,256,000 bytes (30.76 MB)\n",
      "   - Min: 0.000000\n",
      "   - Max: 99.000000\n",
      "   - Mean: 3.440572\n",
      "   - Std: 4.581695\n",
      "   - Sample (first 3x3):\n",
      "[[[[2 1 3 3 3 2 2 4 5 4 3 2]\n",
      "   [4 4 3 7 3 2 5 0 0 0 2 1]]\n",
      "\n",
      "  [[5 2 0 2 1 0 0 1 0 2 0 0]\n",
      "   [9 7 6 2 4 0 0 0 1 0 0 0]]\n",
      "\n",
      "  [[3 4 5 1 0 0 1 1 0 0 0 1]\n",
      "   [3 4 5 0 0 0 1 1 0 1 0 0]]]\n",
      "\n",
      "\n",
      " [[[1 3 3 3 2 2 4 5 4 3 2 2]\n",
      "   [4 3 7 3 2 5 0 0 0 2 1 1]]\n",
      "\n",
      "  [[2 0 2 1 0 0 1 0 2 0 0 0]\n",
      "   [7 6 2 4 0 0 0 1 0 0 0 1]]\n",
      "\n",
      "  [[4 5 1 0 0 1 1 0 0 0 1 0]\n",
      "   [4 5 0 0 0 1 1 0 1 0 0 0]]]\n",
      "\n",
      "\n",
      " [[[3 3 3 2 2 4 5 4 3 2 2 1]\n",
      "   [3 7 3 2 5 0 0 0 2 1 1 1]]\n",
      "\n",
      "  [[0 2 1 0 0 1 0 2 0 0 0 0]\n",
      "   [6 2 4 0 0 0 1 0 0 0 1 0]]\n",
      "\n",
      "  [[5 1 0 0 1 1 0 0 0 1 0 0]\n",
      "   [5 0 0 0 1 1 0 1 0 0 0 0]]]]\n",
      "\n",
      "üî∏ Key: 'test_timestamp'\n",
      "   - Shape: (672, 1)\n",
      "   - Dtype: int64\n",
      "   - Size: 672 elements\n",
      "   - Memory: 5,376 bytes (0.01 MB)\n",
      "   - Min: 3685.000000\n",
      "   - Max: 4356.000000\n",
      "   - Mean: 4020.500000\n",
      "   - Std: 193.989476\n",
      "   - Sample (first 3x3):\n",
      "[[3685]\n",
      " [3686]]\n",
      "\n",
      "üî∏ Key: 'mean'\n",
      "   - Shape: (1, 1, 2, 1)\n",
      "   - Dtype: int64\n",
      "   - Size: 2 elements\n",
      "   - Memory: 16 bytes (0.00 MB)\n",
      "   - Min: 71.000000\n",
      "   - Max: 96.000000\n",
      "   - Mean: 83.500000\n",
      "   - Std: 12.500000\n",
      "   - Sample data:\n",
      "[[[[96]\n",
      "   [71]]]]\n",
      "\n",
      "üî∏ Key: 'std'\n",
      "   - Shape: (1, 1, 2, 1)\n",
      "   - Dtype: int64\n",
      "   - Size: 2 elements\n",
      "   - Memory: 16 bytes (0.00 MB)\n",
      "   - Min: 0.000000\n",
      "   - Max: 0.000000\n",
      "   - Mean: 0.000000\n",
      "   - Std: 0.000000\n",
      "   - Sample data:\n",
      "[[[[0]\n",
      "   [0]]]]\n",
      "\n",
      "üíæ T·ªïng b·ªô nh·ªõ c√°c arrays: 417,154,792 bytes (397.83 MB)\n",
      "\n",
      "============================================================\n",
      "PH√ÇN T√çCH FILE: data/bike_svd.npy\n",
      "============================================================\n",
      "üìÅ K√≠ch th∆∞·ªõc file: 500,128 bytes (0.48 MB)\n",
      "üî∏ Array info:\n",
      "   - Shape: (250, 250)\n",
      "   - Dtype: float64\n",
      "   - Size: 62,500 elements\n",
      "   - Memory: 500,000 bytes (0.48 MB)\n",
      "   - Min: 0.000000\n",
      "   - Max: 0.004293\n",
      "   - Mean: 0.004000\n",
      "   - Std: 0.000267\n",
      "   - Sample (first 5x5):\n",
      "[[0.         0.00411958 0.00412224 0.00413687 0.00411691]\n",
      " [0.00411917 0.         0.00409488 0.00410136 0.00408075]\n",
      " [0.00410511 0.00407828 0.         0.00413638 0.00414056]\n",
      " [0.00412088 0.00408593 0.00413758 0.         0.00414068]\n",
      " [0.00410377 0.00406814 0.00414456 0.00414347 0.        ]]\n",
      "\n",
      "üîç PH√ÇN T√çCH ADJACENCY MATRIX:\n",
      "   - L√† ma tr·∫≠n vu√¥ng: 250x250\n",
      "   - S·ªë edges (non-zero): 62,250\n",
      "   - Density: 99.60%\n",
      "   - Symmetric: False\n",
      "   - Diagonal sum: 0.000000\n",
      "\n",
      "============================================================\n",
      "SO S√ÅNH V√Ä PH√ÇN T√çCH M·ªêI LI√äN H·ªÜ\n",
      "============================================================\n",
      "üîç T√¨m m·ªëi li√™n h·ªá gi·ªØa 2 file:\n",
      "   - train_x shape: (3001, 250, 2, 12)\n",
      "     ‚úÖ C√≥ li√™n h·ªá v·ªÅ s·ªë nodes v·ªõi adjacency matrix\n",
      "   - train_target shape: (3001, 250, 2, 12)\n",
      "     ‚úÖ C√≥ li√™n h·ªá v·ªÅ s·ªë nodes v·ªõi adjacency matrix\n",
      "   - train_timestamp shape: (3001, 1)\n",
      "   - val_x shape: (672, 250, 2, 12)\n",
      "     ‚úÖ C√≥ li√™n h·ªá v·ªÅ s·ªë nodes v·ªõi adjacency matrix\n",
      "   - val_target shape: (672, 250, 2, 12)\n",
      "     ‚úÖ C√≥ li√™n h·ªá v·ªÅ s·ªë nodes v·ªõi adjacency matrix\n",
      "   - val_timestamp shape: (672, 1)\n",
      "   - test_x shape: (672, 250, 2, 12)\n",
      "     ‚úÖ C√≥ li√™n h·ªá v·ªÅ s·ªë nodes v·ªõi adjacency matrix\n",
      "   - test_target shape: (672, 250, 2, 12)\n",
      "     ‚úÖ C√≥ li√™n h·ªá v·ªÅ s·ªë nodes v·ªõi adjacency matrix\n",
      "   - test_timestamp shape: (672, 1)\n",
      "   - mean shape: (1, 1, 2, 1)\n",
      "   - std shape: (1, 1, 2, 1)\n",
      "   - bike_svd.npy shape: (250, 250)\n",
      "   - Adjacency matrix cho 250 nodes\n",
      "\n",
      "============================================================\n",
      "‚úÖ HO√ÄN TH√ÄNH PH√ÇN T√çCH\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def analyze_npz_file(file_path):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch file .npz (compressed numpy arrays)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PH√ÇN T√çCH FILE: {file_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå File kh√¥ng t·ªìn t·∫°i: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    # Ki·ªÉm tra k√≠ch th∆∞·ªõc file\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"üìÅ K√≠ch th∆∞·ªõc file: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)\")\n",
    "    \n",
    "    try:\n",
    "        # ƒê·ªçc file .npz\n",
    "        with np.load(file_path) as data:\n",
    "            print(f\"üîç S·ªë l∆∞·ª£ng arrays trong file: {len(data.files)}\")\n",
    "            print(f\"üìù Danh s√°ch keys: {list(data.files)}\")\n",
    "            \n",
    "            total_memory = 0\n",
    "            \n",
    "            for key in data.files:\n",
    "                array = data[key]\n",
    "                array_size = array.nbytes\n",
    "                total_memory += array_size\n",
    "                \n",
    "                print(f\"\\nüî∏ Key: '{key}'\")\n",
    "                print(f\"   - Shape: {array.shape}\")\n",
    "                print(f\"   - Dtype: {array.dtype}\")\n",
    "                print(f\"   - Size: {array.size:,} elements\")\n",
    "                print(f\"   - Memory: {array_size:,} bytes ({array_size/1024/1024:.2f} MB)\")\n",
    "                \n",
    "                # Hi·ªÉn th·ªã m·ªôt s·ªë th·ªëng k√™ c∆° b·∫£n\n",
    "                if array.size > 0:\n",
    "                    print(f\"   - Min: {array.min():.6f}\")\n",
    "                    print(f\"   - Max: {array.max():.6f}\")\n",
    "                    print(f\"   - Mean: {array.mean():.6f}\")\n",
    "                    print(f\"   - Std: {array.std():.6f}\")\n",
    "                \n",
    "                # Hi·ªÉn th·ªã m·∫´u d·ªØ li·ªáu (n·∫øu kh√¥ng qu√° l·ªõn)\n",
    "                if array.size <= 100:\n",
    "                    print(f\"   - Sample data:\\n{array}\")\n",
    "                elif len(array.shape) >= 2:\n",
    "                    print(f\"   - Sample (first 3x3):\\n{array[:3, :3] if array.shape[0] >= 3 and array.shape[1] >= 3 else array[:2, :2]}\")\n",
    "                else:\n",
    "                    print(f\"   - Sample (first 10): {array.flat[:10]}\")\n",
    "            \n",
    "            print(f\"\\nüíæ T·ªïng b·ªô nh·ªõ c√°c arrays: {total_memory:,} bytes ({total_memory/1024/1024:.2f} MB)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi ƒë·ªçc file: {e}\")\n",
    "\n",
    "def analyze_npy_file(file_path):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch file .npy (single numpy array)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PH√ÇN T√çCH FILE: {file_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå File kh√¥ng t·ªìn t·∫°i: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    # Ki·ªÉm tra k√≠ch th∆∞·ªõc file\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"üìÅ K√≠ch th∆∞·ªõc file: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)\")\n",
    "    \n",
    "    try:\n",
    "        # ƒê·ªçc file .npy\n",
    "        array = np.load(file_path)\n",
    "        \n",
    "        print(f\"üî∏ Array info:\")\n",
    "        print(f\"   - Shape: {array.shape}\")\n",
    "        print(f\"   - Dtype: {array.dtype}\")\n",
    "        print(f\"   - Size: {array.size:,} elements\")\n",
    "        print(f\"   - Memory: {array.nbytes:,} bytes ({array.nbytes/1024/1024:.2f} MB)\")\n",
    "        \n",
    "        # Th·ªëng k√™ c∆° b·∫£n\n",
    "        if array.size > 0:\n",
    "            print(f\"   - Min: {array.min():.6f}\")\n",
    "            print(f\"   - Max: {array.max():.6f}\")\n",
    "            print(f\"   - Mean: {array.mean():.6f}\")\n",
    "            print(f\"   - Std: {array.std():.6f}\")\n",
    "        \n",
    "        # Hi·ªÉn th·ªã m·∫´u d·ªØ li·ªáu\n",
    "        if array.size <= 100:\n",
    "            print(f\"   - Full data:\\n{array}\")\n",
    "        elif len(array.shape) >= 2:\n",
    "            print(f\"   - Sample (first 5x5):\")\n",
    "            print(array[:5, :5] if array.shape[0] >= 5 and array.shape[1] >= 5 else array)\n",
    "        else:\n",
    "            print(f\"   - Sample (first 20): {array.flat[:20]}\")\n",
    "            \n",
    "        # Ph√¢n t√≠ch ƒë·∫∑c bi·ªát cho adjacency matrix\n",
    "        if len(array.shape) == 2 and array.shape[0] == array.shape[1]:\n",
    "            print(f\"\\nüîç PH√ÇN T√çCH ADJACENCY MATRIX:\")\n",
    "            print(f\"   - L√† ma tr·∫≠n vu√¥ng: {array.shape[0]}x{array.shape[1]}\")\n",
    "            print(f\"   - S·ªë edges (non-zero): {np.count_nonzero(array):,}\")\n",
    "            print(f\"   - Density: {np.count_nonzero(array) / array.size * 100:.2f}%\")\n",
    "            print(f\"   - Symmetric: {np.allclose(array, array.T)}\")\n",
    "            print(f\"   - Diagonal sum: {np.trace(array):.6f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi ƒë·ªçc file: {e}\")\n",
    "\n",
    "def compare_files():\n",
    "    \"\"\"\n",
    "    So s√°nh 2 file v√† t√¨m m·ªëi li√™n h·ªá\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SO S√ÅNH V√Ä PH√ÇN T√çCH M·ªêI LI√äN H·ªÜ\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    npz_path = \"data/bike.npz\"\n",
    "    npy_path = \"data/bike_svd.npy\"\n",
    "    \n",
    "    if os.path.exists(npz_path) and os.path.exists(npy_path):\n",
    "        try:\n",
    "            # ƒê·ªçc c·∫£ 2 file\n",
    "            npz_data = np.load(npz_path)\n",
    "            npy_data = np.load(npy_path)\n",
    "            \n",
    "            print(f\"üîç T√¨m m·ªëi li√™n h·ªá gi·ªØa 2 file:\")\n",
    "            \n",
    "            # So s√°nh k√≠ch th∆∞·ªõc\n",
    "            for key in npz_data.files:\n",
    "                array = npz_data[key]\n",
    "                if len(array.shape) >= 2:\n",
    "                    print(f\"   - {key} shape: {array.shape}\")\n",
    "                    if array.shape[0] == npy_data.shape[0] or array.shape[1] == npy_data.shape[0]:\n",
    "                        print(f\"     ‚úÖ C√≥ li√™n h·ªá v·ªÅ s·ªë nodes v·ªõi adjacency matrix\")\n",
    "            \n",
    "            print(f\"   - bike_svd.npy shape: {npy_data.shape}\")\n",
    "            print(f\"   - Adjacency matrix cho {npy_data.shape[0]} nodes\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói khi so s√°nh: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå M·ªôt ho·∫∑c c·∫£ 2 file kh√¥ng t·ªìn t·∫°i\")\n",
    "\n",
    "# Ch·∫°y ph√¢n t√≠ch\n",
    "print(\"üöÄ B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH D·ªÆ LI·ªÜU BIKE\")\n",
    "\n",
    "# Ph√¢n t√≠ch file bike.npz\n",
    "analyze_npz_file(\"data/bike.npz\")\n",
    "\n",
    "# Ph√¢n t√≠ch file bike_svd.npy  \n",
    "analyze_npy_file(\"data/bike_svd.npy\")\n",
    "\n",
    "# So s√°nh 2 file\n",
    "compare_files()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ HO√ÄN TH√ÄNH PH√ÇN T√çCH\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b4a52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä bike_pick shape: (4368, 266)\n",
      "üìä bike_drop shape: (4368, 266)\n",
      "‚úÖ Combined data shape: (4368, 266, 2)\n",
      "   - Time steps: 4368\n",
      "   - Stations: 266\n",
      "   - Features: 2 (pick_up, drop_off)\n",
      "‚úÖ Created bike_raw.npz with shape: (4368, 266, 2)\n"
     ]
    }
   ],
   "source": [
    "def create_bike_raw_npz():\n",
    "    with h5py.File('data/nogrid/bike_data.h5', 'r') as f:\n",
    "        # ƒê·ªçc pick v√† drop ri√™ng bi·ªát\n",
    "        bike_pick = f['bike_pick'][:]  # Shape: (T, N)\n",
    "        bike_drop = f['bike_drop'][:]  # Shape: (T, N)\n",
    "        \n",
    "        print(f\"üìä bike_pick shape: {bike_pick.shape}\")\n",
    "        print(f\"üìä bike_drop shape: {bike_drop.shape}\")\n",
    "        \n",
    "        # K·∫øt h·ª£p th√†nh tensor 3D: (T, N, F)\n",
    "        # F=2: [pick_ups, drop_offs]\n",
    "        raw_data = np.stack([bike_pick, bike_drop], axis=-1)  # Shape: (T, N, 2)\n",
    "        \n",
    "        print(f\"‚úÖ Combined data shape: {raw_data.shape}\")\n",
    "        print(f\"   - Time steps: {raw_data.shape[0]}\")\n",
    "        print(f\"   - Stations: {raw_data.shape[1]}\")\n",
    "        print(f\"   - Features: {raw_data.shape[2]} (pick_up, drop_off)\")\n",
    "        \n",
    "    # L∆∞u th√†nh npz format\n",
    "    np.savez_compressed('bike_raw.npz', data=raw_data)\n",
    "    print(f\"‚úÖ Created bike_raw.npz with shape: {raw_data.shape}\")\n",
    "    \n",
    "    return raw_data\n",
    "\n",
    "# G·ªçi h√†m\n",
    "raw_data = create_bike_raw_npz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "74f0dd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ QUICK CHECK:\n",
      "   Keys match: True\n",
      "   train_x shapes match: True\n",
      "   train_x data identical: True\n",
      "   Timestamps identical: True\n"
     ]
    }
   ],
   "source": [
    "def quick_check():\n",
    "    file1 = 'taxi_raw_r1_d0_w0.npz'\n",
    "    file2 = 'data/taxi.npz'\n",
    "    \n",
    "    try:\n",
    "        data1 = np.load(file1)\n",
    "        data2 = np.load(file2)\n",
    "        \n",
    "        print(\"üöÄ QUICK CHECK:\")\n",
    "        print(f\"   Keys match: {set(data1.files) == set(data2.files)}\")\n",
    "        \n",
    "        if 'train_x' in data1.files and 'train_x' in data2.files:\n",
    "            shapes_match = data1['train_x'].shape == data2['train_x'].shape\n",
    "            print(f\"   train_x shapes match: {shapes_match}\")\n",
    "            \n",
    "            if shapes_match:\n",
    "                data_identical = np.allclose(data1['train_x'], data2['train_x'])\n",
    "                print(f\"   train_x data identical: {data_identical}\")\n",
    "        \n",
    "        if 'train_timestamp' in data1.files and 'train_timestamp' in data2.files:\n",
    "            ts_identical = np.array_equal(data1['train_timestamp'], data2['train_timestamp'])\n",
    "            print(f\"   Timestamps identical: {ts_identical}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "quick_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c94e38b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç KI·ªÇM TRA CHI TI·∫æT V√Ä SO S√ÅNH FILES\n",
      "============================================================\n",
      "üìÇ Loading taxi_raw_r1_d0_w0.npz...\n",
      "üìÇ Loading data/taxi.npz...\n",
      "\n",
      "üìã KEYS COMPARISON:\n",
      "   taxi_raw_r1_d0_w0.npz keys: ['mean', 'std', 'test_target', 'test_timestamp', 'test_x', 'train_target', 'train_timestamp', 'train_x', 'val_target', 'val_timestamp', 'val_x']\n",
      "   data/taxi.npz keys: ['mean', 'std', 'test_target', 'test_timestamp', 'test_x', 'train_target', 'train_timestamp', 'train_x', 'val_target', 'val_timestamp', 'val_x']\n",
      "   ‚úÖ Common keys: ['mean', 'std', 'test_target', 'test_timestamp', 'test_x', 'train_target', 'train_timestamp', 'train_x', 'val_target', 'val_timestamp', 'val_x']\n",
      "\n",
      "üìä DETAILED COMPARISON FOR ALL KEYS:\n",
      "============================================================\n",
      "\n",
      "üîç KEY: mean\n",
      "----------------------------------------\n",
      "   üìê Shapes:\n",
      "      taxi_raw_r1_d0_w0.npz: (1, 1, 2, 1)\n",
      "      data/taxi.npz: (1, 1, 2, 1)\n",
      "      Match: ‚úÖ\n",
      "   üî§ Data types:\n",
      "      taxi_raw_r1_d0_w0.npz: int64\n",
      "      data/taxi.npz: int64\n",
      "      Match: ‚úÖ\n",
      "   üìä Data: ‚úÖ IDENTICAL\n",
      "   üìà Statistics:\n",
      "      taxi_raw_r1_d0_w0.npz: min=669.000000, max=844.000000, mean=756.500000\n",
      "      data/taxi.npz: min=669.000000, max=844.000000, mean=756.500000\n",
      "\n",
      "üîç KEY: std\n",
      "----------------------------------------\n",
      "   üìê Shapes:\n",
      "      taxi_raw_r1_d0_w0.npz: (1, 1, 2, 1)\n",
      "      data/taxi.npz: (1, 1, 2, 1)\n",
      "      Match: ‚úÖ\n",
      "   üî§ Data types:\n",
      "      taxi_raw_r1_d0_w0.npz: int64\n",
      "      data/taxi.npz: int64\n",
      "      Match: ‚úÖ\n",
      "   üìä Data: ‚úÖ IDENTICAL\n",
      "   üìà Statistics:\n",
      "      taxi_raw_r1_d0_w0.npz: min=0.000000, max=0.000000, mean=0.000000\n",
      "      data/taxi.npz: min=0.000000, max=0.000000, mean=0.000000\n",
      "\n",
      "üîç KEY: test_target\n",
      "----------------------------------------\n",
      "   üìê Shapes:\n",
      "      taxi_raw_r1_d0_w0.npz: (672, 266, 2, 12)\n",
      "      data/taxi.npz: (672, 266, 2, 12)\n",
      "      Match: ‚úÖ\n",
      "   üî§ Data types:\n",
      "      taxi_raw_r1_d0_w0.npz: int64\n",
      "      data/taxi.npz: int64\n",
      "      Match: ‚úÖ\n",
      "   üìä Data: ‚úÖ IDENTICAL\n",
      "   üìà Statistics:\n",
      "      taxi_raw_r1_d0_w0.npz: min=0.000000, max=783.000000, mean=24.983816\n",
      "      data/taxi.npz: min=0.000000, max=783.000000, mean=24.983816\n",
      "\n",
      "üîç KEY: test_timestamp\n",
      "----------------------------------------\n",
      "   üìê Shapes:\n",
      "      taxi_raw_r1_d0_w0.npz: (672, 1)\n",
      "      data/taxi.npz: (672, 1)\n",
      "      Match: ‚úÖ\n",
      "   üî§ Data types:\n",
      "      taxi_raw_r1_d0_w0.npz: int64\n",
      "      data/taxi.npz: int64\n",
      "      Match: ‚úÖ\n",
      "   üìä Data: ‚úÖ IDENTICAL\n",
      "   üìà Statistics:\n",
      "      taxi_raw_r1_d0_w0.npz: min=3685.000000, max=4356.000000, mean=4020.500000\n",
      "      data/taxi.npz: min=3685.000000, max=4356.000000, mean=4020.500000\n",
      "\n",
      "üîç KEY: test_x\n",
      "----------------------------------------\n",
      "   üìê Shapes:\n",
      "      taxi_raw_r1_d0_w0.npz: (672, 266, 2, 12)\n",
      "      data/taxi.npz: (672, 266, 2, 12)\n",
      "      Match: ‚úÖ\n",
      "   üî§ Data types:\n",
      "      taxi_raw_r1_d0_w0.npz: float64\n",
      "      data/taxi.npz: float64\n",
      "      Match: ‚úÖ\n",
      "   üìä Data: ‚úÖ IDENTICAL\n",
      "   üìà Statistics:\n",
      "      taxi_raw_r1_d0_w0.npz: min=-1.000000, max=0.855450, mean=-0.932872\n",
      "      data/taxi.npz: min=-1.000000, max=0.855450, mean=-0.932872\n",
      "\n",
      "üîç KEY: train_target\n",
      "----------------------------------------\n",
      "   üìê Shapes:\n",
      "      taxi_raw_r1_d0_w0.npz: (3001, 266, 2, 12)\n",
      "      data/taxi.npz: (3001, 266, 2, 12)\n",
      "      Match: ‚úÖ\n",
      "   üî§ Data types:\n",
      "      taxi_raw_r1_d0_w0.npz: int64\n",
      "      data/taxi.npz: int64\n",
      "      Match: ‚úÖ\n",
      "   üìä Data: ‚úÖ IDENTICAL\n",
      "   üìà Statistics:\n",
      "      taxi_raw_r1_d0_w0.npz: min=0.000000, max=844.000000, mean=26.918123\n",
      "      data/taxi.npz: min=0.000000, max=844.000000, mean=26.918123\n",
      "\n",
      "üîç KEY: train_timestamp\n",
      "----------------------------------------\n",
      "   üìê Shapes:\n",
      "      taxi_raw_r1_d0_w0.npz: (3001, 1)\n",
      "      data/taxi.npz: (3001, 1)\n",
      "      Match: ‚úÖ\n",
      "   üî§ Data types:\n",
      "      taxi_raw_r1_d0_w0.npz: int64\n",
      "      data/taxi.npz: int64\n",
      "      Match: ‚úÖ\n",
      "   üìä Data: ‚úÖ IDENTICAL\n",
      "   üìà Statistics:\n",
      "      taxi_raw_r1_d0_w0.npz: min=12.000000, max=3012.000000, mean=1512.000000\n",
      "      data/taxi.npz: min=12.000000, max=3012.000000, mean=1512.000000\n",
      "\n",
      "üîç KEY: train_x\n",
      "----------------------------------------\n",
      "   üìê Shapes:\n",
      "      taxi_raw_r1_d0_w0.npz: (3001, 266, 2, 12)\n",
      "      data/taxi.npz: (3001, 266, 2, 12)\n",
      "      Match: ‚úÖ\n",
      "   üî§ Data types:\n",
      "      taxi_raw_r1_d0_w0.npz: float64\n",
      "      data/taxi.npz: float64\n",
      "      Match: ‚úÖ\n",
      "   üìä Data: ‚úÖ IDENTICAL\n",
      "   üìà Statistics:\n",
      "      taxi_raw_r1_d0_w0.npz: min=-1.000000, max=1.000000, mean=-0.928083\n",
      "      data/taxi.npz: min=-1.000000, max=1.000000, mean=-0.928083\n",
      "\n",
      "üîç KEY: val_target\n",
      "----------------------------------------\n",
      "   üìê Shapes:\n",
      "      taxi_raw_r1_d0_w0.npz: (672, 266, 2, 12)\n",
      "      data/taxi.npz: (672, 266, 2, 12)\n",
      "      Match: ‚úÖ\n",
      "   üî§ Data types:\n",
      "      taxi_raw_r1_d0_w0.npz: int64\n",
      "      data/taxi.npz: int64\n",
      "      Match: ‚úÖ\n",
      "   üìä Data: ‚úÖ IDENTICAL\n",
      "   üìà Statistics:\n",
      "      taxi_raw_r1_d0_w0.npz: min=0.000000, max=818.000000, mean=26.039870\n",
      "      data/taxi.npz: min=0.000000, max=818.000000, mean=26.039870\n",
      "\n",
      "üîç KEY: val_timestamp\n",
      "----------------------------------------\n",
      "   üìê Shapes:\n",
      "      taxi_raw_r1_d0_w0.npz: (672, 1)\n",
      "      data/taxi.npz: (672, 1)\n",
      "      Match: ‚úÖ\n",
      "   üî§ Data types:\n",
      "      taxi_raw_r1_d0_w0.npz: int64\n",
      "      data/taxi.npz: int64\n",
      "      Match: ‚úÖ\n",
      "   üìä Data: ‚úÖ IDENTICAL\n",
      "   üìà Statistics:\n",
      "      taxi_raw_r1_d0_w0.npz: min=3013.000000, max=3684.000000, mean=3348.500000\n",
      "      data/taxi.npz: min=3013.000000, max=3684.000000, mean=3348.500000\n",
      "\n",
      "üîç KEY: val_x\n",
      "----------------------------------------\n",
      "   üìê Shapes:\n",
      "      taxi_raw_r1_d0_w0.npz: (672, 266, 2, 12)\n",
      "      data/taxi.npz: (672, 266, 2, 12)\n",
      "      Match: ‚úÖ\n",
      "   üî§ Data types:\n",
      "      taxi_raw_r1_d0_w0.npz: float64\n",
      "      data/taxi.npz: float64\n",
      "      Match: ‚úÖ\n",
      "   üìä Data: ‚úÖ IDENTICAL\n",
      "   üìà Statistics:\n",
      "      taxi_raw_r1_d0_w0.npz: min=-1.000000, max=0.938389, mean=-0.930164\n",
      "      data/taxi.npz: min=-1.000000, max=0.938389, mean=-0.930164\n",
      "\n",
      "============================================================\n",
      "üìù FINAL SUMMARY:\n",
      "   Keys match: ‚úÖ\n",
      "   All shapes match: ‚úÖ\n",
      "   All data identical: ‚úÖ\n",
      "   File sizes: taxi_raw_r1_d0_w0.npz (65.80MB) vs data/taxi.npz (65.80MB)\n",
      "\n",
      "üéâ CONCLUSION: Files are IDENTICAL!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def check_and_compare_files_detailed():\n",
    "    file1 = 'taxi_raw_r1_d0_w0.npz'\n",
    "    file2 = 'data/taxi.npz'\n",
    "    \n",
    "    print(\"üîç KI·ªÇM TRA CHI TI·∫æT V√Ä SO S√ÅNH FILES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Ki·ªÉm tra file t·ªìn t·∫°i\n",
    "    if not os.path.exists(file1):\n",
    "        print(f\"‚ùå File kh√¥ng t·ªìn t·∫°i: {file1}\")\n",
    "        return\n",
    "    if not os.path.exists(file2):\n",
    "        print(f\"‚ùå File kh√¥ng t·ªìn t·∫°i: {file2}\")\n",
    "        return\n",
    "    \n",
    "    # Load files\n",
    "    print(f\"üìÇ Loading {file1}...\")\n",
    "    data1 = np.load(file1)\n",
    "    \n",
    "    print(f\"üìÇ Loading {file2}...\")\n",
    "    data2 = np.load(file2)\n",
    "    \n",
    "    # So s√°nh keys\n",
    "    print(f\"\\nüìã KEYS COMPARISON:\")\n",
    "    keys1 = set(data1.files)\n",
    "    keys2 = set(data2.files)\n",
    "    \n",
    "    print(f\"   {file1} keys: {sorted(keys1)}\")\n",
    "    print(f\"   {file2} keys: {sorted(keys2)}\")\n",
    "    \n",
    "    common_keys = keys1.intersection(keys2)\n",
    "    only_in_1 = keys1 - keys2\n",
    "    only_in_2 = keys2 - keys1\n",
    "    \n",
    "    if common_keys:\n",
    "        print(f\"   ‚úÖ Common keys: {sorted(common_keys)}\")\n",
    "    if only_in_1:\n",
    "        print(f\"   ‚ö†Ô∏è Only in {file1}: {sorted(only_in_1)}\")\n",
    "    if only_in_2:\n",
    "        print(f\"   ‚ö†Ô∏è Only in {file2}: {sorted(only_in_2)}\")\n",
    "    \n",
    "    # So s√°nh shapes v√† data cho T·∫§T C·∫¢ common keys\n",
    "    print(f\"\\nüìä DETAILED COMPARISON FOR ALL KEYS:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for key in sorted(common_keys):\n",
    "        print(f\"\\nüîç KEY: {key}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        arr1 = data1[key]\n",
    "        arr2 = data2[key]\n",
    "        \n",
    "        # So s√°nh shapes\n",
    "        shape_match = arr1.shape == arr2.shape\n",
    "        print(f\"   üìê Shapes:\")\n",
    "        print(f\"      {file1}: {arr1.shape}\")\n",
    "        print(f\"      {file2}: {arr2.shape}\")\n",
    "        print(f\"      Match: {'‚úÖ' if shape_match else '‚ùå'}\")\n",
    "        \n",
    "        # So s√°nh data types\n",
    "        dtype_match = arr1.dtype == arr2.dtype\n",
    "        print(f\"   üî§ Data types:\")\n",
    "        print(f\"      {file1}: {arr1.dtype}\")\n",
    "        print(f\"      {file2}: {arr2.dtype}\")\n",
    "        print(f\"      Match: {'‚úÖ' if dtype_match else '‚ùå'}\")\n",
    "        \n",
    "        # So s√°nh data n·∫øu shapes gi·ªëng nhau\n",
    "        if shape_match:\n",
    "            if arr1.size > 0 and arr2.size > 0:\n",
    "                try:\n",
    "                    # Ki·ªÉm tra identical\n",
    "                    is_identical = np.array_equal(arr1, arr2)\n",
    "                    \n",
    "                    if is_identical:\n",
    "                        print(f\"   üìä Data: ‚úÖ IDENTICAL\")\n",
    "                    else:\n",
    "                        # T√≠nh s·ª± kh√°c bi·ªát\n",
    "                        if np.issubdtype(arr1.dtype, np.number) and np.issubdtype(arr2.dtype, np.number):\n",
    "                            max_diff = np.abs(arr1 - arr2).max()\n",
    "                            mean_diff = np.abs(arr1 - arr2).mean()\n",
    "                            \n",
    "                            print(f\"   üìä Data: ‚ùå DIFFERENT\")\n",
    "                            print(f\"      Max difference: {max_diff}\")\n",
    "                            print(f\"      Mean difference: {mean_diff}\")\n",
    "                            \n",
    "                            # Ki·ªÉm tra g·∫ßn ƒë√∫ng\n",
    "                            is_close = np.allclose(arr1, arr2, rtol=1e-5, atol=1e-8)\n",
    "                            print(f\"      Close (rtol=1e-5): {'‚úÖ' if is_close else '‚ùå'}\")\n",
    "                            \n",
    "                            # Sample values ƒë·ªÉ debug\n",
    "                            if arr1.ndim == 1:\n",
    "                                print(f\"   üìã First 5 values:\")\n",
    "                                print(f\"      {file1}: {arr1[:5]}\")\n",
    "                                print(f\"      {file2}: {arr2[:5]}\")\n",
    "                            elif arr1.ndim == 2:\n",
    "                                print(f\"   üìã First 3x3 values:\")\n",
    "                                print(f\"      {file1}:\")\n",
    "                                print(f\"         {arr1[:3, :3]}\")\n",
    "                                print(f\"      {file2}:\")\n",
    "                                print(f\"         {arr2[:3, :3]}\")\n",
    "                            elif arr1.ndim >= 3:\n",
    "                                print(f\"   üìã Sample values (first element):\")\n",
    "                                if arr1.ndim == 3:\n",
    "                                    print(f\"      {file1}: shape {arr1[0].shape}\")\n",
    "                                    print(f\"         {arr1[0, :3, :3] if arr1.shape[1] >= 3 and arr1.shape[2] >= 3 else arr1[0]}\")\n",
    "                                    print(f\"      {file2}: shape {arr2[0].shape}\")\n",
    "                                    print(f\"         {arr2[0, :3, :3] if arr2.shape[1] >= 3 and arr2.shape[2] >= 3 else arr2[0]}\")\n",
    "                                elif arr1.ndim == 4:\n",
    "                                    print(f\"      {file1}: {arr1[0, :2, :2, :3] if min(arr1.shape[1:3]) >= 2 else 'Shape too small'}\")\n",
    "                                    print(f\"      {file2}: {arr2[0, :2, :2, :3] if min(arr2.shape[1:3]) >= 2 else 'Shape too small'}\")\n",
    "                        else:\n",
    "                            print(f\"   üìä Data: ‚ùå DIFFERENT (non-numeric)\")\n",
    "                            print(f\"   üìã First few values:\")\n",
    "                            print(f\"      {file1}: {arr1.flat[:5] if arr1.size >= 5 else arr1.flat[:]}\")\n",
    "                            print(f\"      {file2}: {arr2.flat[:5] if arr2.size >= 5 else arr2.flat[:]}\")\n",
    "                    \n",
    "                    # Th·ªëng k√™ c∆° b·∫£n cho numeric data\n",
    "                    if np.issubdtype(arr1.dtype, np.number):\n",
    "                        print(f\"   üìà Statistics:\")\n",
    "                        print(f\"      {file1}: min={arr1.min():.6f}, max={arr1.max():.6f}, mean={arr1.mean():.6f}\")\n",
    "                        print(f\"      {file2}: min={arr2.min():.6f}, max={arr2.max():.6f}, mean={arr2.mean():.6f}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Error comparing data: {e}\")\n",
    "            else:\n",
    "                print(f\"   üìä Data: Empty arrays\")\n",
    "        else:\n",
    "            print(f\"   üìä Data: Cannot compare - different shapes\")\n",
    "    \n",
    "    # T·ªïng k·∫øt cu·ªëi\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"üìù FINAL SUMMARY:\")\n",
    "    \n",
    "    all_shapes_match = True\n",
    "    all_data_identical = True\n",
    "    \n",
    "    for key in sorted(common_keys):\n",
    "        arr1 = data1[key]\n",
    "        arr2 = data2[key]\n",
    "        \n",
    "        shape_match = arr1.shape == arr2.shape\n",
    "        if not shape_match:\n",
    "            all_shapes_match = False\n",
    "            \n",
    "        if shape_match and arr1.size > 0:\n",
    "            try:\n",
    "                data_identical = np.array_equal(arr1, arr2)\n",
    "                if not data_identical:\n",
    "                    all_data_identical = False\n",
    "            except:\n",
    "                all_data_identical = False\n",
    "    \n",
    "    print(f\"   Keys match: {'‚úÖ' if keys1 == keys2 else '‚ùå'}\")\n",
    "    print(f\"   All shapes match: {'‚úÖ' if all_shapes_match else '‚ùå'}\")\n",
    "    print(f\"   All data identical: {'‚úÖ' if all_data_identical else '‚ùå'}\")\n",
    "    \n",
    "    # File sizes\n",
    "    size1 = os.path.getsize(file1) / (1024*1024)\n",
    "    size2 = os.path.getsize(file2) / (1024*1024)\n",
    "    print(f\"   File sizes: {file1} ({size1:.2f}MB) vs {file2} ({size2:.2f}MB)\")\n",
    "    \n",
    "    if all_data_identical and keys1 == keys2:\n",
    "        print(f\"\\nüéâ CONCLUSION: Files are IDENTICAL!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è CONCLUSION: Files are DIFFERENT!\")\n",
    "\n",
    "# Ch·∫°y ki·ªÉm tra chi ti·∫øt\n",
    "check_and_compare_files_detailed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8a830a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Functions defined!\n"
     ]
    }
   ],
   "source": [
    "def search_data(sequence_length, num_of_depend, label_start_idx,\n",
    "                num_for_predict, units, points_per_hour):\n",
    "    if points_per_hour < 0:\n",
    "        raise ValueError(\"points_per_hour should be greater than 0!\")\n",
    "\n",
    "    if label_start_idx + num_for_predict > sequence_length:\n",
    "        return None\n",
    "\n",
    "    x_idx = []\n",
    "    for i in range(1, num_of_depend + 1):\n",
    "        start_idx = label_start_idx - points_per_hour * units * i\n",
    "        end_idx = start_idx + num_for_predict\n",
    "        if start_idx >= 0:\n",
    "            x_idx.append((start_idx, end_idx))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    if len(x_idx) != num_of_depend:\n",
    "        return None\n",
    "\n",
    "    return x_idx[::-1]\n",
    "\n",
    "\n",
    "def get_sample_indices(data_sequence, num_of_weeks, num_of_days, num_of_hours,\n",
    "                       label_start_idx, num_for_predict, points_per_hour=12):\n",
    "    week_sample, day_sample, hour_sample = None, None, None\n",
    "\n",
    "    if label_start_idx + num_for_predict > data_sequence.shape[0]:\n",
    "        return week_sample, day_sample, hour_sample, None\n",
    "\n",
    "    if num_of_weeks > 0:\n",
    "        week_indices = search_data(data_sequence.shape[0], num_of_weeks,\n",
    "                                   label_start_idx, num_for_predict,\n",
    "                                   7 * 24, points_per_hour)\n",
    "        if not week_indices:\n",
    "            return None, None, None, None\n",
    "\n",
    "        week_sample = np.concatenate([data_sequence[i: j]\n",
    "                                      for i, j in week_indices], axis=0)\n",
    "\n",
    "    if num_of_days > 0:\n",
    "        day_indices = search_data(data_sequence.shape[0], num_of_days,\n",
    "                                  label_start_idx, num_for_predict,\n",
    "                                  24, points_per_hour)\n",
    "        if not day_indices:\n",
    "            return None, None, None, None\n",
    "\n",
    "        day_sample = np.concatenate([data_sequence[i: j]\n",
    "                                     for i, j in day_indices], axis=0)\n",
    "\n",
    "    if num_of_hours > 0:\n",
    "        hour_indices = search_data(data_sequence.shape[0], num_of_hours,\n",
    "                                   label_start_idx, num_for_predict,\n",
    "                                   1, points_per_hour)\n",
    "        if not hour_indices:\n",
    "            return None, None, None, None\n",
    "\n",
    "        hour_sample = np.concatenate([data_sequence[i: j]\n",
    "                                      for i, j in hour_indices], axis=0)\n",
    "\n",
    "    target = data_sequence[label_start_idx: label_start_idx + num_for_predict]\n",
    "\n",
    "    return week_sample, day_sample, hour_sample, target\n",
    "\n",
    "\n",
    "def MinMaxnormalization(train, val, test):\n",
    "    assert train.shape[1:] == val.shape[1:] and val.shape[1:] == test.shape[1:]\n",
    "\n",
    "    _max = train.max(axis=(0, 1, 3), keepdims=True)\n",
    "    _min = train.min(axis=(0, 1, 3), keepdims=True)\n",
    "\n",
    "    print('_max.shape:', _max.shape)\n",
    "    print('_min.shape:', _min.shape)\n",
    "\n",
    "    def normalize(x):\n",
    "        x = 1. * (x - _min) / (_max - _min)\n",
    "        x = 2. * x - 1.\n",
    "        return x\n",
    "\n",
    "    train_norm = normalize(train)\n",
    "    val_norm = normalize(val)\n",
    "    test_norm = normalize(test)\n",
    "\n",
    "    return {'_max': _max, '_min': _min}, train_norm, val_norm, test_norm\n",
    "\n",
    "print(\"‚úÖ Functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4a69af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Function with CORRECT target shape and INT64 timestamps defined!\n"
     ]
    }
   ],
   "source": [
    "def read_and_generate_dataset_encoder_decoder_correct(graph_signal_matrix_filename,\n",
    "                                              num_of_weeks, num_of_days,\n",
    "                                              num_of_hours, num_for_predict,\n",
    "                                              points_per_hour=12, save=False):\n",
    "    \n",
    "    data_seq = np.load(graph_signal_matrix_filename)['data']  # (T, N, F)\n",
    "    print(f\"üìä Loaded data shape: {data_seq.shape}\")\n",
    "\n",
    "    all_samples = []\n",
    "    for idx in range(data_seq.shape[0]):\n",
    "        sample = get_sample_indices(data_seq, num_of_weeks, num_of_days,\n",
    "                                    num_of_hours, idx, num_for_predict,\n",
    "                                    points_per_hour)\n",
    "        if ((sample[0] is None) and (sample[1] is None) and (sample[2] is None)):\n",
    "            continue\n",
    "\n",
    "        week_sample, day_sample, hour_sample, target = sample\n",
    "\n",
    "        sample = []\n",
    "\n",
    "        if num_of_weeks > 0:\n",
    "            week_sample = np.expand_dims(week_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(week_sample)\n",
    "\n",
    "        if num_of_days > 0:\n",
    "            day_sample = np.expand_dims(day_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(day_sample)\n",
    "\n",
    "        if num_of_hours > 0:\n",
    "            hour_sample = np.expand_dims(hour_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(hour_sample)\n",
    "\n",
    "        # üî• S·ª¨A: GI·ªÆ NGUY√äN C·∫¢ 2 FEATURES CHO TARGET\n",
    "        target = np.expand_dims(target, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "        sample.append(target)\n",
    "\n",
    "        # üî• S·ª¨A: TIMESTAMP TH√ÄNH INT64\n",
    "        time_sample = np.expand_dims(np.array([idx], dtype=np.int64), axis=0)  # (1,1) int64\n",
    "        sample.append(time_sample)\n",
    "\n",
    "        all_samples.append(sample)\n",
    "\n",
    "    print(f\"üìä Total samples created: {len(all_samples)}\")\n",
    "\n",
    "    # Chia d·ªØ li·ªáu chronological\n",
    "    total_samples = len(all_samples)\n",
    "    \n",
    "    if total_samples >= 1344:\n",
    "        test_samples = all_samples[-672:]\n",
    "        val_samples = all_samples[-1344:-672]\n",
    "        train_samples = all_samples[:-1344]\n",
    "        \n",
    "        print(f\"üìä Chronological split:\")\n",
    "        print(f\"   Train samples: {len(train_samples)} (earliest)\")\n",
    "        print(f\"   Val samples: {len(val_samples)} (middle)\")\n",
    "        print(f\"   Test samples: {len(test_samples)} (latest)\")\n",
    "    else:\n",
    "        split_line1 = int(total_samples * 0.6)\n",
    "        split_line2 = int(total_samples * 0.8)\n",
    "        train_samples = all_samples[:split_line1]\n",
    "        val_samples = all_samples[split_line1:split_line2]\n",
    "        test_samples = all_samples[split_line2:]\n",
    "\n",
    "    # Concatenate samples\n",
    "    training_set = [np.concatenate(i, axis=0) for i in zip(*train_samples)]\n",
    "    validation_set = [np.concatenate(i, axis=0) for i in zip(*val_samples)]\n",
    "    testing_set = [np.concatenate(i, axis=0) for i in zip(*test_samples)]\n",
    "\n",
    "    train_x = np.concatenate(training_set[:-2], axis=-1)  # (B,N,F,T_total)\n",
    "    val_x = np.concatenate(validation_set[:-2], axis=-1)\n",
    "    test_x = np.concatenate(testing_set[:-2], axis=-1)\n",
    "\n",
    "    train_target = training_set[-2]  # (B,N,F,T)\n",
    "    val_target = validation_set[-2]\n",
    "    test_target = testing_set[-2]\n",
    "\n",
    "    train_timestamp = training_set[-1]  # int64\n",
    "    val_timestamp = validation_set[-1]   # int64\n",
    "    test_timestamp = testing_set[-1]     # int64\n",
    "\n",
    "    print(f\"üîç Before normalization shapes:\")\n",
    "    print(f\"   train_target: {train_target.shape}\")\n",
    "    print(f\"   val_target: {val_target.shape}\")\n",
    "    print(f\"   test_target: {test_target.shape}\")\n",
    "    \n",
    "    # üî• KI·ªÇM TRA TIMESTAMP DTYPES\n",
    "    print(f\"üîç Timestamp dtypes:\")\n",
    "    print(f\"   train_timestamp: {train_timestamp.dtype}\")\n",
    "    print(f\"   val_timestamp: {val_timestamp.dtype}\")\n",
    "    print(f\"   test_timestamp: {test_timestamp.dtype}\")\n",
    "\n",
    "    # Normalization ch·ªâ cho input X\n",
    "    (stats, train_x_norm, val_x_norm, test_x_norm) = MinMaxnormalization(train_x, val_x, test_x)\n",
    "\n",
    "    all_data = {\n",
    "        'train': {\n",
    "            'x': train_x_norm,\n",
    "            'target': train_target,  # (B,N,F,T) = (3001,250,2,12)\n",
    "            'timestamp': train_timestamp,\n",
    "        },\n",
    "        'val': {\n",
    "            'x': val_x_norm,\n",
    "            'target': val_target,    # (B,N,F,T) = (672,250,2,12)\n",
    "            'timestamp': val_timestamp,\n",
    "        },\n",
    "        'test': {\n",
    "            'x': test_x_norm,\n",
    "            'target': test_target,   # (B,N,F,T) = (672,250,2,12)\n",
    "            'timestamp': test_timestamp,\n",
    "        },\n",
    "        'stats': {\n",
    "            '_max': stats['_max'],\n",
    "            '_min': stats['_min'],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print('‚úÖ CORRECTED Results:')\n",
    "    print('train x:', all_data['train']['x'].shape, all_data['train']['x'].dtype)\n",
    "    print('train target:', all_data['train']['target'].shape, all_data['train']['target'].dtype)\n",
    "    print('train timestamp:', all_data['train']['timestamp'].shape, all_data['train']['timestamp'].dtype)\n",
    "    print()\n",
    "    print('val x:', all_data['val']['x'].shape, all_data['val']['x'].dtype)\n",
    "    print('val target:', all_data['val']['target'].shape, all_data['val']['target'].dtype)\n",
    "    print('val timestamp:', all_data['val']['timestamp'].shape, all_data['val']['timestamp'].dtype)\n",
    "    print()\n",
    "    print('test x:', all_data['test']['x'].shape, all_data['test']['x'].dtype)\n",
    "    print('test target:', all_data['test']['target'].shape, all_data['test']['target'].dtype)\n",
    "    print('test timestamp:', all_data['test']['timestamp'].shape, all_data['test']['timestamp'].dtype)\n",
    "\n",
    "    if save:\n",
    "        file = os.path.basename(graph_signal_matrix_filename).split('.')[0]\n",
    "        dirpath = os.path.dirname(graph_signal_matrix_filename)\n",
    "        filename = os.path.join(dirpath,\n",
    "                                file + '_r' + str(num_of_hours) + '_d' + str(num_of_days) + '_w' + str(num_of_weeks))\n",
    "        print(f'üíæ Saving to: {filename}.npz')\n",
    "        np.savez_compressed(filename,\n",
    "                            train_x=all_data['train']['x'], train_target=all_data['train']['target'],\n",
    "                            train_timestamp=all_data['train']['timestamp'],\n",
    "                            val_x=all_data['val']['x'], val_target=all_data['val']['target'],\n",
    "                            val_timestamp=all_data['val']['timestamp'],\n",
    "                            test_x=all_data['test']['x'], test_target=all_data['test']['target'],\n",
    "                            test_timestamp=all_data['test']['timestamp'],\n",
    "                            mean=all_data['stats']['_max'], std=all_data['stats']['_min']\n",
    "                            )\n",
    "    return all_data\n",
    "\n",
    "print(\"‚úÖ Function with CORRECT target shape and INT64 timestamps defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c8f6570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuration:\n",
      "   Data file: taxi_raw.npz\n",
      "   Vertices: 266\n",
      "   Points per hour: 12\n",
      "   Predict steps: 12\n",
      "   Temporal: W=0, D=0, H=1\n",
      "‚úÖ Data shape: (4368, 266, 2)\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªçc config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('configurations/taxi.conf')\n",
    "\n",
    "data_config = config['Data']\n",
    "training_config = config['Training']\n",
    "\n",
    "# L·∫•y parameters\n",
    "graph_signal_matrix_filename = data_config['graph_signal_matrix_filename']\n",
    "num_of_vertices = int(data_config['num_of_vertices'])\n",
    "points_per_hour = int(data_config['points_per_hour'])\n",
    "num_for_predict = int(data_config['num_for_predict'])\n",
    "num_of_weeks = int(training_config['num_of_weeks'])\n",
    "num_of_days = int(training_config['num_of_days'])\n",
    "num_of_hours = int(training_config['num_of_hours'])\n",
    "\n",
    "print(f\"üìã Configuration:\")\n",
    "print(f\"   Data file: {graph_signal_matrix_filename}\")\n",
    "print(f\"   Vertices: {num_of_vertices}\")\n",
    "print(f\"   Points per hour: {points_per_hour}\")\n",
    "print(f\"   Predict steps: {num_for_predict}\")\n",
    "print(f\"   Temporal: W={num_of_weeks}, D={num_of_days}, H={num_of_hours}\")\n",
    "\n",
    "# Ki·ªÉm tra data shape\n",
    "data = np.load(graph_signal_matrix_filename)\n",
    "print(f\"‚úÖ Data shape: {data['data'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ebc4365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting data processing...\n",
      "üìä Loaded data shape: (4368, 266, 2)\n",
      "üìä Total samples created: 4345\n",
      "üìä Chronological split:\n",
      "   Train samples: 3001 (earliest)\n",
      "   Val samples: 672 (middle)\n",
      "   Test samples: 672 (latest)\n",
      "üîç Before normalization shapes:\n",
      "   train_target: (3001, 266, 2, 12)\n",
      "   val_target: (672, 266, 2, 12)\n",
      "   test_target: (672, 266, 2, 12)\n",
      "üîç Timestamp dtypes:\n",
      "   train_timestamp: int64\n",
      "   val_timestamp: int64\n",
      "   test_timestamp: int64\n",
      "_max.shape: (1, 1, 2, 1)\n",
      "_min.shape: (1, 1, 2, 1)\n",
      "‚úÖ CORRECTED Results:\n",
      "train x: (3001, 266, 2, 12) float64\n",
      "train target: (3001, 266, 2, 12) int64\n",
      "train timestamp: (3001, 1) int64\n",
      "\n",
      "val x: (672, 266, 2, 12) float64\n",
      "val target: (672, 266, 2, 12) int64\n",
      "val timestamp: (672, 1) int64\n",
      "\n",
      "test x: (672, 266, 2, 12) float64\n",
      "test target: (672, 266, 2, 12) int64\n",
      "test timestamp: (672, 1) int64\n",
      "üíæ Saving to: taxi_raw_r1_d0_w0.npz\n",
      "\n",
      "üéâ Processing complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Starting data processing...\")\n",
    "\n",
    "all_data = read_and_generate_dataset_encoder_decoder_correct(\n",
    "    graph_signal_matrix_filename=graph_signal_matrix_filename,\n",
    "    num_of_weeks=num_of_weeks,\n",
    "    num_of_days=num_of_days,\n",
    "    num_of_hours=num_of_hours,\n",
    "    num_for_predict=num_for_predict,\n",
    "    points_per_hour=points_per_hour,\n",
    "    save=True\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d56c1a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating SVD with advanced method\n",
      "   Dataset: bike\n",
      "   City: NYC\n",
      "   Normalization: MinMax01\n",
      "   Categories: ['bike']\n",
      "   Hidden size: 250\n",
      "   Graph normalization: randomwalk\n",
      "üìÇ Processing category: bike\n",
      "   Reading: data/bike/NYC/bike_data.h5\n",
      "   üìã Keys in file: ['bike_drop', 'bike_pick']\n",
      "   üìä Pick data shape: (4368, 250)\n",
      "   üìä Drop data shape: (4368, 250)\n",
      "   üìä Normalized data shape: (4368, 250, 2)\n",
      "üìä Concatenated data shape: (4368, 2, 250)\n",
      "üìä Training data shape after removal: (3024, 2, 250)\n",
      "üìä Reshaped inputs for SVD: (6048, 250)\n",
      "üîß Performing SVD...\n",
      "üìä SVD shapes - U: (6048, 250), S: (250,), V: (250, 250)\n",
      "üìä Weight matrix shape: (250, 250)\n",
      "üîß Creating graph...\n",
      "üìä Graph shape: (250, 250)\n",
      "üìä Support matrix stats: min=0.000582, max=1.000000\n",
      "üîß Applying random walk normalization...\n",
      "üìä Final support matrix shape: (250, 250)\n",
      "üìä Final support matrix stats: min=0.000000, max=0.020004\n",
      "‚úÖ Saved SVD weight matrix to: data/bike_svd1.npy\n",
      "‚úÖ Saved support matrix to: data/bike_svd1_support.npy\n",
      "\n",
      "üéâ SVD creation completed!\n",
      "\n",
      "üìä Results:\n",
      "   Weight matrix shape: (250, 250)\n",
      "   Support matrix shape: (250, 250)\n",
      "   ‚úÖ Saved SVD shape: (250, 250)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import os\n",
    "\n",
    "# ========================================================================================\n",
    "# NORMALIZATION METHODS\n",
    "# ========================================================================================\n",
    "\n",
    "class normalization:\n",
    "    @staticmethod\n",
    "    def MinMax01():\n",
    "        return MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def MinMax11():\n",
    "        return MinMaxScaler(feature_range=(-1, 1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def Standard():\n",
    "        return StandardScaler()\n",
    "    \n",
    "    @staticmethod\n",
    "    def None_():\n",
    "        class NoNormalization:\n",
    "            def fit_transform(self, x):\n",
    "                return x\n",
    "            def transform(self, x):\n",
    "                return x\n",
    "        return NoNormalization()\n",
    "\n",
    "# ========================================================================================\n",
    "# GRAPH NORMALIZATION METHODS\n",
    "# ========================================================================================\n",
    "\n",
    "def random_walk_matrix(adj_matrix):\n",
    "    \"\"\"Random walk normalization\"\"\"\n",
    "    d = np.sum(adj_matrix, axis=1)\n",
    "    d_inv = np.power(d, -1)\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    d_mat_inv = np.diag(d_inv)\n",
    "    return d_mat_inv.dot(adj_matrix)\n",
    "\n",
    "def normalized_laplacian(adj_matrix):\n",
    "    \"\"\"Normalized Laplacian\"\"\"\n",
    "    d = np.sum(adj_matrix, axis=1)\n",
    "    d_inv_sqrt = np.power(d, -0.5)\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = np.diag(d_inv_sqrt)\n",
    "    return d_mat_inv_sqrt.dot(adj_matrix).dot(d_mat_inv_sqrt)\n",
    "\n",
    "# ========================================================================================\n",
    "# MAIN FUNCTION\n",
    "# ========================================================================================\n",
    "\n",
    "def create_svd_from_h5_advanced(dataset='bike', city='NYC', \n",
    "                               Normal_Method='MinMax01', \n",
    "                               data_category=['bike'],\n",
    "                               hidden_size=50,\n",
    "                               normalized_category='randomwalk',\n",
    "                               output_path='data/bike_svd.npy',\n",
    "                               _len=[672, 672]):\n",
    "    \"\"\"\n",
    "    T·∫°o SVD theo ph∆∞∆°ng ph√°p trong prompt\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset: str, t√™n dataset\n",
    "    city: str, t√™n th√†nh ph·ªë  \n",
    "    Normal_Method: str, ph∆∞∆°ng ph√°p normalization\n",
    "    data_category: list, danh s√°ch categories\n",
    "    hidden_size: int, s·ªë dimensions cho SVD\n",
    "    normalized_category: str, 'randomwalk' or 'laplacian'\n",
    "    output_path: str, ƒë∆∞·ªùng d·∫´n l∆∞u file SVD\n",
    "    _len: list, [val_len, test_len] ƒë·ªÉ lo·∫°i b·ªè\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Creating SVD with advanced method\")\n",
    "    print(f\"   Dataset: {dataset}\")\n",
    "    print(f\"   City: {city}\")\n",
    "    print(f\"   Normalization: {Normal_Method}\")\n",
    "    print(f\"   Categories: {data_category}\")\n",
    "    print(f\"   Hidden size: {hidden_size}\")\n",
    "    print(f\"   Graph normalization: {normalized_category}\")\n",
    "    \n",
    "    # L·∫•y normalization method\n",
    "    normal_method = getattr(normalization, Normal_Method)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for category in data_category:\n",
    "        print(f\"üìÇ Processing category: {category}\")\n",
    "        \n",
    "        normal = normal_method()\n",
    "        \n",
    "        # ƒê·ªçc pick data\n",
    "        h5_file = f\"data/{dataset}/{city}/{category}_data.h5\"\n",
    "        print(f\"   Reading: {h5_file}\")\n",
    "        \n",
    "        if not os.path.exists(h5_file):\n",
    "            print(f\"   ‚ùå File not found: {h5_file}\")\n",
    "            continue\n",
    "            \n",
    "        with h5py.File(h5_file, 'r') as hf:\n",
    "            print(f\"   üìã Keys in file: {list(hf.keys())}\")\n",
    "            data_pick = hf[f'{category}_pick'][:]\n",
    "            print(f\"   üìä Pick data shape: {data_pick.shape}\")\n",
    "        \n",
    "        # ƒê·ªçc drop data\n",
    "        with h5py.File(h5_file, 'r') as hf:\n",
    "            data_drop = hf[f'{category}_drop'][:]\n",
    "            print(f\"   üìä Drop data shape: {data_drop.shape}\")\n",
    "        \n",
    "        # Stack v√† normalize\n",
    "        stacked_data = np.stack([data_pick, data_drop], axis=2)  # (T, N, 2)\n",
    "        normalized_data = normal.fit_transform(stacked_data.reshape(-1, stacked_data.shape[-1]))\n",
    "        normalized_data = normalized_data.reshape(stacked_data.shape)\n",
    "        \n",
    "        print(f\"   üìä Normalized data shape: {normalized_data.shape}\")\n",
    "        data.append(normalized_data)\n",
    "    \n",
    "    # Concatenate data\n",
    "    data = np.concatenate(data, axis=1).transpose((0, 2, 1))  # (T, F, N)\n",
    "    print(f\"üìä Concatenated data shape: {data.shape}\")\n",
    "    \n",
    "    # Remove validation and test data\n",
    "    data = data[:-(_len[0] + _len[1])]\n",
    "    print(f\"üìä Training data shape after removal: {data.shape}\")\n",
    "    \n",
    "    T, input_dim, N = data.shape\n",
    "    \n",
    "    # Reshape for SVD\n",
    "    inputs = data.reshape(-1, N)  # (T*F, N)\n",
    "    print(f\"üìä Reshaped inputs for SVD: {inputs.shape}\")\n",
    "    \n",
    "    # Perform SVD\n",
    "    print(f\"üîß Performing SVD...\")\n",
    "    u, s, v = np.linalg.svd(inputs, full_matrices=False)\n",
    "    print(f\"üìä SVD shapes - U: {u.shape}, S: {s.shape}, V: {v.shape}\")\n",
    "    \n",
    "    # Create weight matrix\n",
    "    w = np.diag(s[:hidden_size]).dot(v[:hidden_size, :]).T  # (N, hidden_size)\n",
    "    print(f\"üìä Weight matrix shape: {w.shape}\")\n",
    "    \n",
    "    # Create graph using euclidean distance\n",
    "    print(f\"üîß Creating graph...\")\n",
    "    graph = cdist(w, w, metric='euclidean')  # (N, N)\n",
    "    print(f\"üìä Graph shape: {graph.shape}\")\n",
    "    \n",
    "    # Create support matrix\n",
    "    support = graph * -1 / (np.std(graph) ** 2)\n",
    "    support = np.exp(support)\n",
    "    print(f\"üìä Support matrix stats: min={support.min():.6f}, max={support.max():.6f}\")\n",
    "    \n",
    "    # Remove self-connections\n",
    "    support = support - np.identity(support.shape[0])\n",
    "    \n",
    "    # Apply graph normalization\n",
    "    if normalized_category == 'randomwalk':\n",
    "        print(f\"üîß Applying random walk normalization...\")\n",
    "        support = random_walk_matrix(support)\n",
    "    elif normalized_category == 'laplacian':\n",
    "        print(f\"üîß Applying Laplacian normalization...\")\n",
    "        support = normalized_laplacian(support)\n",
    "    \n",
    "    print(f\"üìä Final support matrix shape: {support.shape}\")\n",
    "    print(f\"üìä Final support matrix stats: min={support.min():.6f}, max={support.max():.6f}\")\n",
    "    \n",
    "    # Save SVD (s·ª≠ d·ª•ng weight matrix w thay v√¨ support)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    np.save(output_path, w)  # L∆∞u weight matrix (N, hidden_size)\n",
    "    print(f\"‚úÖ Saved SVD weight matrix to: {output_path}\")\n",
    "    \n",
    "    # L∆∞u support matrix n·∫øu c·∫ßn\n",
    "    support_path = output_path.replace('.npy', '_support.npy')\n",
    "    np.save(support_path, support)\n",
    "    print(f\"‚úÖ Saved support matrix to: {support_path}\")\n",
    "    \n",
    "    return w, support\n",
    "\n",
    "# ========================================================================================\n",
    "# WRAPPER FUNCTION CHO BIKE DATA\n",
    "# ========================================================================================\n",
    "\n",
    "def create_bike_svd_advanced():\n",
    "    \"\"\"\n",
    "    T·∫°o bike SVD theo ph∆∞∆°ng ph√°p advanced\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ki·ªÉm tra structure th∆∞ m·ª•c\n",
    "    base_path = \"data/bike/NYC\"\n",
    "    h5_file = f\"{base_path}/bike_data.h5\"\n",
    "    \n",
    "    if not os.path.exists(h5_file):\n",
    "        print(f\"‚ùå File not found: {h5_file}\")\n",
    "        print(\"üîç Trying alternative paths...\")\n",
    "        \n",
    "        # Th·ª≠ c√°c ƒë∆∞·ªùng d·∫´n kh√°c\n",
    "        alternative_paths = [\n",
    "            \"data/nogrid/bike_data.h5\",\n",
    "            \"data/bike_data.h5\",\n",
    "            \"bike_data.h5\"\n",
    "        ]\n",
    "        \n",
    "        found = False\n",
    "        for alt_path in alternative_paths:\n",
    "            if os.path.exists(alt_path):\n",
    "                print(f\"‚úÖ Found file at: {alt_path}\")\n",
    "                # T·∫°o symbolic link ho·∫∑c copy\n",
    "                os.makedirs(os.path.dirname(h5_file), exist_ok=True)\n",
    "                import shutil\n",
    "                shutil.copy2(alt_path, h5_file)\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(\"‚ùå No H5 file found. Please check file paths.\")\n",
    "            return None\n",
    "    \n",
    "    # Ch·∫°y SVD\n",
    "    try:\n",
    "        w, support = create_svd_from_h5_advanced(\n",
    "            dataset='bike',\n",
    "            city='NYC', \n",
    "            Normal_Method='MinMax01',\n",
    "            data_category=['bike'],\n",
    "            hidden_size=250,\n",
    "            normalized_category='randomwalk',\n",
    "            output_path='data/bike_svd1.npy',\n",
    "            _len=[672, 672]\n",
    "        )\n",
    "        \n",
    "        print(\"\\nüéâ SVD creation completed!\")\n",
    "        return w, support\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# ========================================================================================\n",
    "# CH·∫†Y\n",
    "# ========================================================================================\n",
    "\n",
    "# T·∫°o bike SVD\n",
    "result = create_bike_svd_advanced()\n",
    "\n",
    "if result is not None:\n",
    "    w, support = result\n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"   Weight matrix shape: {w.shape}\")\n",
    "    print(f\"   Support matrix shape: {support.shape}\")\n",
    "    \n",
    "    # Verify files\n",
    "    if os.path.exists('data/bike_svd1.npy'):\n",
    "        loaded = np.load('data/bike_svd1.npy')\n",
    "        print(f\"   ‚úÖ Saved SVD shape: {loaded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59b15d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ data/bike_svd.npy:\n",
      "   Shape: (250, 250)\n",
      "   Range: [0.000000, 0.004293]\n",
      "   Mean: 0.004000\n",
      "   Std: 0.000267\n",
      "\n",
      "‚úÖ data/bike_svd_support.npy:\n",
      "   Shape: (250, 250)\n",
      "   Range: [0.000000, 0.020004]\n",
      "   Mean: 0.004000\n",
      "   Std: 0.001453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def quick_check():\n",
    "    \"\"\"\n",
    "    Ki·ªÉm tra nhanh 2 file SVD\n",
    "    \"\"\"\n",
    "    files = ['data/bike_svd.npy', 'data/bike_svd_support.npy']\n",
    "    \n",
    "    for file in files:\n",
    "        if os.path.exists(file):\n",
    "            data = np.load(file)\n",
    "            print(f\"‚úÖ {file}:\")\n",
    "            print(f\"   Shape: {data.shape}\")\n",
    "            print(f\"   Range: [{data.min():.6f}, {data.max():.6f}]\")\n",
    "            print(f\"   Mean: {data.mean():.6f}\")\n",
    "            print(f\"   Std: {data.std():.6f}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"‚ùå {file} not found\")\n",
    "\n",
    "quick_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: process_bike_data.py\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ PROCESSING BIKE DATA\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # B∆∞·ªõc 1: T·∫°o bike_raw.npz\n",
    "    print(\"üìù Step 1: Creating bike_raw.npz...\")\n",
    "    raw_data = create_bike_raw_npz_flexible()\n",
    "    \n",
    "    # B∆∞·ªõc 2: Validate data\n",
    "    print(\"\\nüìù Step 2: Validating data...\")\n",
    "    # validate_bike_data()\n",
    "    \n",
    "    # B∆∞·ªõc 3: Ch·∫°y preprocessing\n",
    "    print(\"\\nüìù Step 3: Running preprocessing...\")\n",
    "    all_data = read_and_generate_dataset_encoder_decoder(\n",
    "        graph_signal_matrix_filename='data/bike_raw.npz',\n",
    "        num_of_weeks=1,\n",
    "        num_of_days=1,\n",
    "        num_of_hours=1,\n",
    "        num_for_predict=12,\n",
    "        points_per_hour=12,\n",
    "        save=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ DONE! Files created:\")\n",
    "    print(\"   - data/bike_raw.npz (raw data)\")\n",
    "    print(\"   - data/bike_raw_r1_d1_w1.npz (processed data)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
