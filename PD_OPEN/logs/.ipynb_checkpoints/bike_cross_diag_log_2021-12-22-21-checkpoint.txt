total training epoch, fine tune epoch: 30 , 40
batch_size: 4
folder_dir: MAE_ASTGNN_h1d0w0_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_diag
load file: data/bike.npz
ori length: 3001 , percent: 1.0 , scale: 3001
train: torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12])
val: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
test: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
TemporalPositionalEncoding max_len: 12
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (src_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
  (prediction_generator2): Linear(in_features=64, out_features=1, bias=True)
)
delete the old one and create params directory experiments/bike/MAE_ASTGNN_h1d0w0_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_diag
Net's state_dict:
encoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
encoder.norm2.weight 	 torch.Size([64])
encoder.norm2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
decoder.norm2.weight 	 torch.Size([64])
decoder.norm2.bias 	 torch.Size([64])
src_embed.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed.1.embedding.weight 	 torch.Size([250, 64])
src_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed2.1.embedding.weight 	 torch.Size([250, 64])
trg_embed.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.1.embedding.weight 	 torch.Size([250, 64])
trg_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed2.1.embedding.weight 	 torch.Size([250, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
prediction_generator2.weight 	 torch.Size([1, 64])
prediction_generator2.bias 	 torch.Size([1])
Net's total params: 1140610
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405]}]
predicting testing set batch 1 / 168, time: 0.72s
predicting testing set batch 101 / 168, time: 33.70s
test time on whole data:55.51s
38.014671251844796 43.25374718965063 1541.6172389839821 -0.03624930176753597 
35.87946372801571 40.56649529970652 1454.2131975898076 -0.03260308443673531 
35.036854723344426 39.696272204701245 1419.3053575955457 -0.027594935280986648 
33.271857152260665 38.358420900122496 1346.7799868227148 -0.02349092621918004 
31.025109782382224 35.99141571502087 1254.3624795972873 -0.01978308000894916 
28.791958043463172 32.951775182431334 1162.459299680156 -0.016056220306904952 
26.139082338316545 29.428675292602335 1053.859559079993 -0.012707854029391877 
23.459994932566282 26.120401729414475 944.3126381269769 -0.009572228092219932 
21.836230959209036 24.056718878416792 877.7650264778212 -0.006363686757015989 
21.35838936791152 23.340003588825095 858.2053815677234 -0.0032679144063749842 
20.805774358825758 22.78114995394618 836.1908097412563 -0.00037984805147806873 
19.592405458368983 21.710290430051252 787.7973031115474 0.0027807193090903983 
27.934316008042426 32.412719302612366 1128.0569149047408 0.0027807193090903983 
epoch: 0, train time every whole data:217.46s
epoch: 0, total time:283.95s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 34.42s
test time on whole data:56.45s
4.2026514850048615 5.881774027219305 133.26012884676456 0.3392092673388327 
4.206516192115666 5.914300297091083 132.7554818773411 0.31760698676772275 
4.191057135904917 5.930658393520099 131.22022200835164 0.29257331777390044 
4.201978245575424 5.971670481519316 130.70962938694072 0.26384444936946283 
4.200744824442392 5.99633032072735 129.64172081776283 0.23477992351059676 
4.2165269192021695 6.038687476465415 129.50841891524337 0.2097632192243587 
4.26117380715747 6.103580336547201 130.64582854715778 0.183586605312386 
4.2593681079244154 6.116286008730163 130.02657708039038 0.1628086254737593 
4.319421302333918 6.179893099717735 132.0299257521272 0.14028491297380266 
4.357979778724678 6.222967904748837 133.40733394319636 0.12552028685631159 
4.3356753580618115 6.200265132130604 132.3508234233724 0.11405522854351047 
4.351106336953828 6.200305141242952 132.6860523550149 0.09860376260864044 
4.258683291116796 6.0642093608840355 131.52017457668484 0.09860376260864044 
epoch: 1, train time every whole data:215.48s
epoch: 1, total time:567.86s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 34.20s
test time on whole data:57.23s
2.7469022170259128 4.590687063971634 76.13775518492527 0.615366651376022 
2.7572723580500376 4.626891990334464 75.96358658224617 0.5902901431932489 
2.795147313322872 4.700682968193701 76.4240418628769 0.5579163527862877 
2.836155814242771 4.770331605146789 77.19527392165611 0.5214268319724942 
2.872892735090905 4.835758617347434 77.6918620564647 0.4812437257528813 
2.9365018799474374 4.920511706065412 79.27987424093556 0.43941456811250895 
3.025059421227624 5.023108474835767 81.8648333232789 0.39396959000705667 
3.1059846121578345 5.113263274414578 84.36975035893157 0.35303230740325797 
3.1860517715334535 5.195005097862089 86.95762719257701 0.31281319388390166 
3.2481423072974596 5.2542634587374275 89.03717816559544 0.2795144157268104 
3.2897433296509444 5.289356536664405 90.36218508961943 0.24955448301737052 
3.3235549061175966 5.309864361048039 91.23867236255646 0.22005552770301431 
3.010284055472071 4.975568750434516 82.21052603174222 0.22005552770301431 
epoch: 2, train time every whole data:214.85s
epoch: 2, total time:850.94s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 34.25s
test time on whole data:57.32s
2.111187342219232 3.7069775454338374 62.146612001161515 0.6363559012540148 
2.148282923452645 3.792998102272313 63.366831742142516 0.6096902434520514 
2.238391376280536 3.9514532875031403 60.076347546816244 0.5725967662363308 
2.289639876324329 4.03259552436252 61.51822124113742 0.5372921396937296 
2.3544747245608875 4.12053501088373 63.330927270677726 0.49699485662832193 
2.4230979679964837 4.212202001881757 65.04943061685233 0.45673514286689826 
2.489699956961597 4.305243932070004 66.13648210137669 0.41704342167905534 
2.552892163161011 4.385504969929431 66.04069403930077 0.3817341188529172 
2.6143006577400403 4.454869468148148 65.85461277168622 0.34950684562418477 
2.6771389312922422 4.512681811441406 67.04210648924827 0.3199922463186776 
2.7423085978809034 4.555631715015225 68.37009934866704 0.2936345613926727 
2.8060372430481726 4.586940973775123 69.50067816978289 0.2705042463188717 
2.45395431340984 4.227859263646308 64.86956189700437 0.2705042463188717 
epoch: 3, train time every whole data:215.01s
epoch: 3, total time:1134.58s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 32.94s
test time on whole data:54.85s
2.8459934679201493 4.448762113721806 85.66650711591303 0.5865927583448355 
2.934435622803423 4.608183352616697 87.40518068413866 0.5394657509356534 
3.146105004312648 4.957112005609215 90.94135011817596 0.4565529063019365 
3.177814301638465 5.017997663829045 91.30552770603046 0.42803363332467387 
3.228209439922568 5.090490218576442 92.3774486759033 0.3987458191414897 
3.2881585580141595 5.159998128283168 93.7373716139198 0.37229257669365357 
3.349258115601593 5.214163054657157 94.8124290027105 0.3611509707814202 
3.3845822197457864 5.250647025617423 95.03764831356581 0.35362830131972567 
3.442625122895287 5.307836644847996 96.50697390831137 0.33852026537717095 
3.4916656198750826 5.365973658812677 98.08269901601258 0.31801875887713377 
3.492731736746129 5.391537591569119 97.79293044987305 0.29158515429672327 
3.5108448155561374 5.421162046784758 98.17641429728697 0.2597600586001731 
3.274368668752619 5.111262390854137 93.48710516011533 0.2597600586001731 
epoch: 4, train time every whole data:214.66s
epoch: 4, total time:1416.54s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.16s
test time on whole data:55.51s
1.9631443810083327 3.5372018313989497 57.6126531084483 0.7228330317197039 
2.0797653263129647 3.767404391263879 60.72968383857999 0.6700626559847407 
2.2384326988564185 4.091658270809467 62.682916735452835 0.6013646789454566 
2.305607831362369 4.207805144137717 63.638854831887336 0.5658618520921105 
2.379699328727222 4.320328249751848 63.80774884604151 0.5332517619413631 
2.4164921516193343 4.352194047598086 63.53283634332472 0.5235520052362325 
2.4440094517273385 4.360450289598089 63.54025823840813 0.5278994936234671 
2.443853036501933 4.336807904381379 62.77356489509411 0.5314907696417044 
2.4574012490825816 4.340369652616153 62.75909155455467 0.5320107546303813 
2.4977646630079087 4.397088524313521 64.41096725251319 0.5235417188228689 
2.51541962129126 4.428755976480225 64.62690674496442 0.5059939535527295 
2.54490552968044 4.465885274514115 65.20751854969258 0.4826452936664228 
2.357207939098175 4.22602936096567 62.94366601375374 0.4826452936664228 
epoch: 5, train time every whole data:215.11s
epoch: 5, total time:1696.79s
predicting testing set batch 1 / 168, time: 0.33s
predicting testing set batch 101 / 168, time: 33.02s
test time on whole data:54.90s
1.9033715133393805 3.3784064560328178 58.93606530574195 0.7337274770941605 
2.0182064051915494 3.6074634767411036 61.71132487763016 0.6871516162415421 
2.164896255407039 3.910013933355308 63.8502184298116 0.6206147819588647 
2.2218889224887604 4.01196487425361 64.4500808952441 0.5925980723746593 
2.312347901422353 4.164185428111832 65.12075215788087 0.5557866684676811 
2.354561209321377 4.217110406336983 65.15370186082176 0.5431216792317671 
2.40349969501545 4.2505477273183025 66.47857835559348 0.5372623925315658 
2.4172083254653427 4.254807297466473 66.7465706714289 0.5368190459948533 
2.4589732755583134 4.301661660181628 67.76098854412088 0.5377425193860316 
2.4933970280342868 4.3348328113723085 69.07500178271142 0.537304688801141 
2.4844455474088236 4.334156597171809 68.43664375370706 0.5357448385084596 
2.49466345641762 4.35024408384898 68.518933666813 0.5257476769626115 
2.3106216279225245 4.103972680508073 65.52006993209521 0.5257476769626115 
epoch: 6, train time every whole data:218.39s
epoch: 6, total time:1980.98s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 32.90s
test time on whole data:54.90s
2.3077056264460443 3.2614175629075612 85.12985680735423 0.7413846263142342 
2.388078867461266 3.4119710546896895 87.70728198209788 0.7029695995742278 
2.468071806748087 3.5631707092034564 89.33551448370018 0.662249634296213 
2.5238167152968014 3.6540065186856974 90.2852585811328 0.6335527140881886 
2.5799735639662082 3.7253850491429383 91.55357106048828 0.6114271637155682 
2.617577735100385 3.7506221587991218 92.44968712492583 0.604543881108936 
2.6372430279333856 3.7456040918446294 92.87263253444857 0.6071468049127597 
2.6259075639576963 3.7279308934072324 91.19494163009266 0.6102769885742323 
2.6263927507540656 3.7262681935711393 90.22267736145079 0.6096567877558795 
2.593374396301185 3.710872791613382 86.519879415306 0.6095273965723766 
2.5739069624297497 3.7182692708034635 83.94126215622816 0.6039633344688476 
2.6098055273967664 3.778769239083123 84.57614542002814 0.5828556828614455 
2.5459878786493033 3.6510191967326033 88.81573865751801 0.5828556828614455 
epoch: 7, train time every whole data:213.99s
epoch: 7, total time:2260.86s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 34.71s
test time on whole data:57.78s
1.9077968734248232 3.3545394531469346 55.5027182694449 0.7464650444679836 
1.9904405640454164 3.5324179281263812 57.12930194678162 0.7109022544859276 
2.10786835040498 3.7994513896480773 57.7428178985015 0.6642108653318822 
2.1614950284944583 3.8953999637746555 58.21683320780306 0.6447877393840176 
2.233134316229927 4.034621206307093 58.29458836836381 0.6189698855522433 
2.2733801195104384 4.088601003599787 58.377090335007864 0.6140249235819895 
2.3095213036418314 4.110227156059989 59.40573532744047 0.6182645269910299 
2.325802973354856 4.116998278309957 59.92576368049447 0.6233422828705417 
2.3524005854803893 4.149269112135281 61.012212904504906 0.621301935328374 
2.3855061352627263 4.190969791188781 61.61824782973171 0.6160852518118837 
2.4160334224309774 4.246423785175648 61.94047416122783 0.6048865679021481 
2.4412236644587524 4.29894324832408 62.26566450905781 0.579314812999243 
2.2420502780616314 3.9945070786432892 59.2860666059666 0.579314812999243 
epoch: 8, train time every whole data:220.52s
epoch: 8, total time:2555.61s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 34.56s
test time on whole data:57.63s
1.8032052756682748 3.2370642361127215 54.75373743410634 0.7371920425528169 
1.864581146789892 3.35750973712774 56.86794005355375 0.7110200892387045 
1.9342454486865372 3.4945216426002483 58.39555838503344 0.6817622465781841 
1.9573184872900269 3.535429098458412 58.298624225532 0.6775344693637606 
1.9836811904983507 3.574052933855994 58.40131269348481 0.6697469415295775 
1.9946160759132887 3.5853454960717284 58.531945586832975 0.668207036551233 
2.0096404325789994 3.6081560179633474 58.93698624169458 0.6642193617058227 
2.031455309985649 3.646788729392083 59.430278798188795 0.6569962575162844 
2.068388086818336 3.713173462213345 59.86448640653119 0.6517606055317973 
2.0972236182450184 3.776391986857735 59.66374195888135 0.6476943174046887 
2.144902123396684 3.8702433392924043 59.53000501754697 0.6367898300116874 
2.2147443025987594 3.984038605862242 61.31147387967542 0.595486105590743 
2.0086667915391514 3.620646261703785 58.665589851165656 0.595486105590743 
epoch: 9, train time every whole data:214.18s
epoch: 9, total time:2837.76s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 34.48s
test time on whole data:57.56s
1.9358671023340097 3.0367159842824374 57.04123846521072 0.7645158506595785 
1.9960106634895007 3.195021793277143 59.177038009931756 0.7306179402220312 
2.0383603028589414 3.3305328907117735 59.81918117125704 0.7069854459985071 
2.0740181613253164 3.4152261445343743 60.580374182036124 0.6857532371290135 
2.100774747252908 3.476959550699998 61.11032863617828 0.6732624226861303 
2.1117158959640103 3.5159864132207512 60.73999090078036 0.6651235989184312 
2.1183675550116847 3.533266401591974 60.57392862418569 0.6611974728212905 
2.126066345643075 3.544296288828879 60.856708616876254 0.6563736913455609 
2.129801361281425 3.5561798670731006 60.673672596239214 0.6545529251368928 
2.1388694494534284 3.5459432246739473 61.477474375275264 0.6556073850014744 
2.1472248365080784 3.5593818120227034 60.4657454637044 0.655791706452586 
2.1882742674867846 3.623710351096536 61.392828777189614 0.6376030778285938 
2.092112557384097 3.4484784953887915 60.32576385313917 0.6376030778285938 
epoch: 10, train time every whole data:213.95s
epoch: 10, total time:3121.22s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 32.97s
test time on whole data:54.85s
1.8233387578449078 3.1070073416391852 59.769758675677295 0.7684977984929741 
1.9007467170548404 3.280370436506976 61.45524678567662 0.7398964548961905 
1.9986533526826118 3.475113681815415 62.17424858453434 0.7102512738947573 
2.02583547957277 3.5216717773823754 62.38460644150112 0.7060172220977552 
2.0640992858830307 3.5671232354978506 62.938320073319254 0.6951437488357133 
2.077163499602161 3.5546853914677308 63.31303767628087 0.6962722726402829 
2.0799761523230975 3.5278176419322174 64.13420458751115 0.696167445977779 
2.0763361509944356 3.513149250779034 64.50306205832386 0.6986844854628795 
2.1086669307084134 3.557665857348031 65.50385554259185 0.6913332771752586 
2.1518205895641196 3.6232753721331084 67.18388829111778 0.6820684343885353 
2.2183242885971532 3.735550844193308 68.34092442613151 0.6698529468419252 
2.2545889284504312 3.8195377429465407 68.7945038804114 0.6566448515183538 
2.0649625111064975 3.528090877367602 64.20811248383377 0.6566448515183538 
epoch: 11, train time every whole data:214.85s
epoch: 11, total time:3404.43s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 32.94s
test time on whole data:54.81s
1.7422957650972086 3.088873852039857 55.05646327060142 0.7791819188565556 
1.7889362589087159 3.197603905185114 57.247066838091044 0.7553767580751977 
1.8429651666748381 3.300514070395457 59.1011489899459 0.7317366434943128 
1.8667673029967895 3.308017919335602 59.92828550139073 0.7309099639602795 
1.896981090997745 3.3279067667186335 60.819752974409916 0.7227435725690923 
1.9217342343549466 3.3501266606881193 61.360376741495685 0.7204154988820338 
1.9581799642991806 3.429128989560649 61.89200345399261 0.7084954925124459 
1.9911064256917508 3.5124042466192735 62.284939947985016 0.6947410905162539 
2.036101810739952 3.619467559824036 62.452282326941635 0.6796197271432706 
2.0640816295588653 3.6762373872235927 62.68857774256605 0.6712079437282955 
2.1040221736828486 3.747620515812797 63.01523644155529 0.6617095406448834 
2.129743865266531 3.811213996619479 62.91994056266662 0.6461515694219155 
1.9452429740224477 3.4542448258572875 60.730645105781036 0.6461515694219155 
epoch: 12, train time every whole data:214.12s
epoch: 12, total time:3683.90s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.05s
test time on whole data:54.96s
1.7485686416984314 2.9486850973947853 54.84041439475909 0.7873586118692582 
1.8338170174614836 3.142694846860188 56.69976789888464 0.7603770897872341 
1.911393660040661 3.322208711293178 57.63081440517348 0.7373323592146644 
1.9538716887190406 3.4014329604916504 58.688345580936286 0.7257685367348303 
2.0080017945385937 3.507852462914384 59.708888186572175 0.7061924282945298 
2.053851630629351 3.5833837553672288 60.24114545664545 0.6964180870790991 
2.0934460490779685 3.657884282953187 61.06398252593115 0.6852256682922003 
2.138976045132304 3.746657928395917 61.98100343229031 0.6683524276675785 
2.1945629915991534 3.8582428518261067 62.84580208320296 0.6457122177371356 
2.2374193456825755 3.9410699291832993 63.79990768979873 0.6287446004547755 
2.2927861927123296 4.024208475020379 65.21923244464867 0.6152029072156662 
2.340366090733558 4.099077329223146 66.22444576270843 0.5984132496802405 
2.0672550956687874 3.618886587539047 60.745499402875524 0.5984132496802405 
epoch: 13, train time every whole data:214.08s
epoch: 13, total time:3964.76s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 34.46s
test time on whole data:57.52s
1.8334620318240709 3.1424654557687512 57.85557785314948 0.8000153784785737 
1.9006216390102746 3.2779814183465774 59.611626558165135 0.7775344169325217 
1.9615450827452754 3.4040072144864566 60.96437454493915 0.7575670796415042 
1.9923291262899125 3.4316342158316466 62.21108322923109 0.7549818628688884 
2.034690303162095 3.477269449090198 63.97245625220032 0.7463529610406395 
2.0590431251985333 3.495352656967744 65.28512085878836 0.7377632354591799 
2.084364813151991 3.5309350065119216 66.91664103122751 0.7235406092240518 
2.097987639697978 3.5522381458779284 67.9378240791045 0.7122789224117915 
2.127855967605664 3.615643456673479 68.81222037495914 0.695912751776525 
2.158251154574255 3.68756018981788 69.65510780294267 0.6810617644135426 
2.2092101743994723 3.7917141906901675 70.61443812684793 0.6636979696466422 
2.245299494068094 3.8852687839045137 70.80067256030968 0.6446225931013562 
2.058721712643968 3.5298938664053856 65.38668358009471 0.6446225931013562 
epoch: 14, train time every whole data:214.11s
epoch: 14, total time:4247.69s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 32.94s
test time on whole data:54.80s
1.699279123436837 2.7920411208619735 61.54609792731661 0.8023007482141183 
1.763139569632709 2.912042598053576 63.564511075140786 0.7824007327950013 
1.813674377199617 3.0184589572973395 64.66137827310352 0.7657723314931366 
1.8464335491910933 3.0692409096923163 65.25455852887853 0.760184926450109 
1.8777268174044965 3.1279249751736056 65.34222709525767 0.7540117698246781 
1.9173336251359432 3.1981103815519454 65.33900691953122 0.7481396687784267 
1.9754316262490161 3.3230060310521665 65.59208222023042 0.7351165864616045 
2.0220880015944798 3.428467334961622 65.65973679546245 0.7231179824357736 
2.0710560678938137 3.5429174083154082 65.7246772321919 0.7078435707703923 
2.1221526487620457 3.6448012204596827 66.42491940838443 0.6934553783305987 
2.1725464348647567 3.752830580988423 66.76510323214353 0.6773802392177869 
2.221333278929815 3.852401055268373 67.2508766998504 0.6573342595871773 
1.9585162600245518 3.321357314454148 65.26050551754534 0.6573342595871773 
epoch: 15, train time every whole data:213.91s
epoch: 15, total time:4528.18s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.35s
test time on whole data:56.41s
1.8469548127472046 2.783103651412188 66.20303263641196 0.7996697130297319 
1.8818657903483227 2.8994554118432943 66.16420894466633 0.7767012595623981 
1.8765881011511776 2.9618112286472704 64.13150360117173 0.7648545482447958 
1.8888710490070462 3.0159064081010833 63.256849576988394 0.7557075223954254 
1.9102892668788276 3.102885870633921 62.059890064945264 0.7426751608219014 
1.933109199806161 3.1699532818861074 61.2404716604602 0.7322850157022521 
1.9495371617617174 3.215434201853634 61.207833724077254 0.7203160570904723 
1.965954598401895 3.2460716315416525 61.19276553453854 0.7117095332242012 
1.983233021636538 3.2735482653085546 61.11598303146032 0.7049808088479252 
1.9936724043011311 3.273732943558163 61.25565030329201 0.7055274504826283 
2.0098602615026313 3.317667242851861 60.67627982142037 0.6979857970780587 
2.0658318929538306 3.4347927920599766 61.12967790334769 0.6732889360670609 
1.9421472967080402 3.1465747330116653 62.46940224100699 0.6732889360670609 
epoch: 16, train time every whole data:214.18s
epoch: 16, total time:4810.46s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 32.99s
test time on whole data:54.88s
1.7514533773019378 2.881872548068257 58.225301365599414 0.8179319298225421 
1.8279360191043288 3.0584007635292174 59.605096525785825 0.7975503849634414 
1.8982741802248748 3.2334883539908597 60.1718885336829 0.7803549290066124 
1.9509523881051156 3.3465885028419486 61.52458513611445 0.7626210283432793 
2.0206691548729405 3.4981483688681756 62.66941635426847 0.7439875952337847 
2.079417389933641 3.6246511361380915 63.406050308619 0.7259711137328398 
2.1260517973365882 3.7185077570988723 64.19838502591084 0.7092882193957769 
2.1593714366902907 3.7810894986855415 64.71583975289224 0.6968526714363595 
2.162803059555679 3.805643042605832 64.37833340365904 0.6878009454647288 
2.169544269179127 3.826276271782557 64.35944701526849 0.6797573063757504 
2.185807062389329 3.8594244876490285 64.37039145369786 0.6736943303377794 
2.220597472487727 3.931660358603601 64.64098438322397 0.6582139686877445 
2.0460731339317983 3.5624149275155936 62.68894100501561 0.6582139686877445 
epoch: 17, train time every whole data:214.01s
epoch: 17, total time:5091.39s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 32.92s
test time on whole data:54.78s
1.6393557558311593 2.740445982647438 53.71695182010334 0.8181270969407387 
1.7000121530549868 2.902787287253035 54.8356372150056 0.7981032369982808 
1.7656121804857892 3.0808875956007915 55.148596001136205 0.7818263408132861 
1.8100129327950556 3.1687216841750576 56.00016119247704 0.7719431009880653 
1.8707811558071878 3.2973123497063805 57.29865417587759 0.7536023173898387 
1.9170746791699813 3.38500302671577 58.29910274285011 0.7348007939271205 
1.9490798180177809 3.4495418284273534 59.65851601731692 0.7193270886841481 
1.9746318896388013 3.497351774145566 60.52630388795232 0.7113423657320272 
2.0074630495504966 3.5689938846541973 61.073152184150146 0.7012194997225863 
2.0494081415641343 3.6542085711771164 61.53067070055667 0.6931939834417882 
2.1076187580637633 3.7762400400064777 62.0370529657749 0.6837203775765963 
2.1740375188856076 3.9200448661128973 62.56467096250391 0.663472828018001 
1.9137573360720619 3.3868360900499024 58.5576339249699 0.663472828018001 
epoch: 18, train time every whole data:213.25s
epoch: 18, total time:5370.43s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.00s
test time on whole data:54.90s
1.6365791295833354 2.6866506048091905 56.10905391934507 0.8140553255940737 
1.7010316985883707 2.8341789783871656 58.51544985929122 0.7899354085894889 
1.7274717445655592 2.8752633563766845 59.89453954728323 0.7827206256396455 
1.776338764982298 2.9669859461230033 62.002721340150025 0.7667909142911397 
1.8210717709291548 3.0705585522888645 62.426025345916855 0.74955274808035 
1.8599289997729163 3.1589554053119615 62.31179960119544 0.735547094117102 
1.8922830689749015 3.2568441025774626 61.467137012323626 0.7209690123385396 
1.9264467279257342 3.3501841970264596 60.63570786138002 0.7097941466409021 
1.956932888431031 3.438928309316653 59.50085706924252 0.70177197355922 
1.9901044920411493 3.51168903446818 59.33855047135004 0.6947646917439949 
2.0174982205321568 3.5657556140566244 58.96073345497349 0.6895736198347582 
2.0826955802879694 3.700135540480887 59.56377679849765 0.663424405636102 
1.8656985905512147 3.216140383203656 60.060558258576535 0.663424405636102 
epoch: 19, train time every whole data:214.47s
epoch: 19, total time:5650.96s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.98s
test time on whole data:57.05s
1.6750018074786557 2.7364668505613583 57.84389313189159 0.8077892447427321 
1.7317877762921687 2.8759534690371136 59.22698963421739 0.7854207112712355 
1.771812022032837 2.9942335992548323 58.908978500656985 0.769844710446213 
1.8036686774543709 3.0550200299680483 59.722464844432444 0.7591875176384689 
1.8442294660308176 3.1382459575415513 60.10628636519536 0.7463339079501574 
1.8773478403632484 3.2070509525043445 60.03718969641058 0.7336730805652495 
1.9090397902602951 3.2921694471623097 60.04153660520032 0.7160421336119517 
1.9228165807552813 3.330651776499355 59.318618693870874 0.7101938347885538 
1.9295720322558745 3.3528161589151564 58.46187434494154 0.7073654574937314 
1.9326203667349404 3.3686316970032233 57.968693119222166 0.7053583957071319 
1.9524658358558302 3.4183048917440972 57.52655516565375 0.698897625511243 
2.0035073025647017 3.521820465072373 57.817284485428864 0.6853515486092198 
1.8628224581732518 3.1989634944316245 58.91502048121981 0.6853515486092198 
epoch: 20, train time every whole data:214.05s
epoch: 20, total time:5933.92s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 32.91s
test time on whole data:54.76s
1.6945278351430204 2.6594584019280427 60.406223141785155 0.8159164587627374 
1.745443334708966 2.8002659296266756 61.57589779646004 0.7939565685709851 
1.7481640157959468 2.8238563943913717 60.57483361352992 0.7889887858243547 
1.7537124319279656 2.8526688014566006 60.01422507926763 0.785713477480707 
1.7788638229433091 2.918556196589977 59.36553732322369 0.780271631634438 
1.8120679360632563 2.998844897361143 58.71587755972888 0.7723761161196254 
1.8419513025509993 3.0745650373150206 58.78302470667439 0.7621658105067821 
1.8660381090481366 3.1229334580356802 58.8178316542504 0.7580724915695644 
1.8841754654150102 3.1905690476660857 58.54693338704672 0.7514325459388024 
1.9136065081838696 3.2595477048437878 59.18790198950033 0.741428977017816 
1.9468246697184763 3.340825728521884 59.910520428094785 0.7270802470468961 
2.00698865126233 3.478683776072635 60.76597212239482 0.7012572332107002 
1.8326970068967738 3.0524516988887505 59.722026606376 0.7012572332107002 
epoch: 21, train time every whole data:214.13s
epoch: 21, total time:6213.00s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.01s
test time on whole data:54.91s
1.6783537210948942 2.809682008290585 53.18953694297003 0.8074167613835912 
1.750351519103295 3.0021916278699443 53.29511878187498 0.7854809659279571 
1.8101040870219114 3.1443092679468228 52.86528180194103 0.7734397580427071 
1.858764171890737 3.249915170004976 52.66355763682371 0.7685629340121236 
1.9147316674615833 3.3809486496568506 52.48653009733363 0.7606176390296352 
1.9470698972164342 3.4210818784337063 52.10328241483141 0.7607271943443755 
1.9451293948784116 3.3831364986590486 51.97106901326526 0.7649161551898676 
1.9230914008944695 3.300872859910342 52.02372852936753 0.7722393882966787 
1.8966817247143815 3.229107524591806 52.39845040678925 0.7706924724967951 
1.8863190668400023 3.2011430803819003 52.877059485257405 0.7664178478659442 
1.9026038630330668 3.235449341427559 53.26782133146162 0.7608942861417991 
1.9544859087347453 3.358635519459965 53.953790251528844 0.7450464607248738 
1.8723072019069944 3.230777816038514 52.75792832611208 0.7450464607248738 
epoch: 22, train time every whole data:214.04s
epoch: 22, total time:6493.62s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.00s
test time on whole data:55.86s
1.659432779883611 2.695943878878418 58.04177468594827 0.8103132141031704 
1.7093977658686539 2.801519930343231 59.83265444944899 0.7940702607168981 
1.7247053252999627 2.8363491221929213 60.593613286536865 0.7889389592340902 
1.7526783310209535 2.8796691645636443 62.20396434550071 0.7809773601754056 
1.7887004175940973 2.9356469317318856 63.36720692644114 0.772572980897629 
1.8313606927365598 3.0201437285047565 63.745439692648056 0.7597335121377639 
1.8761884073816417 3.1499799145357206 63.84557723409057 0.7395403476509694 
1.920098630170737 3.271981293807325 63.84381115850108 0.7191130562527487 
1.9558627724950868 3.366844197591785 64.19782117768482 0.701356860343283 
1.9895914514919832 3.4490087861896868 64.62676661174615 0.6844854615301583 
2.0164233664180196 3.5098427513937698 64.53246370545237 0.6723866638101117 
2.0627261334992943 3.60166792174469 64.91824339055874 0.6531580507066836 
1.8572638394883834 3.140379935509392 62.81256636586233 0.6531580507066836 
epoch: 23, train time every whole data:215.33s
epoch: 23, total time:6777.31s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 32.98s
test time on whole data:54.86s
1.6220779381936328 2.637684543591363 56.327255578838844 0.8201290973618434 
1.6602974875142709 2.7300388593325478 57.442711300617034 0.8065091158698331 
1.6789420877490193 2.7861254885289344 57.433585409904374 0.800341743067545 
1.701924516293442 2.822043445688329 58.10274178588051 0.7954640997006092 
1.7418529734111259 2.916671063548711 58.6431272554856 0.7823862522777808 
1.7895797860942604 3.038062616526234 59.266371989086096 0.7640749207043122 
1.8374781312171724 3.1635718330723326 59.80997472931021 0.7448115616468414 
1.8732975169996775 3.252190575827261 59.82768868624804 0.7329547935070422 
1.9039476488240596 3.3260710647169858 59.719110925011464 0.7254898976523233 
1.931551354359658 3.39820267505292 59.80041203836017 0.7173161890192239 
1.9608678154057513 3.46684192033189 59.80095461047963 0.7097708723175726 
2.0065827075820417 3.5634183786048044 60.23869730062057 0.6935140183282438 
1.8090333303036759 3.106258666002237 58.86779191574504 0.6935140183282438 
epoch: 24, train time every whole data:217.65s
epoch: 24, total time:7060.67s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.00s
test time on whole data:54.90s
1.7305601118610552 2.83196524210655 57.5515842124695 0.8264348926789111 
1.7678934071771801 2.8879817589503674 58.95691291150992 0.8164891486302882 
1.7978695704290377 2.951600485062566 59.46212056233728 0.8082760531212093 
1.835462660985245 3.0300030757354617 60.52462033918431 0.8001044764743064 
1.9082718162175623 3.1800305133589797 61.60158212782919 0.7856287682935282 
1.9687856081742794 3.3176016242405817 62.210872729186306 0.7680772609305444 
2.0173970084824555 3.4132777886071812 63.43789142081021 0.7528552681534953 
2.043710922528147 3.441553620755092 64.39760804206568 0.7478154450042029 
2.0520442998405724 3.437801807638189 65.33281736330966 0.7457734324364859 
2.0602900996610876 3.433763532803104 66.17046636326394 0.7439722086217326 
2.0737354311972322 3.4535037638314723 67.0784072786799 0.7375360062583546 
2.1212083134091504 3.5443170905340797 68.08812741038307 0.7231215485732099 
1.9481024374969171 3.25275839580253 62.90127200679731 0.7231215485732099 
epoch: 25, train time every whole data:214.11s
epoch: 25, total time:7340.88s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 32.99s
test time on whole data:54.85s
1.6983513539185127 2.8915343896320396 53.9674859532924 0.8198158854747489 
1.722978092592653 2.9391668338303307 54.622806613659534 0.8108228779427534 
1.736449053137785 2.9201420548359165 55.659261309870786 0.8073132921862995 
1.7424597739283705 2.913263441648846 56.4359141887794 0.8073781572144914 
1.7714654018747664 2.974427286320094 57.51588525259604 0.8001685415118726 
1.7984183641193168 3.0312701907381627 58.07559127833929 0.7938221503898837 
1.816812301181523 3.07403286309108 58.69214109591066 0.7890154094303291 
1.8409774477469425 3.1255942761927415 59.119419026843055 0.7812244558744662 
1.8633140750074138 3.1956782094096763 59.73654318413579 0.7658007207505914 
1.8893264846294409 3.268251863773323 60.17777631211367 0.7504783293074776 
1.8928674948393767 3.28971125770965 59.86872326425584 0.7478012704949805 
1.9238118334019645 3.3598571336082097 60.30539390681575 0.7343042497645157 
1.808102639698172 3.0859022202927306 57.84820812459858 0.7343042497645157 
epoch: 26, train time every whole data:213.98s
epoch: 26, total time:7621.05s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 34.62s
test time on whole data:57.70s
1.680915924320707 2.637817708566076 53.11208171048183 0.8290445185275396 
1.7218089302670032 2.7635591649203475 53.32287938910588 0.8159853702471784 
1.7529340957706527 2.853063602097771 52.968863033799266 0.8065327834654179 
1.7588904542418287 2.8530205846379846 52.713243073339186 0.8053609706302302 
1.7604538499677465 2.8441894077162084 52.69909007797986 0.8040396987817834 
1.7584364771432288 2.8041617440735913 53.05962964919356 0.8058772499695626 
1.7634169799559527 2.801967364502151 53.84016800736583 0.8011856381762377 
1.7843284467585563 2.848704836803718 54.52593585112627 0.7908543237755271 
1.8132591370981008 2.9123623645362904 55.21350925557365 0.7783700310852738 
1.8524761845920057 3.0045542440016146 55.99904042389762 0.7616345456528797 
1.8778796616162041 3.063180114062195 55.817521669282655 0.7524042478667216 
1.9292326589114077 3.190841291284616 56.49827225827926 0.7290314543711909 
1.7878360667202828 2.8848405217814412 54.147583911256724 0.7290314543711909 
epoch: 27, train time every whole data:214.88s
epoch: 27, total time:7904.26s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.33s
test time on whole data:55.24s
1.6850983745812305 2.7476838726614794 52.952003459424915 0.8202050225356675 
1.7306055325651986 2.8504274554724556 53.752607590855014 0.8018223741744747 
1.761263522685195 2.8869459651934113 54.754727270371575 0.788165532385103 
1.7650613972811648 2.8713219655141806 54.84357100834546 0.7886485025608082 
1.7806598742082715 2.8905305690892527 55.02007910386658 0.7869738379069151 
1.8061798207854762 2.947752819414694 55.02859527806206 0.7799086727329534 
1.8348682879347886 3.0298874286373825 55.543714638005646 0.7676809674776142 
1.8552492311627027 3.0771199421558553 55.903981145831864 0.7609532211209449 
1.8691969263754076 3.1113594238593776 56.52483115804316 0.7523989373911703 
1.879227606193473 3.138881814613101 57.47701344777053 0.744863117043183 
1.894234263092102 3.17684178039256 57.891119159150406 0.737059880289133 
1.9368984409187522 3.2838663603359377 58.69817121762983 0.7161502978512531 
1.8165452731486469 3.0049429734350728 55.699286154228524 0.7161502978512531 
epoch: 28, train time every whole data:213.95s
epoch: 28, total time:8185.03s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.29s
test time on whole data:56.38s
1.6234035389959103 2.725979698489552 54.78449845703977 0.8163657043401337 
1.6686199087230933 2.832123659000417 54.968475056318354 0.8051453484145856 
1.6922579224617886 2.859893743255391 54.971745906963385 0.8044559196897267 
1.7010861607554946 2.8576227286161258 55.233601847997335 0.8067164068079665 
1.7323364011743771 2.916723426050903 55.67463249529927 0.8007238677208189 
1.7744567283907284 3.009949617940857 56.23601941638354 0.7855794468447167 
1.8171986593665288 3.1105231802348077 57.32410130571516 0.7661845875467385 
1.8432898304245124 3.1497345263283636 58.23413765291986 0.7584393620710962 
1.8666020933240652 3.1962477332705466 58.57864301549185 0.75258504982884 
1.8902502376007893 3.24295165856315 59.508989620900074 0.746176287352907 
1.9237866206665835 3.3098478728535548 60.60538519087761 0.7370409918932799 
1.988685291423684 3.4298035892910663 62.18417426369781 0.7203239751880227 
1.7934977827756295 3.0606579431657464 57.35882246567817 0.7203239751880227 
epoch: 29, train time every whole data:214.05s
epoch: 29, total time:8466.12s
fine tune the model ... 
epoch: 30, train time every whole data:424.66s
epoch: 30, total time:8890.79s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.90s
test time on whole data:57.99s
1.5417141920912656 2.5360594461082013 55.20532672648629 0.839593821693039 
1.5715728419900294 2.6110750443275865 55.54475049512815 0.8319351933038264 
1.5975172282189485 2.6582722863760067 55.53735387784039 0.8282081214259844 
1.6153821842391043 2.688218047458628 55.67011439929151 0.8263848106934226 
1.6468331446909301 2.7499695482679605 56.267342516563325 0.8199941464235834 
1.6804069528534475 2.8171713585701985 56.72235957623791 0.8114872111235762 
1.7180494531344268 2.900405456208809 57.61449222458743 0.7995598737801156 
1.745514489599814 2.9569010127357984 58.30083153908778 0.7903544352403872 
1.7696724478138521 3.0063694948918975 58.469496462030534 0.7826192974270072 
1.7908986010963008 3.060553360820062 58.64902540455527 0.773810561139476 
1.8092041359508322 3.1067596182830286 58.997721839572726 0.7662790585572724 
1.8478873864339576 3.1945681774887893 59.5644357015896 0.7537551550202604 
1.694554421509409 2.864424115923897 57.21202498572124 0.7537551550202604 
epoch: 31, train time every whole data:427.05s
epoch: 31, total time:9386.92s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.97s
test time on whole data:55.92s
1.5369496106152378 2.5306449999813503 55.097748562014125 0.8423733853238002 
1.5688253421129748 2.6113506063086014 55.434173736075785 0.8341363779402708 
1.5959796956743868 2.667770671399302 55.6500442800304 0.8287853876020179 
1.6112527976122109 2.6835939912750066 55.98494393512928 0.8285241658060504 
1.6375775586023396 2.732943462565914 56.7385221059058 0.8228023843748968 
1.6673256276124822 2.794721791362706 57.37134004442499 0.8143964206435912 
1.7047509678864763 2.8766449250626054 58.362309712469575 0.802819116410816 
1.7339484754665977 2.933713205322187 59.14866953004319 0.7943735644171883 
1.7644834011992705 2.9970713718767903 59.37750019360859 0.7852768631159328 
1.7931587126770367 3.068165023021827 59.613049572991784 0.7745233982129146 
1.8171402053517245 3.1276771402626338 60.039583050595205 0.7649965983649342 
1.8619173712799593 3.2254840392430895 60.80908866240866 0.7500143896771333 
1.6911091471742248 2.861896038535255 57.80236089985794 0.7500143896771333 
epoch: 32, train time every whole data:428.18s
epoch: 32, total time:9882.35s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.87s
test time on whole data:57.96s
1.53461426041701 2.505588989235162 56.34090519639652 0.8422327124657454 
1.565904183433613 2.582546512110778 56.74054309840359 0.8338422465127243 
1.5914153686603087 2.6373368784472713 56.92005992362016 0.8287590779263929 
1.610025316198312 2.6660563973893496 57.28371735582757 0.8272454780636058 
1.6414554867066797 2.7269783329497543 58.02795073686424 0.8204799549264512 
1.6741577387271183 2.794108343625028 58.64370645466358 0.8116009168390451 
1.7112828744588686 2.8783753258835203 59.54673090580999 0.7993429617225618 
1.7369993902329532 2.927625736588588 60.26273453404081 0.7918359588149045 
1.764079107245874 2.9884186684244374 60.49577144900174 0.7820654673294277 
1.7908748498231706 3.057417707617926 60.751218543330424 0.7713385525361408 
1.8143570398705169 3.1174357365976633 61.12726175066169 0.7622396135072671 
1.8582798204332414 3.21582108912652 61.7685306111955 0.7477155038807476 
1.6911204530173056 2.8496896681171635 58.992535861148646 0.7477155038807476 
epoch: 33, train time every whole data:423.60s
epoch: 33, total time:10374.27s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.11s
test time on whole data:55.06s
1.530661740867687 2.5143790575912437 54.95001889065302 0.8442166023098633 
1.5618991959468418 2.5935157092521552 55.29741963055754 0.8363007373878758 
1.5899279647942277 2.6551615847879435 55.52600202011554 0.8301226219863767 
1.6092950740059218 2.6846239260478386 55.75508210175508 0.828834640709364 
1.6405539234608766 2.7477944313186193 56.45701707863269 0.8217768823975481 
1.6705815115312912 2.8051134722252926 57.050347384816014 0.814099969491246 
1.7047116124769228 2.87738208136072 57.99587825634883 0.8028397873117329 
1.7272288060298278 2.918573814220297 58.82763367106922 0.7954109735627862 
1.7499571189337544 2.9698190141316374 59.0364747414727 0.7865579778312737 
1.7704608288814447 3.0211250285400495 59.2846635983374 0.7780392709174276 
1.7921462656497247 3.081667614750311 59.76289283932593 0.7674610947004465 
1.8305635724526252 3.17109206012476 60.42726673286649 0.7533842463394937 
1.6814989679192622 2.843435020379151 57.53099967756417 0.7533842463394937 
epoch: 34, train time every whole data:428.67s
epoch: 34, total time:10871.05s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 34.41s
test time on whole data:57.51s
1.5318340871987777 2.522171686865807 55.042145582899074 0.8437776440447599 
1.5638437580471591 2.6029442461510173 55.259225827538536 0.8357844014019219 
1.5929342293277204 2.667566459735044 55.25133325088717 0.8306415166225548 
1.6124006996430633 2.7031735871344984 55.37157113764073 0.8294409025474755 
1.6450255642415335 2.766836809620174 56.00890420835947 0.8228947200781692 
1.6733064692396493 2.8236367503486295 56.646802950549905 0.8146311686395936 
1.7023340178279296 2.883029284990718 57.59275696812476 0.8044790510214176 
1.724251863300889 2.921628149084478 58.4789866917187 0.7966338448746322 
1.746799810227805 2.9712749475121107 58.78836923506828 0.7875043319218682 
1.7691996704348851 3.0268960545074184 59.10729411734641 0.778326201114451 
1.78935942734228 3.079929255007747 59.64421701636958 0.7693208764043762 
1.8252374913973646 3.1532300363836643 60.30603525667328 0.759205805259215 
1.6813772573524215 2.849783687276675 57.29157546671999 0.759205805259215 
epoch: 35, train time every whole data:425.07s
epoch: 35, total time:11364.75s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.08s
test time on whole data:55.01s
1.5208560098234032 2.473772919301802 56.151515733926075 0.8454810097358689 
1.5491190654219438 2.545281690644079 56.1151305339418 0.8382560898908817 
1.5749742736969852 2.608307969291891 55.93328473209187 0.8325832160105545 
1.5930440278947353 2.6419402786128288 55.94890263510731 0.8310937423875248 
1.6230627992655196 2.7048505014506863 56.400216930987824 0.8245703131489093 
1.652371808113619 2.7597929179853047 57.00063113002493 0.8169859265574227 
1.6857989539508487 2.828748185782971 57.905766875321675 0.8064017328025077 
1.7097716120826525 2.873525890794433 58.73110970902323 0.7984847138995834 
1.7340887382915686 2.929919481023912 58.97870932567885 0.7885974593871373 
1.7558519881380101 2.986171491732179 59.241013570699195 0.7789892093238079 
1.7754074180324873 3.0379760002283644 59.74362319544606 0.769773122991528 
1.80712564875354 3.1073727372017017 60.36089536204242 0.7593001279694256 
1.665122695288776 2.7982060118584466 57.70932033483292 0.7593001279694256 
epoch: 36, train time every whole data:423.94s
epoch: 36, total time:11854.72s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.15s
test time on whole data:55.11s
1.5302060441464362 2.517353688143399 54.85239761521535 0.8447064767512005 
1.5617170965461327 2.591964278897375 55.224867789197376 0.8372547576410243 
1.5899963443906358 2.6503292422281692 55.58249310305789 0.83169310399588 
1.607241865323679 2.678978400825907 55.962139342809294 0.8299001171086596 
1.6365051264645798 2.7385960270274463 56.82561428993947 0.8225238228015516 
1.6649347346888943 2.7912320939339033 57.61127934047582 0.8148363262281665 
1.6995396949568142 2.8640101783007297 58.66652355192832 0.8037417327705282 
1.727899142000558 2.9182429754380568 59.597818837046 0.7949245824933245 
1.7555880863139672 2.98388726945256 59.899517762192886 0.7839295320694184 
1.778316725989981 3.0395841088527225 60.12808635929413 0.7741233422803894 
1.7950799162154574 3.087023773412174 60.676645417293265 0.765046735460847 
1.824984255757715 3.1470971754837302 61.402633361491546 0.7553636621901573 
1.6810007527329043 2.8408140140091933 58.035966126044336 0.7553636621901573 
epoch: 37, train time every whole data:423.41s
epoch: 37, total time:12343.68s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 32.98s
test time on whole data:54.84s
1.5330544281381049 2.497875823755016 55.72939915112889 0.8457202892998248 
1.5645308796719959 2.5681449072268516 56.00082425916123 0.8388956228981713 
1.5928528903874435 2.635170019072741 56.27809974514223 0.8320214706452557 
1.6118394534584313 2.666397588919683 56.56311276091355 0.8305132522169131 
1.6441788506632937 2.728438190620463 57.27048338519333 0.8239661891073078 
1.6738619414442883 2.7881484770393294 58.04769267305279 0.8157361613127538 
1.7080165615956344 2.8623433223932864 59.107503269420214 0.80432390507645 
1.7355042848082347 2.912819948355496 60.037038085465774 0.796050035575466 
1.7621006045858598 2.9774787711623474 60.35880686946141 0.7852601413913369 
1.787081257921422 3.036725759710104 60.56622644145967 0.7758942347917672 
1.8045948666646368 3.087826237845178 60.929121786117705 0.767856204238568 
1.8367632497015098 3.154469858475538 61.497393016921365 0.7587501782511104 
1.687864939086738 2.8336763429544543 58.53226256586185 0.7587501782511104 
epoch: 38, train time every whole data:423.67s
epoch: 38, total time:12834.55s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.10s
test time on whole data:55.03s
1.5247766164691143 2.486225795693734 56.178020319888525 0.8447120572438841 
1.5531514528546304 2.5550059869870707 56.44456714124579 0.8371137853209699 
1.5764985260334576 2.6081223764286254 56.71929773962889 0.8311724819549916 
1.5898258800482643 2.629419807689781 56.879994194561114 0.8300961843897681 
1.6184814402306718 2.686810570074035 57.57479804704645 0.8232222674083174 
1.6470853456932875 2.746716792727734 58.352629781210396 0.8146570582510984 
1.6816521086422282 2.8178894852683283 59.38188720617785 0.8042413065945415 
1.7112383685692967 2.87080227957238 60.24328635249867 0.7968389085764657 
1.7412769124557574 2.9472406896947803 60.445870129505444 0.785556199031389 
1.7692925435344555 3.0167733978889855 60.56561836854759 0.7755437874704709 
1.7895590725985489 3.0719784040647378 60.82303619892573 0.767531597302871 
1.8200732472421868 3.134888841326893 61.28588826231459 0.7590072305696113 
1.6685759595309917 2.8050945467492534 58.74135154810022 0.7590072305696113 
epoch: 39, train time every whole data:427.57s
epoch: 39, total time:13328.75s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.05s
test time on whole data:54.99s
1.520043930707056 2.48319596553796 55.66578468670021 0.8454509850080469 
1.5488305000075626 2.550372349962361 55.94455130741024 0.8382709331215237 
1.5779268111008264 2.6216836127188388 56.001086155028275 0.8315981451418423 
1.5988011859226086 2.6673022747224375 55.99155833631075 0.829317378552648 
1.6365643862219794 2.7423303214153756 56.569709745652695 0.8226122844118613 
1.6719577707312114 2.815717718187543 57.269085270327594 0.8135821420190449 
1.7102117647722896 2.8953442831180296 58.257816716973046 0.8021031095024463 
1.7383400856701745 2.9469617265683112 59.17245701706497 0.7940586988214129 
1.7639230580725485 3.0022753357227434 59.49832352150587 0.7846525188720491 
1.7879641297135858 3.0645558714261285 59.73330484578059 0.7747482774221335 
1.8043688510571207 3.1072840268954023 60.100280070358934 0.7682124452165605 
1.834627819597455 3.1660092917908904 60.593083325268005 0.7610122115008624 
1.6827966911312016 2.846907975515627 57.899856516937454 0.7610122115008624 
epoch: 40, train time every whole data:424.23s
epoch: 40, total time:13820.60s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.07s
test time on whole data:55.00s
1.539945402177849 2.51273111169489 55.927346009654656 0.8452040078413586 
1.569724727279285 2.5772749320951225 56.36477114954095 0.8387333957152319 
1.5959365479067145 2.631772427091232 56.87092697645018 0.8327270328727773 
1.6134127565265766 2.6582188055499896 57.22249912644671 0.8312480708099099 
1.6475368341678487 2.723453783494488 58.124645196019905 0.8237592541694669 
1.6811702886012694 2.7911785439527317 59.06330458911204 0.8145063798392623 
1.7191296967725669 2.864705414307382 60.19400714349017 0.8041445347671987 
1.753128177349234 2.928547309322966 61.15614856247905 0.7945838358458717 
1.7846856876767816 3.005032507051072 61.4638497271814 0.782444050568306 
1.809250721269775 3.0659065365623484 61.560750006921715 0.7729816818997535 
1.824589344271769 3.105238544281044 61.98189825625921 0.7663640278231497 
1.854420786196632 3.1583566077449663 62.71569459393515 0.7587326391464345 
1.6994109141830251 2.842930527468741 59.387292660932644 0.7587326391464345 
epoch: 41, train time every whole data:424.94s
epoch: 41, total time:14311.43s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.02s
test time on whole data:54.93s
1.5220797676285285 2.4814947545905546 55.944720241170884 0.8447271878523226 
1.5527500544797097 2.555315320446921 55.80366923821277 0.8380851246956796 
1.5839506937621073 2.629338759994874 55.81180306465654 0.8314971520846673 
1.6047799220762793 2.671750486798084 55.996711802696154 0.8284551285379431 
1.6362800336828722 2.7352097131828774 56.71627974011266 0.8211036268051733 
1.6651813919357956 2.7904915162266986 57.5595944874296 0.8130787505819451 
1.699666349967853 2.856998680219479 58.719553977786454 0.8024114775370632 
1.7263123598104964 2.9071002530728367 59.75764595121771 0.7936863845190476 
1.752964780924487 2.9701196960624294 60.15007040249299 0.7828883574243446 
1.775071180637305 3.0304634644137187 60.488341513272836 0.7725015397346425 
1.7934008088093252 3.0805345171987923 61.11449123698469 0.7632956927452401 
1.8210361040278915 3.1375910943707637 61.86360280015669 0.7540961737510439 
1.6777894539785543 2.8278452427763767 58.327331525976625 0.7540961737510439 
epoch: 42, train time every whole data:424.03s
epoch: 42, total time:14802.30s
predicting testing set batch 1 / 168, time: 0.33s
predicting testing set batch 101 / 168, time: 34.05s
test time on whole data:57.19s
1.5156397131156354 2.4748188028343656 55.587972809830596 0.8464530976702989 
1.5416248756573492 2.5312833899624496 55.97912984739103 0.8404480980604813 
1.5678474542652923 2.5928430145202306 56.298825095488326 0.8337487344185496 
1.5856557565036984 2.627235198762103 56.45076987461466 0.8313836488949935 
1.6166909222972712 2.6914093497969267 57.113511838479106 0.8244735924329041 
1.647833964203706 2.7607479708901312 57.84626306482146 0.8155441659991634 
1.684440580303204 2.8366856986404647 58.80527063155533 0.8045691167851369 
1.7125832420693976 2.8878036230449036 59.6377854930855 0.7970398039802539 
1.740534568235899 2.9576707786504186 59.922412842024364 0.7860674106376476 
1.763512934544168 3.016134305389169 60.10478801091877 0.7771801054798069 
1.780101980744462 3.0559335228499394 60.453843495969195 0.7712319251921975 
1.8086636022090734 3.111243043689286 60.97447720254322 0.763765012816145 
1.6637607995124297 2.8028613076019453 58.26469815681092 0.763765012816145 
epoch: 43, train time every whole data:421.82s
epoch: 43, total time:15291.80s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.27s
test time on whole data:55.43s
1.5193019646385773 2.4850574173153173 55.41264350339428 0.8453785763991447 
1.5483649808006095 2.5475503772073864 55.69558317906519 0.8389330943799975 
1.5755069421645964 2.6137348421480713 55.95773530362108 0.8319012325321224 
1.5955447392513356 2.6517850762242796 56.108018895500955 0.829428363722616 
1.6300830719003543 2.7248508900420436 56.756524513835316 0.8212979306875902 
1.6619584834685637 2.7925527334700857 57.56390775330543 0.8118924744291273 
1.6968788384032392 2.862666985926571 58.62079767009142 0.8010117198156101 
1.7224715501440777 2.9081971552529438 59.51118400574933 0.7937326459444796 
1.7473337007138345 2.9673580782107933 59.774644453162566 0.7841942451089106 
1.7694066521932503 3.0260483296793113 59.98696650449404 0.7747969107331794 
1.785883153720626 3.06373309512912 60.42903196099135 0.7686408465538612 
1.812280475364998 3.1075071710321565 61.01820773697412 0.7628751323372913 
1.672084546063672 2.819721869701883 58.069719323742675 0.7628751323372913 
epoch: 44, train time every whole data:427.46s
epoch: 44, total time:15786.24s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.04s
test time on whole data:54.98s
1.5223199865138601 2.4889548956365495 55.6799208389137 0.8447229182793458 
1.5496402507683351 2.5505180732632167 55.99586487430015 0.8380286886295378 
1.5756007837483748 2.613689241118975 56.23303721463565 0.8313761755779058 
1.5939977566067662 2.6470206434177235 56.3030427318793 0.8303913093627179 
1.6299091553269398 2.7214755026971806 56.9123832370453 0.8233269959988724 
1.6662395007243114 2.800989464308865 57.701759917293685 0.8138200147060468 
1.7045481836229917 2.8768766310202984 58.75370071716193 0.803171144897638 
1.7353363981855412 2.9347288648718264 59.7271156959067 0.7938848723097528 
1.7615204407038787 2.9989638918577244 60.074849364818306 0.7829245665643956 
1.7839360943237053 3.052606623930218 60.34429724184557 0.7744747989007675 
1.7999552587620204 3.092164074358016 60.71947561216627 0.7681679630228815 
1.8272494060243702 3.143199167503221 61.3479738479489 0.7611706440249305 
1.6791877679425913 2.8347916369082755 58.316234478446695 0.7611706440249305 
epoch: 45, train time every whole data:425.20s
epoch: 45, total time:16278.27s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.00s
test time on whole data:54.90s
1.5229352808664776 2.505699808298504 54.7359282591304 0.8463334330928323 
1.5563725486836795 2.585769470303897 54.990707904010826 0.8390634804166239 
1.590359682366518 2.6652795720640223 55.20236811646888 0.8319102639512095 
1.6134464513991205 2.7086026081718093 55.410610758132805 0.8290477179440294 
1.6485591242783482 2.780142390624006 56.150172857567405 0.8210514079239828 
1.6821002414955624 2.8526637532022847 57.038748074424895 0.8105495268377897 
1.7137021185900307 2.912756404601622 58.120071275196516 0.8005321657280549 
1.7399924325039167 2.9594962067797974 59.06968331918897 0.791973250403737 
1.7663724321218297 3.0230060280510695 59.45258918914534 0.7810419250336562 
1.790343643888299 3.0815932748602384 59.73002130721662 0.7715597579297371 
1.8058928542523867 3.117960251173466 60.07742295495889 0.7653015866771473 
1.8323745766098478 3.1656475666087216 60.63759834354072 0.7586537632729001 
1.6885376155880014 2.870720171573686 57.55145182423298 0.7586537632729001 
epoch: 46, train time every whole data:425.70s
epoch: 46, total time:16769.67s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.90s
test time on whole data:57.18s
1.5212296589626266 2.4864547719421393 55.1716441024519 0.8457762863224674 
1.5502717218930344 2.5529569702267674 55.50331894145077 0.8389539941162052 
1.5787065345966922 2.6220395988991516 55.785711196470814 0.8316170728316054 
1.598950596413708 2.6573631090736707 55.99960080328545 0.8293587537521517 
1.631060565933585 2.7181693136908187 56.72712061102515 0.822620502692513 
1.663494069690683 2.7842188686980993 57.63417439953683 0.8130868492176078 
1.6972117031429494 2.851516741154467 58.72955083998199 0.8017136121407655 
1.7216619684451393 2.8960937920006673 59.65111422966384 0.7934702028272816 
1.7465625632957866 2.9541192112293038 59.92944335267132 0.7833245351799517 
1.76610850489654 3.0068967828980693 60.09156640595046 0.7745463164259053 
1.779305743818127 3.040763762790848 60.322645891783054 0.7687804231734096 
1.801301579517268 3.080140368398301 60.848437042984415 0.763221134111157 
1.671322100883845 2.810653847528983 58.032982689912984 0.763221134111157 
epoch: 47, train time every whole data:423.70s
epoch: 47, total time:17261.48s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 32.97s
test time on whole data:55.63s
1.5218430560129206 2.4759798269963533 55.95903033648079 0.8448390071192179 
1.54932370545449 2.539137194903913 56.51828518679978 0.8378012245226537 
1.5774201246453006 2.610325931651592 56.94083144394856 0.8297575295136566 
1.5983097059749776 2.647538825226712 57.143329008455055 0.8268141349224368 
1.6322942432320366 2.712907941665018 57.84235842277581 0.8192082625619354 
1.6713728572419357 2.799417267845718 58.72326834054613 0.8070646656727221 
1.7089482818904909 2.8804598020527994 59.713050303795825 0.7944724273218794 
1.7363159379725832 2.929967372787203 60.66060150185349 0.7860357921341832 
1.7638478119161334 2.9901064479476944 61.11963493060244 0.7754999098622142 
1.7846592588131094 3.043964964270758 61.452506689832795 0.7662378597184816 
1.799257322591153 3.0784726630321235 61.75395856642047 0.7602421883739909 
1.8210192844120874 3.1212780200841452 62.19490527165831 0.7537669643449155 
1.6803842991797682 2.827029518837414 59.16860828893146 0.7537669643449155 
epoch: 48, train time every whole data:423.34s
epoch: 48, total time:17751.71s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 32.99s
test time on whole data:54.89s
1.5222537203811641 2.489186514812781 55.22690283859141 0.8460005422914051 
1.5518481123457175 2.563525804920002 55.54975691864401 0.8387852427183649 
1.582819952575756 2.6384289800730825 55.904871673441626 0.8309821895090249 
1.602395215824956 2.6727772381482864 56.08061388884883 0.8285704195693478 
1.6329606979603746 2.7313523190965268 56.769320658924116 0.822027687617992 
1.6651597746006612 2.799664763006557 57.65031646267729 0.8119876063474573 
1.6974904678615608 2.8627585272273666 58.7070470998323 0.8010673813699218 
1.7188073135082211 2.8960533551031986 59.59241790275759 0.7943388490747366 
1.7411784764345557 2.947314100493252 59.95475733646594 0.7849281513498645 
1.7618980538061864 3.0027693336121484 60.27512043354161 0.7754729189084784 
1.7763399729912301 3.0363868944761907 60.58767128966135 0.7697727336577862 
1.797990722832669 3.0782514311211506 60.935532437239004 0.7641669480682064 
1.6709285400935878 2.8159107574450286 58.10298304467562 0.7641669480682064 
epoch: 49, train time every whole data:424.91s
epoch: 49, total time:18241.79s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 34.74s
test time on whole data:57.84s
1.5133227310056487 2.462972975332784 56.1860667894692 0.8465639117677377 
1.5384150156088705 2.5203512051744483 56.54221303175042 0.8403673781229934 
1.5642971306311055 2.5841778297191964 56.899097619454345 0.8330323144847314 
1.5806352061663886 2.6120665199537094 57.03530008194361 0.8311080396748487 
1.6103656147951704 2.669041809311165 57.72848756451442 0.8247299554904048 
1.6429869540243276 2.739487372224159 58.614088960320366 0.8152345599072512 
1.6787477203073602 2.811963349342253 59.68136491943869 0.8039847982119815 
1.7079203395851488 2.8696444153465217 60.703112369194045 0.7940462949123649 
1.735797689937144 2.931424393422956 61.238891017072795 0.783232826819264 
1.7568788853562658 2.9861968873124485 61.75585556659633 0.7734822771546361 
1.7715766460155802 3.0195179306532824 62.36818437468019 0.7674645641680374 
1.7931912141097266 3.0571113694002534 63.033226792937704 0.7617865859095234 
1.6578445956285615 2.7788469447554776 59.315626168013736 0.7617865859095234 
epoch: 50, train time every whole data:426.79s
epoch: 50, total time:18736.67s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.06s
test time on whole data:55.01s
1.5155107198338955 2.4706376200293616 55.41137219979593 0.8465597015909839 
1.5420182485076643 2.530015859949521 55.639026015721186 0.8403220985881583 
1.5691537809305425 2.5986006618604818 55.74971260451834 0.8333550774861531 
1.589495474500493 2.637611064802041 55.818265302079716 0.8309306553309808 
1.6213934763442015 2.702982344885792 56.53782475443102 0.8239744239032778 
1.6542812025592264 2.771101336827818 57.413714913797286 0.8148140844233748 
1.6893709233077687 2.838884657049186 58.53918686110714 0.8042421200322476 
1.7188973928813247 2.893790441425521 59.473452086765036 0.7955355658998571 
1.7477196052228765 2.966246132440985 59.782491458175016 0.784113666841549 
1.7705258498255696 3.0306875381635696 59.894821359603114 0.7742798198091809 
1.7819392340225833 3.062368484162389 60.07298441896053 0.7698139324209853 
1.8021573085610179 3.096677554608641 60.52331165122631 0.7657505973579188 
1.666871934708097 2.807495163292742 57.9047948330908 0.7657505973579188 
epoch: 51, train time every whole data:424.13s
epoch: 51, total time:19227.51s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.02s
test time on whole data:55.38s
1.5138330547082282 2.477587522905894 54.964649587802185 0.8465642489223395 
1.5421238927959153 2.541750828109434 55.291227383034126 0.8398970167396584 
1.568068392695327 2.6024697046341734 55.66044085097649 0.8333713148151678 
1.5848713785486208 2.6306059698618425 55.92923309614856 0.8314425308973481 
1.6155128193559747 2.691004272053886 56.776090605649486 0.824640442206303 
1.6488715317675045 2.7629461068127985 57.71232306719284 0.8152061285699889 
1.6858431949352046 2.838447796227692 58.811370063197565 0.804429732622208 
1.7167206608239738 2.8977692900250682 59.603163807438584 0.7961728753734897 
1.7483944991706382 2.976065919267187 59.812284884665004 0.7850664171320509 
1.77318875121343 3.043667677668643 59.872491588651 0.7757479882139701 
1.7875411836050805 3.080440926986396 60.06833649256937 0.7707593229646008 
1.8126797329747073 3.1240075996504473 60.595168373364984 0.7646940812981322 
1.666470757716217 2.8135168168561298 57.924854233795074 0.7646940812981322 
epoch: 52, train time every whole data:424.04s
epoch: 52, total time:19718.33s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.07s
test time on whole data:55.14s
1.5125037350766362 2.4692491215176333 55.64510508423809 0.846703312884349 
1.5399057932565254 2.53518326058161 55.86028761837811 0.8400776182397771 
1.5692211437578358 2.6108707122909895 55.994852620486924 0.8330297804643257 
1.59204897987745 2.6582966017239333 56.112413705698 0.8296556815075055 
1.626590017104965 2.7271659667102877 56.82948382610009 0.8222727658894289 
1.6597536749804305 2.797737623833131 57.717511807753084 0.8123732234592547 
1.6905003693879006 2.8534610928410955 58.77993096196352 0.803230233376057 
1.7147442820760466 2.896507803472888 59.6885460961328 0.7954893943641057 
1.7408615269974939 2.960519623698027 60.04185479979848 0.7846470944717724 
1.7624715886133768 3.0197918754739708 60.20838015982165 0.7748945548877486 
1.7735829933354896 3.0467469187941667 60.45202177210277 0.7703926599686979 
1.7930848677301159 3.0805518701003995 60.901985384585124 0.7656639005040025 
1.6646057476828555 2.811591865217056 58.1861471286139 0.7656639005040025 
epoch: 53, train time every whole data:428.28s
epoch: 53, total time:20213.96s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.13s
test time on whole data:55.56s
1.5168424971859369 2.4866536196826177 55.49845358469805 0.8463001978900163 
1.5456998160968403 2.552616145934494 56.015344480603005 0.8391497621234312 
1.573655015192039 2.616601597586772 56.48376662505673 0.8328082226278083 
1.5948128651408922 2.6562719602916127 56.81931974819114 0.8297265994860303 
1.624868614475376 2.713413174785689 57.704275885107414 0.8227333416163207 
1.6562001350130886 2.7760902104053806 58.74746900057375 0.8133588979915701 
1.6885469771617403 2.840466075642596 59.908755751948505 0.8025703011882904 
1.7150815532957868 2.888461706076112 60.942287750605004 0.7938752443819886 
1.7425964758117638 2.950625357249526 61.46022011505827 0.7830165237686455 
1.7623293081727767 3.0025689299817953 61.8024511817448 0.7739435167590221 
1.7765317027562608 3.040134552999257 62.27723023541354 0.7669319710805588 
1.7986013975790036 3.081671472737471 62.8399155765787 0.7604125281472595 
1.666313863156792 2.806883709032801 59.20844190184303 0.7604125281472595 
epoch: 54, train time every whole data:424.16s
epoch: 54, total time:20705.84s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.28s
test time on whole data:55.88s
1.5133574052555043 2.462262292193207 55.90563952263972 0.846876097588396 
1.5438739176993037 2.539444862756378 55.965328610470586 0.8391325908519826 
1.5715825496273381 2.6078266097887908 56.18280028144705 0.8318104015835293 
1.5900117704286463 2.6393773729739514 56.321852501342285 0.8294013128752215 
1.6181659547546434 2.69311829158146 57.080715479758794 0.822649916642041 
1.6470173808221837 2.752034060944271 58.014518230977444 0.8139024767945384 
1.6800575709829018 2.8213015139695394 59.18616798781212 0.8029601263292453 
1.708615023558782 2.875319333569616 60.18087156344295 0.7944925996314403 
1.7393428219369658 2.9472197248664864 60.6528237215076 0.7831153740239946 
1.7634712222948492 3.011184460021749 60.862668388023046 0.7732606396205516 
1.7784793491276603 3.050175459859847 61.023557168708464 0.7675952144471259 
1.7993871435374023 3.0927333385769424 61.34765638536352 0.7621635192990777 
1.662780175835515 2.798128470609045 58.56050903423434 0.7621635192990777 
epoch: 55, train time every whole data:423.70s
epoch: 55, total time:21195.88s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.02s
test time on whole data:54.92s
1.5178642618665916 2.4708229318618145 55.99185459582561 0.8477840251850677 
1.5464184300301684 2.535191269626955 56.41770787251702 0.8407864921435657 
1.574599983262785 2.6015976791068307 56.79388948770777 0.8339160728658792 
1.5959377895308038 2.6414785115114614 57.02541484913799 0.8307212036037199 
1.6288597269882226 2.7044398355016264 57.88350149252843 0.8235061133905025 
1.661906568739652 2.76988783343462 58.86821467234691 0.8145488911764354 
1.6947514049987353 2.8344148894814842 60.01757345628717 0.8043620681862139 
1.7226211127727513 2.8863765006143067 60.97527476244715 0.7955085660342185 
1.7514605357080166 2.957865905242781 61.52727379139736 0.7833726955909627 
1.7700700539806413 3.006588284865973 61.74653690420331 0.7751708220282708 
1.7809498124488052 3.0344242571105426 62.09640428697995 0.7702401229274217 
1.800035905606424 3.066621411598634 62.6134061909962 0.7657827112553606 
1.6704562988277998 2.7992509672216315 59.32989426819228 0.7657827112553606 
epoch: 56, train time every whole data:424.44s
epoch: 56, total time:21685.82s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.08s
test time on whole data:55.02s
1.5173354299756743 2.4721966935634003 55.65062145356996 0.8475338996786088 
1.5451449956728056 2.5380966803579117 55.825966204886804 0.8412141562094008 
1.5727704725249538 2.602988323390711 56.212615653978126 0.8348451356489607 
1.5951035953912707 2.6451053857880837 56.528405953462645 0.8315392299263186 
1.6269683907022257 2.7061885274921083 57.440938910803574 0.8245843099507273 
1.661220106193441 2.7783233336846855 58.49253698975818 0.8145806020392748 
1.694982987343999 2.841495436143527 59.62140243448677 0.8050526836880464 
1.7247023210894494 2.8934488025436873 60.556739503322795 0.7969925952307236 
1.7563762531763918 2.970810800263887 61.036620282986696 0.7846380342241984 
1.7821586250453478 3.039770990421513 61.25876057359021 0.7738931398183406 
1.799188294387999 3.081838915516306 61.63963976575359 0.7671317466669275 
1.8220478210391566 3.124169519042851 62.269428981670686 0.7607192407049314 
1.6748332743785594 2.8157087904826583 58.87794786400321 0.7607192407049314 
epoch: 57, train time every whole data:424.63s
epoch: 57, total time:22177.11s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.87s
test time on whole data:57.81s
1.5184666873912016 2.4861080320525577 54.91750580448854 0.8471753984445809 
1.5502205476016693 2.56486794841297 55.275732805553034 0.8395341342510559 
1.580945878714944 2.638453568100891 55.64316116040975 0.831901216115929 
1.6012428783783246 2.6753799369753297 55.87620960201962 0.828887711518454 
1.631980897986818 2.7334358004154944 56.65107287407542 0.8220356068301129 
1.6654259756926801 2.8042329488003697 57.6527475774855 0.8117627356561736 
1.698788101066525 2.863393965426108 58.79679497708204 0.8025627670814282 
1.72450779228214 2.906209816083157 59.73719765483176 0.7949128876933946 
1.7535675793973995 2.97486443653028 60.30665087171526 0.7830629952044427 
1.7754101642951192 3.0359821478289906 60.563257781946234 0.7730009136685346 
1.785270168138668 3.0640452716494693 60.79932434450291 0.767978307227689 
1.8033944286717367 3.0960169121431957 61.28249642086215 0.762930077313046 
1.6741017583014355 2.8269469713390665 58.12531594772974 0.762930077313046 
epoch: 58, train time every whole data:423.67s
epoch: 58, total time:22669.65s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.03s
test time on whole data:56.10s
1.5183634856275858 2.4953041915902543 54.66578650414296 0.8471045668889581 
1.5472033451500216 2.5599454611812043 55.08112372777927 0.8401380184604351 
1.5751622394099831 2.626800007496265 55.43439701560643 0.8331598864538775 
1.595346492271072 2.6637130348048594 55.57722565964851 0.8308252902075852 
1.628056083220811 2.728131019922694 56.25316217926435 0.824234706973397 
1.6641548415805612 2.805306584727806 57.13202095390501 0.8145247831752237 
1.701097754380178 2.879621925864167 58.19588721758785 0.803849419087312 
1.730958594929693 2.931677999873743 59.10203626458457 0.7958713879371716 
1.7606058710832149 2.9997353974342507 59.62198245347873 0.7845372843331235 
1.7844815186172547 3.0602362804904595 60.03025994455806 0.7747055431232138 
1.7973276233624311 3.0894189084248502 60.50841937060647 0.7693903995429346 
1.8164517971744671 3.121513783897962 61.2110821153035 0.7642966905018606 
1.6766008039006062 2.837641735855863 57.734578266520465 0.7642966905018606 
epoch: 59, train time every whole data:423.44s
epoch: 59, total time:23158.31s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.05s
test time on whole data:54.97s
1.513329907310151 2.477582216214318 55.09703391226797 0.8482271944002223 
1.5395083628137196 2.5372252997129263 55.60903631657463 0.8409721092931425 
1.5613198587957415 2.5842908853348967 56.20548805139981 0.8346383295116426 
1.5766673345066429 2.6062479065872215 56.53217631514109 0.8327596317091315 
1.6040637536281277 2.6570706434131424 57.395687433099226 0.8264255607534254 
1.6367725671794975 2.73116494895525 58.34489976509259 0.8165088996494336 
1.6714492454236993 2.8003898187539003 59.45517232902032 0.8066046848791095 
1.7002758605885542 2.851225575346822 60.36875763037875 0.7990491871177178 
1.730162302372711 2.9217285294634756 60.855612485412394 0.7879733994281489 
1.7521634742126224 2.9801234066314106 61.175845744948795 0.7785156555128444 
1.763854026799046 3.010583110350654 61.6597841418723 0.772913497283694 
1.783477702435993 3.0454003717230655 62.3903702874685 0.7672917730701865 
1.6527536996722088 2.7732904253229433 58.75763391362033 0.7672917730701865 
epoch: 60, train time every whole data:423.88s
epoch: 60, total time:23648.77s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.05s
test time on whole data:54.95s
1.5148394077664153 2.4828232957422176 54.66504607845849 0.8473021971021317 
1.5444017492360658 2.5463252671399816 54.963284396579034 0.8413178004056583 
1.5716138741356276 2.6094220426976023 55.25238834291636 0.8348284042755874 
1.593842485961194 2.6528740616543196 55.46012460902192 0.831496790323668 
1.6235971160986948 2.711410288731559 56.2903062224173 0.8245487722039266 
1.6529314200439977 2.772231782279829 57.266587056402166 0.8153555340507149 
1.6839129239869792 2.829682366280531 58.46052386261559 0.8052906098550087 
1.710658545646639 2.87895635731286 59.540372365978165 0.7960134535450007 
1.7397444189012583 2.944461494308836 60.20197151354611 0.7844246383893217 
1.7614772546240793 3.001995422313527 60.72142899082402 0.7746955690728536 
1.776692486966295 3.039913909151785 61.360805438902844 0.7677527244222867 
1.7982311126013242 3.079931754615086 62.10255703011024 0.7614759169053955 
1.6643285663307141 2.802326430440469 58.02393251173286 0.7614759169053955 
epoch: 61, train time every whole data:437.11s
epoch: 61, total time:24152.29s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.05s
test time on whole data:54.95s
1.5086939844528124 2.4617154443252454 55.02629233201562 0.848363745336875 
1.5382432298803967 2.5345185360305638 55.263777666180516 0.8410127273317827 
1.5664574668768438 2.6036798100585408 55.559554402961886 0.8340557918644212 
1.58936707201334 2.6485320455611707 55.723596897624425 0.8308306612869748 
1.6206607956360315 2.7091837852822254 56.500705712262324 0.8243099195912037 
1.6553095607016057 2.786201740072969 57.446003441819826 0.8138247047813877 
1.6870780204922138 2.845772906477493 58.48213693363709 0.8047663228653467 
1.713293583884037 2.8909466347146933 59.349271801452375 0.7972341641873807 
1.7393424367330792 2.952057338941694 59.76563439844842 0.7868791929163887 
1.7602563624172693 3.0055277340283517 60.07083627363051 0.7780164639392788 
1.7690935462793955 3.0286875646534206 60.340280233281504 0.7737443084928938 
1.785434615975512 3.0578315239724456 60.85007236576999 0.7692080341809922 
1.6611025562785449 2.8004078865484514 57.86497169149233 0.7692080341809922 
epoch: 62, train time every whole data:427.03s
epoch: 62, total time:24645.41s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.08s
test time on whole data:55.01s
1.5124735161172491 2.460676865735578 55.74914789194207 0.8481386780599769 
1.542390493408289 2.5336348199507643 55.976223996782274 0.8404538127327231 
1.5700390731331315 2.596926649876812 56.32078057072461 0.8341142093184541 
1.5896040505930071 2.629793234116972 56.567038606225054 0.8319346303743375 
1.619478183878852 2.6833935579526536 57.42225967327737 0.8260619798145702 
1.6524414966820429 2.7533423928218577 58.40965975384954 0.8166483101078404 
1.6879975358785264 2.8223153063286337 59.58260900478146 0.8060117806361236 
1.718085863523895 2.8735214740457247 60.59080908375184 0.7976596097947338 
1.7496038269134504 2.948589735487162 61.167060425558816 0.785217536968261 
1.7723956862295136 3.004045354193664 61.56960971064082 0.7763380254117088 
1.7850157182057875 3.034366290923916 61.93994282496409 0.7714819689711548 
1.8063159534085897 3.0708120145752105 62.55088128776731 0.7665173584326711 
1.6671534498310279 2.7912645927550956 58.9873119958374 0.7665173584326711 
epoch: 63, train time every whole data:423.49s
epoch: 63, total time:25135.17s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.05s
test time on whole data:54.99s
1.509447282563629 2.4598853476234797 56.27593110985437 0.8472245282376019 
1.5385630641137915 2.5310013653135464 56.48708120637412 0.8396607998940343 
1.566112967057863 2.602044882457577 56.68982872169938 0.8325592038130079 
1.589568945865812 2.6468219106675206 56.788930994435496 0.8296418788688925 
1.6257511268750364 2.719697685221067 57.4610255295615 0.822411270008563 
1.664490312747746 2.801144965836691 58.43922190043443 0.8121377294137463 
1.6999763664099432 2.8736545526809874 59.53788556549773 0.8008890622780467 
1.728639061579392 2.9258795042968555 60.591214628069444 0.7915357580320302 
1.7589459192327979 2.990676514107885 61.264655669664194 0.7800371915802082 
1.7823951941769394 3.0495004841845876 61.77799716232067 0.7698083421163543 
1.7947681406338123 3.079147397382471 62.14690759052122 0.7643223621696797 
1.8173958369520093 3.120519577969281 62.819407851023136 0.7573175095678628 
1.6730045181840643 2.8248529159143545 59.190142650539116 0.7573175095678628 
epoch: 64, train time every whole data:428.74s
epoch: 64, total time:25630.62s
predicting testing set batch 1 / 168, time: 0.33s
predicting testing set batch 101 / 168, time: 34.25s
test time on whole data:57.33s
1.5072382104542283 2.462098898050547 55.534102542063565 0.848089304736414 
1.5347024975624823 2.5285626644297357 55.898611030527256 0.8409392699512578 
1.5599080311300322 2.58882873614285 56.26233609157811 0.8344826375974373 
1.5777839537163576 2.619685940152987 56.48993293735576 0.8321936049884439 
1.6071301655827888 2.6762427868541345 57.28761964122583 0.8254083958237803 
1.6392949794733098 2.7477920544583925 58.31046129071582 0.8147872518311927 
1.6694583161022691 2.8050067006335304 59.468105833173055 0.8050399978553268 
1.6936896328918103 2.846663802113545 60.45761251395386 0.7976292934415479 
1.721085631482037 2.9096932725399616 61.05743526481755 0.7871608476611184 
1.7444802373598018 2.9729616761541013 61.39321422460814 0.7769149687223658 
1.7544212254625524 2.995826332472236 61.635610952484534 0.7734057465338032 
1.7749899707890693 3.0352497820629867 62.123724716926674 0.7676814125393238 
1.648681904333895 2.7718386726357993 58.826704944508535 0.7676814125393238 
epoch: 65, train time every whole data:423.59s
epoch: 65, total time:26122.77s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 32.99s
test time on whole data:54.88s
1.511131481252788 2.4685584939012855 55.326620844951876 0.8480670198517405 
1.5447880191528016 2.5541630820377996 55.77121223487107 0.8390665274020717 
1.5742265475411854 2.622386791512093 56.28047099789627 0.831721422547147 
1.5949846197889141 2.6581830036923995 56.505820001355055 0.8288890047458332 
1.6253461927276638 2.714286219243235 57.2585647424512 0.822633670617782 
1.661391270815173 2.788532095437931 58.25902799228595 0.8123299196994136 
1.6961815704091319 2.8600275300010654 59.3747271643319 0.8003370807595075 
1.7229599409956662 2.9126388444492104 60.323149790954666 0.7902744828271719 
1.750508968743274 2.971975094530127 60.92224033396532 0.7793116695427912 
1.7704469049857663 3.02327335402355 61.31307397550678 0.7702103109305227 
1.7832760614999348 3.051660163784432 61.692379962041954 0.7648640011707223 
1.802467305931573 3.086615390059462 62.27686746037092 0.7594422788161478 
1.669809073653656 2.816314153696425 58.77548847999601 0.7594422788161478 
epoch: 66, train time every whole data:426.69s
epoch: 66, total time:26616.17s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.12s
test time on whole data:55.08s
1.5087556768439356 2.4562151963930723 56.113406042562076 0.8481627962262288 
1.5346573371108443 2.514880417101544 56.62810329729978 0.8411351364429608 
1.5587402860749335 2.5696898577479392 57.11577851503474 0.8346612824092174 
1.5768558951537346 2.6058850809123624 57.3227257587289 0.8312300452704195 
1.604311969089721 2.6574386059345088 58.0969386475077 0.8253637394256944 
1.6372329653631896 2.7259165787278827 59.0103789740691 0.8165739286361058 
1.67203308887487 2.7911270173354215 59.997036604813324 0.8076455972470361 
1.7047581749142457 2.8567124140915445 60.80424564098502 0.798265907358879 
1.7374558318204114 2.9346262625191715 61.29543418365525 0.7861824293447012 
1.7615219292039317 2.99994392898533 61.542411636096084 0.775691933863459 
1.772109752432665 3.0284306262609983 61.822977363708496 0.7707556660000937 
1.7907596682361548 3.066588716621119 62.366357931135106 0.764579239224276 
1.6549327145932198 2.7746232461689457 59.343111155397295 0.764579239224276 
epoch: 67, train time every whole data:430.44s
epoch: 67, total time:27113.21s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.00s
test time on whole data:54.87s
1.510286277210872 2.4744939319420816 54.69291623560408 0.8491337933479698 
1.5430778321995444 2.557511332628875 54.95026604457708 0.8413164167351873 
1.5730073586257973 2.628335015476074 55.33109284719791 0.8339608690543185 
1.5939896276307604 2.6659490877817547 55.619370370094835 0.830692994799231 
1.6188625742911937 2.7089889061769785 56.47723209832183 0.8252409662359295 
1.6438646744651986 2.7586647054324662 57.5211813170281 0.8172752686732803 
1.669785740467764 2.805382456819091 58.687342047735136 0.8087108169059 
1.6918166170950446 2.83816984163021 59.72778168477899 0.802136723003419 
1.7173544365758342 2.898040773281782 60.44728062394057 0.7915826832971281 
1.7377492802412737 2.9520163600180194 60.85239031199791 0.7823883455824238 
1.7460031952992792 2.971830312494869 61.17911787909347 0.7785715528285685 
1.7619181278677036 3.0002459380738125 61.59564343250585 0.7742448230652433 
1.650642978497522 2.776387589511639 58.090283710663435 0.7742448230652433 
epoch: 68, train time every whole data:424.21s
epoch: 68, total time:27603.18s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 34.65s
test time on whole data:57.59s
1.5085320571850809 2.4439922816232262 55.58238812162808 0.8489313927343985 
1.5354134891065991 2.5099732627975655 56.02396457963196 0.841253566141304 
1.5602224043448945 2.5714136013373796 56.3470873415567 0.8345621807723363 
1.578551793586108 2.6065473358058515 56.46341398701207 0.832080737806088 
1.606673608959697 2.663436528699703 57.10516732988397 0.8260752362024898 
1.640730199500741 2.7378120798868935 57.983508416749885 0.8165694110032327 
1.6746761172774824 2.8077509962596268 58.9487306463939 0.8063241872535925 
1.7042289760003664 2.862847081822323 59.82479107776605 0.7975730085603387 
1.7346045487882302 2.9350220415199955 60.34011168162841 0.7855266962907101 
1.7560684744064652 2.990475785634408 60.64000073950609 0.7762323230757087 
1.7655320719030818 3.0129826544056346 60.84918343354618 0.7720572497487548 
1.783660398092387 3.0516480366326504 61.354299932812715 0.7656380047022925 
1.6540745115959277 2.773326961093258 58.45534060963149 0.7656380047022925 
epoch: 69, train time every whole data:423.72s
epoch: 69, total time:28095.90s
predicting testing set batch 1 / 168, time: 0.32s
predicting testing set batch 101 / 168, time: 33.02s
test time on whole data:54.95s
1.5065271888160634 2.45765976682173 55.540763908693314 0.8485697329241519 
1.5349438816967997 2.524479285869914 55.8651970204332 0.8416769989768539 
1.5616722992694094 2.586735109467795 56.22579975992831 0.83485808702078 
1.5820294083334683 2.627215642862889 56.39380527117757 0.8313675983228146 
1.6129774220448343 2.6885704274817512 57.07355966225541 0.8246427142424095 
1.6473115907204117 2.76428852150678 57.98210501409303 0.814529148190796 
1.6770543711512749 2.820074143340908 58.949619548025275 0.8059936585854122 
1.7006095632759943 2.8600832066245174 59.79664324583415 0.79922014099727 
1.7267673540666166 2.9266261240692404 60.367904741719 0.7877673271756517 
1.7491254072733047 2.983572798757309 60.73010158353726 0.7783415593981144 
1.7587565018800753 3.0080666306410015 60.95725472015652 0.7742027601797704 
1.7783469661269338 3.04693521879574 61.39504471342454 0.7683689822173567 
1.6530101628879321 2.781011057882834 58.43994042358914 0.7683689822173567 
