CUDA: True cuda:0
Read configuration file: configurations/bike.conf
total training epoch, fine tune epoch: 50 , 50
batch_size: 4
folder_dir: MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1
load file: data/bike.npz
ori length: 3001 , percent: 1.0 , scale: 3001
train: torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12])
val: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
test: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
TemporalPositionalEncoding max_len: 12
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder2): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (src_embed2): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed2): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
  (prediction_generator2): Linear(in_features=64, out_features=1, bias=True)
)
delete the old one and create params directory experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.0.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.0.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn.gate_convs.bias 	 torch.Size([64])
encoder.layers.0.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.0.tcn2.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn2.gate_convs.bias 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.1.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.1.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn.gate_convs.bias 	 torch.Size([64])
encoder.layers.1.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.1.tcn2.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn2.gate_convs.bias 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.2.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.2.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn.gate_convs.bias 	 torch.Size([64])
encoder.layers.2.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.2.tcn2.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn2.gate_convs.bias 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.3.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.3.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn.gate_convs.bias 	 torch.Size([64])
encoder.layers.3.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.3.tcn2.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn2.gate_convs.bias 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.3.sublayer2.norm2.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
encoder.norm2.weight 	 torch.Size([64])
encoder.norm2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.0.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.tcn.gate_convs.bias 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.1.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.tcn.gate_convs.bias 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.2.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.tcn.gate_convs.bias 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.3.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.tcn.gate_convs.bias 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.0.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.0.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.0.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.0.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.0.tcn.gate_convs.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.1.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.1.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.1.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.1.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.1.tcn.gate_convs.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.2.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.2.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.2.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.2.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.2.tcn.gate_convs.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.3.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.3.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.3.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.3.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.3.tcn.gate_convs.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.norm.weight 	 torch.Size([64])
decoder2.norm.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed.2.embedding.weight 	 torch.Size([250, 64])
src_embed2.0.weight 	 torch.Size([64, 1])
src_embed2.0.bias 	 torch.Size([64])
src_embed2.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed2.2.embedding.weight 	 torch.Size([250, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([250, 64])
trg_embed2.0.weight 	 torch.Size([64, 1])
trg_embed2.0.bias 	 torch.Size([64])
trg_embed2.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed2.2.embedding.weight 	 torch.Size([250, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
prediction_generator2.weight 	 torch.Size([1, 64])
prediction_generator2.bias 	 torch.Size([1])
Net's total params: 852354
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367]}]
validation batch 1 / 168, loss: 0.69
validation batch 101 / 168, loss: 0.74
validation cost time: 49.7522s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1/epoch_0.params
epoch: 0, learning rate 0.001000
epoch: 0, train time every whole data:171.32s
epoch: 0, total time:221.17s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.04
validation cost time: 49.0641s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1/epoch_1.params
epoch: 1, learning rate 0.001000
epoch: 1, train time every whole data:172.48s
epoch: 1, total time:442.80s
validation batch 1 / 168, loss: 0.08
validation batch 101 / 168, loss: 0.09
validation cost time: 49.2504s
epoch: 2, learning rate 0.001000
epoch: 2, train time every whole data:173.24s
epoch: 2, total time:665.29s
validation batch 1 / 168, loss: 0.14
validation batch 101 / 168, loss: 0.10
validation cost time: 49.2437s
epoch: 3, learning rate 0.001000
epoch: 3, train time every whole data:172.93s
epoch: 3, total time:887.47s
validation batch 1 / 168, loss: 0.08
validation batch 101 / 168, loss: 0.05
validation cost time: 51.4183s
epoch: 4, learning rate 0.001000
epoch: 4, train time every whole data:172.53s
epoch: 4, total time:1111.42s
validation batch 1 / 168, loss: 0.11
validation batch 101 / 168, loss: 0.08
validation cost time: 50.9819s
epoch: 5, learning rate 0.001000
epoch: 5, train time every whole data:172.80s
epoch: 5, total time:1335.20s
validation batch 1 / 168, loss: 0.10
validation batch 101 / 168, loss: 0.03
validation cost time: 49.2281s
epoch: 6, learning rate 0.001000
epoch: 6, train time every whole data:172.98s
epoch: 6, total time:1557.41s
validation batch 1 / 168, loss: 0.09
validation batch 101 / 168, loss: 0.05
validation cost time: 49.4102s
epoch: 7, learning rate 0.001000
epoch: 7, train time every whole data:172.91s
epoch: 7, total time:1779.74s
validation batch 1 / 168, loss: 0.12
validation batch 101 / 168, loss: 0.04
validation cost time: 49.1682s
epoch: 8, learning rate 0.001000
epoch: 8, train time every whole data:172.81s
epoch: 8, total time:2001.73s
validation batch 1 / 168, loss: 0.09
validation batch 101 / 168, loss: 0.03
validation cost time: 49.1228s
epoch: 9, learning rate 0.001000
epoch: 9, train time every whole data:172.85s
epoch: 9, total time:2223.71s
validation batch 1 / 168, loss: 0.12
validation batch 101 / 168, loss: 0.08
validation cost time: 49.2515s
epoch: 10, learning rate 0.001000
epoch: 10, train time every whole data:172.79s
epoch: 10, total time:2445.76s
validation batch 1 / 168, loss: 0.10
validation batch 101 / 168, loss: 0.09
validation cost time: 50.2831s
epoch: 11, learning rate 0.001000
epoch: 11, train time every whole data:173.11s
epoch: 11, total time:2669.16s
validation batch 1 / 168, loss: 0.10
validation batch 101 / 168, loss: 0.06
validation cost time: 49.1471s
epoch: 12, learning rate 0.001000
epoch: 12, train time every whole data:173.07s
epoch: 12, total time:2891.38s
validation batch 1 / 168, loss: 0.09
validation batch 101 / 168, loss: 0.03
validation cost time: 49.0621s
epoch: 13, learning rate 0.001000
epoch: 13, train time every whole data:172.86s
epoch: 13, total time:3113.31s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.03
validation cost time: 49.1613s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1/epoch_14.params
epoch: 14, learning rate 0.001000
epoch: 14, train time every whole data:172.76s
epoch: 14, total time:3335.33s
validation batch 1 / 168, loss: 0.09
validation batch 101 / 168, loss: 0.02
validation cost time: 49.1942s
epoch: 15, learning rate 0.001000
epoch: 15, train time every whole data:172.86s
epoch: 15, total time:3557.39s
validation batch 1 / 168, loss: 0.10
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2876s
epoch: 16, learning rate 0.001000
epoch: 16, train time every whole data:173.20s
epoch: 16, total time:3779.88s
validation batch 1 / 168, loss: 0.09
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2582s
epoch: 17, learning rate 0.001000
epoch: 17, train time every whole data:171.97s
epoch: 17, total time:4001.11s
validation batch 1 / 168, loss: 0.09
validation batch 101 / 168, loss: 0.03
validation cost time: 49.0095s
epoch: 18, learning rate 0.001000
epoch: 18, train time every whole data:172.58s
epoch: 18, total time:4222.71s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 49.0273s
epoch: 19, learning rate 0.001000
epoch: 19, train time every whole data:172.22s
epoch: 19, total time:4443.96s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2349s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1/epoch_20.params
epoch: 20, learning rate 0.001000
epoch: 20, train time every whole data:172.64s
epoch: 20, total time:4665.96s
validation batch 1 / 168, loss: 0.12
validation batch 101 / 168, loss: 0.03
validation cost time: 49.1315s
epoch: 21, learning rate 0.001000
epoch: 21, train time every whole data:172.18s
epoch: 21, total time:4887.28s
validation batch 1 / 168, loss: 0.08
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2232s
epoch: 22, learning rate 0.001000
epoch: 22, train time every whole data:173.01s
epoch: 22, total time:5109.51s
validation batch 1 / 168, loss: 0.11
validation batch 101 / 168, loss: 0.03
validation cost time: 49.3689s
epoch: 23, learning rate 0.001000
epoch: 23, train time every whole data:172.88s
epoch: 23, total time:5331.76s
validation batch 1 / 168, loss: 0.10
validation batch 101 / 168, loss: 0.03
validation cost time: 50.0145s
epoch: 24, learning rate 0.001000
epoch: 24, train time every whole data:172.82s
epoch: 24, total time:5554.60s
validation batch 1 / 168, loss: 0.09
validation batch 101 / 168, loss: 0.02
validation cost time: 49.0300s
epoch: 25, learning rate 0.001000
epoch: 25, train time every whole data:172.86s
epoch: 25, total time:5776.49s
validation batch 1 / 168, loss: 0.10
validation batch 101 / 168, loss: 0.02
validation cost time: 49.1257s
epoch: 26, learning rate 0.001000
epoch: 26, train time every whole data:172.89s
epoch: 26, total time:5998.51s
validation batch 1 / 168, loss: 0.11
validation batch 101 / 168, loss: 0.03
validation cost time: 49.3088s
epoch: 27, learning rate 0.001000
epoch: 27, train time every whole data:172.96s
epoch: 27, total time:6220.78s
validation batch 1 / 168, loss: 0.10
validation batch 101 / 168, loss: 0.02
validation cost time: 49.1778s
epoch: 28, learning rate 0.001000
epoch: 28, train time every whole data:172.84s
epoch: 28, total time:6442.81s
validation batch 1 / 168, loss: 0.09
validation batch 101 / 168, loss: 0.02
validation cost time: 49.4139s
epoch: 29, learning rate 0.001000
epoch: 29, train time every whole data:173.44s
epoch: 29, total time:6665.66s
validation batch 1 / 168, loss: 0.12
validation batch 101 / 168, loss: 0.02
validation cost time: 49.0701s
epoch: 30, learning rate 0.001000
epoch: 30, train time every whole data:172.34s
epoch: 30, total time:6887.07s
validation batch 1 / 168, loss: 0.12
validation batch 101 / 168, loss: 0.02
validation cost time: 49.0434s
epoch: 31, learning rate 0.001000
epoch: 31, train time every whole data:172.93s
epoch: 31, total time:7109.05s
validation batch 1 / 168, loss: 0.12
validation batch 101 / 168, loss: 0.02
validation cost time: 49.0767s
epoch: 32, learning rate 0.001000
epoch: 32, train time every whole data:172.78s
epoch: 32, total time:7330.92s
validation batch 1 / 168, loss: 0.11
validation batch 101 / 168, loss: 0.03
validation cost time: 49.2491s
epoch: 33, learning rate 0.001000
epoch: 33, train time every whole data:173.49s
epoch: 33, total time:7553.66s
validation batch 1 / 168, loss: 0.11
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2434s
epoch: 34, learning rate 0.001000
epoch: 34, train time every whole data:172.75s
epoch: 34, total time:7775.66s
validation batch 1 / 168, loss: 0.13
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2932s
epoch: 35, learning rate 0.001000
epoch: 35, train time every whole data:172.83s
epoch: 35, total time:7997.79s
validation batch 1 / 168, loss: 0.11
validation batch 101 / 168, loss: 0.02
validation cost time: 49.1143s
epoch: 36, learning rate 0.001000
epoch: 36, train time every whole data:172.79s
epoch: 36, total time:8219.69s
validation batch 1 / 168, loss: 0.12
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2698s
epoch: 37, learning rate 0.001000
epoch: 37, train time every whole data:174.88s
epoch: 37, total time:8443.84s
validation batch 1 / 168, loss: 0.12
validation batch 101 / 168, loss: 0.02
validation cost time: 50.5506s
epoch: 38, learning rate 0.001000
epoch: 38, train time every whole data:174.68s
epoch: 38, total time:8669.08s
validation batch 1 / 168, loss: 0.10
validation batch 101 / 168, loss: 0.02
validation cost time: 51.0895s
epoch: 39, learning rate 0.001000
epoch: 39, train time every whole data:174.81s
epoch: 39, total time:8894.99s
validation batch 1 / 168, loss: 0.10
validation batch 101 / 168, loss: 0.02
validation cost time: 50.1944s
epoch: 40, learning rate 0.001000
epoch: 40, train time every whole data:174.94s
epoch: 40, total time:9120.12s
validation batch 1 / 168, loss: 0.09
validation batch 101 / 168, loss: 0.02
validation cost time: 51.6009s
epoch: 41, learning rate 0.001000
epoch: 41, train time every whole data:175.03s
epoch: 41, total time:9346.76s
validation batch 1 / 168, loss: 0.12
validation batch 101 / 168, loss: 0.02
validation cost time: 50.8305s
epoch: 42, learning rate 0.001000
epoch: 42, train time every whole data:175.20s
epoch: 42, total time:9572.79s
validation batch 1 / 168, loss: 0.13
validation batch 101 / 168, loss: 0.02
validation cost time: 50.3602s
epoch: 43, learning rate 0.001000
epoch: 43, train time every whole data:175.29s
epoch: 43, total time:9798.45s
validation batch 1 / 168, loss: 0.10
validation batch 101 / 168, loss: 0.02
validation cost time: 50.8988s
epoch: 44, learning rate 0.001000
epoch: 44, train time every whole data:175.03s
epoch: 44, total time:10024.39s
validation batch 1 / 168, loss: 0.12
validation batch 101 / 168, loss: 0.02
validation cost time: 50.7681s
epoch: 45, learning rate 0.001000
epoch: 45, train time every whole data:175.33s
epoch: 45, total time:10250.49s
validation batch 1 / 168, loss: 0.11
validation batch 101 / 168, loss: 0.02
validation cost time: 50.2811s
epoch: 46, learning rate 0.001000
epoch: 46, train time every whole data:174.94s
epoch: 46, total time:10475.72s
validation batch 1 / 168, loss: 0.12
validation batch 101 / 168, loss: 0.02
validation cost time: 50.3616s
epoch: 47, learning rate 0.001000
epoch: 47, train time every whole data:174.86s
epoch: 47, total time:10700.94s
validation batch 1 / 168, loss: 0.11
validation batch 101 / 168, loss: 0.02
validation cost time: 50.8990s
epoch: 48, learning rate 0.001000
epoch: 48, train time every whole data:175.00s
epoch: 48, total time:10926.84s
validation batch 1 / 168, loss: 0.13
validation batch 101 / 168, loss: 0.03
validation cost time: 50.5426s
epoch: 49, learning rate 0.001000
epoch: 49, train time every whole data:174.74s
epoch: 49, total time:11152.13s
best epoch: 20
apply the best val model on the test data set ...
load weight from: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1/epoch_20.params
predicting testing set batch 1 / 168, time: 0.29s
predicting testing set batch 101 / 168, time: 31.21s
test time on whole data:51.24s
current epoch: 20, predict 0 points
current epoch: 20, predict 1 points
current epoch: 20, predict 2 points
current epoch: 20, predict 3 points
current epoch: 20, predict 4 points
current epoch: 20, predict 5 points
current epoch: 20, predict 6 points
current epoch: 20, predict 7 points
current epoch: 20, predict 8 points
current epoch: 20, predict 9 points
current epoch: 20, predict 10 points
current epoch: 20, predict 11 points
1.7371596137187666 2.816319537184462 57.52741732824288 0.8391998066416567 
1.8309776284919963 2.9576858609170347 62.30443573657827 0.8019444390318354 
1.9495673015814807 3.1845895073291652 67.43620714182578 0.7543248546773965 
2.089064361494478 3.468559752271028 72.50512281160381 0.7002443207392215 
2.220850594485356 3.729621252880755 76.88436919021807 0.6510406633218734 
2.3227366136638006 3.9332935929861965 80.3391541416011 0.6134586032570717 
2.3863017690111663 4.064793963163751 82.42495362160388 0.59029982227029 
2.4064987698871465 4.087590999922293 82.78719879711302 0.5882310980701295 
2.387241948870942 4.025415180029588 81.97075421923458 0.599404543258268 
2.359996323029023 3.9415349657303635 81.56004234296573 0.6143420023514469 
2.33528829930838 3.8501021221886242 81.44559937958829 0.6259952364809643 
2.3684392514156087 3.8597908883143197 84.34033982120353 0.6221772561505716 
2.199510206246512 3.6847075766756183 75.9609691312304 0.6221772561505716 
fine tune the model ... 
epoch: 50, learning rate 0.000100
epoch: 50, train time every whole data:371.05s
epoch: 50, total time:11587.19s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 50.3389s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1/epoch_50.params
epoch: 51, learning rate 0.000100
epoch: 51, train time every whole data:371.24s
epoch: 51, total time:12008.83s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 50.6614s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1/epoch_51.params
epoch: 52, learning rate 0.000100
epoch: 52, train time every whole data:367.77s
epoch: 52, total time:12427.39s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.3124s
epoch: 53, learning rate 0.000100
epoch: 53, train time every whole data:369.97s
epoch: 53, total time:12846.67s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 50.2956s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1/epoch_53.params
epoch: 54, learning rate 0.000100
epoch: 54, train time every whole data:372.18s
epoch: 54, total time:13269.31s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 50.4482s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1/epoch_54.params
epoch: 55, learning rate 0.000100
epoch: 55, train time every whole data:370.72s
epoch: 55, total time:13690.58s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 50.4039s
epoch: 56, learning rate 0.000100
epoch: 56, train time every whole data:373.08s
epoch: 56, total time:14114.07s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 50.2001s
epoch: 57, learning rate 0.000100
epoch: 57, train time every whole data:371.91s
epoch: 57, total time:14536.18s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 50.4723s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1/epoch_57.params
epoch: 58, learning rate 0.000100
epoch: 58, train time every whole data:361.24s
epoch: 58, total time:14948.00s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.0576s
epoch: 59, learning rate 0.000100
epoch: 59, train time every whole data:361.50s
epoch: 59, total time:15358.57s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.1557s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1/epoch_59.params
epoch: 60, learning rate 0.000100
epoch: 60, train time every whole data:364.83s
epoch: 60, total time:15772.65s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 49.1269s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1/epoch_60.params
epoch: 61, learning rate 0.000100
epoch: 61, train time every whole data:364.00s
epoch: 61, total time:16185.89s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 49.1160s
epoch: 62, learning rate 0.000100
epoch: 62, train time every whole data:362.26s
epoch: 62, total time:16597.27s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2945s
epoch: 63, learning rate 0.000100
epoch: 63, train time every whole data:362.75s
epoch: 63, total time:17009.32s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2357s
epoch: 64, learning rate 0.000100
epoch: 64, train time every whole data:372.46s
epoch: 64, total time:17431.02s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.7932s
epoch: 65, learning rate 0.000100
epoch: 65, train time every whole data:373.15s
epoch: 65, total time:17853.97s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.7555s
epoch: 66, learning rate 0.000100
epoch: 66, train time every whole data:373.66s
epoch: 66, total time:18277.38s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.7539s
epoch: 67, learning rate 0.000100
epoch: 67, train time every whole data:373.20s
epoch: 67, total time:18700.34s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.7487s
epoch: 68, learning rate 0.000100
epoch: 68, train time every whole data:373.07s
epoch: 68, total time:19123.17s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.4692s
epoch: 69, learning rate 0.000100
epoch: 69, train time every whole data:372.19s
epoch: 69, total time:19544.83s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.4403s
epoch: 70, learning rate 0.000100
epoch: 70, train time every whole data:369.36s
epoch: 70, total time:19963.63s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.7244s
epoch: 71, learning rate 0.000100
epoch: 71, train time every whole data:368.21s
epoch: 71, total time:20381.57s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.3248s
epoch: 72, learning rate 0.000100
epoch: 72, train time every whole data:369.93s
epoch: 72, total time:20800.82s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.5831s
epoch: 73, learning rate 0.000100
epoch: 73, train time every whole data:370.12s
epoch: 73, total time:21220.53s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.4875s
epoch: 74, learning rate 0.000100
epoch: 74, train time every whole data:370.85s
epoch: 74, total time:21640.87s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.6661s
epoch: 75, learning rate 0.000100
epoch: 75, train time every whole data:370.73s
epoch: 75, total time:22061.27s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.3696s
epoch: 76, learning rate 0.000100
epoch: 76, train time every whole data:362.70s
epoch: 76, total time:22473.34s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 49.3857s
epoch: 77, learning rate 0.000100
epoch: 77, train time every whole data:363.94s
epoch: 77, total time:22886.67s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.1756s
epoch: 78, learning rate 0.000100
epoch: 78, train time every whole data:363.25s
epoch: 78, total time:23299.10s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.7278s
epoch: 79, learning rate 0.000100
epoch: 79, train time every whole data:362.20s
epoch: 79, total time:23711.04s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.3042s
epoch: 80, learning rate 0.000100
epoch: 80, train time every whole data:363.30s
epoch: 80, total time:24123.64s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2626s
epoch: 81, learning rate 0.000100
epoch: 81, train time every whole data:361.68s
epoch: 81, total time:24534.59s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.0958s
epoch: 82, learning rate 0.000100
epoch: 82, train time every whole data:360.25s
epoch: 82, total time:24943.94s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.0894s
epoch: 83, learning rate 0.000100
epoch: 83, train time every whole data:360.52s
epoch: 83, total time:25353.55s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2877s
epoch: 84, learning rate 0.000100
epoch: 84, train time every whole data:361.84s
epoch: 84, total time:25764.68s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.1185s
epoch: 85, learning rate 0.000100
epoch: 85, train time every whole data:362.05s
epoch: 85, total time:26175.85s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.4053s
epoch: 86, learning rate 0.000100
epoch: 86, train time every whole data:364.41s
epoch: 86, total time:26589.67s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.3008s
epoch: 87, learning rate 0.000100
epoch: 87, train time every whole data:362.43s
epoch: 87, total time:27001.40s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2953s
epoch: 88, learning rate 0.000100
epoch: 88, train time every whole data:362.13s
epoch: 88, total time:27412.83s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 50.3424s
epoch: 89, learning rate 0.000100
epoch: 89, train time every whole data:362.44s
epoch: 89, total time:27825.62s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2829s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1/epoch_89.params
epoch: 90, learning rate 0.000100
epoch: 90, train time every whole data:362.00s
epoch: 90, total time:28237.01s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.1970s
epoch: 91, learning rate 0.000100
epoch: 91, train time every whole data:361.73s
epoch: 91, total time:28647.94s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 52.4096s
epoch: 92, learning rate 0.000100
epoch: 92, train time every whole data:360.85s
epoch: 92, total time:29061.21s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.1732s
epoch: 93, learning rate 0.000100
epoch: 93, train time every whole data:359.65s
epoch: 93, total time:29470.03s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.1668s
epoch: 94, learning rate 0.000100
epoch: 94, train time every whole data:360.08s
epoch: 94, total time:29879.28s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2962s
epoch: 95, learning rate 0.000100
epoch: 95, train time every whole data:361.84s
epoch: 95, total time:30290.42s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.1703s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1/epoch_95.params
epoch: 96, learning rate 0.000100
epoch: 96, train time every whole data:365.08s
epoch: 96, total time:30704.79s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2340s
epoch: 97, learning rate 0.000100
epoch: 97, train time every whole data:362.40s
epoch: 97, total time:31116.42s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.7917s
epoch: 98, learning rate 0.000100
epoch: 98, train time every whole data:362.04s
epoch: 98, total time:31528.25s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.2718s
epoch: 99, learning rate 0.000100
epoch: 99, train time every whole data:363.92s
epoch: 99, total time:31941.45s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 49.3032s
best epoch: 95
apply the best val model on the test data set ...
load weight from: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1/epoch_95.params
predicting testing set batch 1 / 168, time: 0.29s
predicting testing set batch 101 / 168, time: 29.70s
test time on whole data:49.36s
current epoch: 95, predict 0 points
current epoch: 95, predict 1 points
current epoch: 95, predict 2 points
current epoch: 95, predict 3 points
current epoch: 95, predict 4 points
current epoch: 95, predict 5 points
current epoch: 95, predict 6 points
current epoch: 95, predict 7 points
current epoch: 95, predict 8 points
current epoch: 95, predict 9 points
current epoch: 95, predict 10 points
current epoch: 95, predict 11 points
1.4825153753537508 2.418329671293032 53.8964998972985 0.8565883854251116 
1.5453484390778023 2.5419174700740643 55.13875157591943 0.8423131950126923 
1.575742598800432 2.611384588493385 55.68807559871968 0.8341081446752687 
1.5987354502974167 2.661060950206076 56.006826468439726 0.829380064797051 
1.6255312774961015 2.716098254419999 56.543256419072364 0.8239057649514889 
1.6575111791035604 2.7855117432115644 57.150862915579445 0.815948474523705 
1.6879180400886884 2.8499532368550606 57.74724612921055 0.8072197063990116 
1.7139919743593781 2.9051242486355693 58.08507845734147 0.799727657833669 
1.7389106959072607 2.9619001669074367 58.45072987289422 0.7910684724114051 
1.7581378745936034 2.999844387798931 58.85489095361501 0.7843378906755387 
1.7731373389618736 3.0250815374745716 59.0295687364248 0.7810820010027437 
1.794883569491495 3.066360711361371 59.5363534563047 0.7767952295625036 
1.662696984460947 2.802292048413903 57.17744410469484 0.7767952295625036 
