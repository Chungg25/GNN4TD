total training epoch, fine tune epoch: 30 , 40
batch_size: 4
folder_dir: MAE_ASTGNN_h1d0w0_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_diag
load file: data/bike.npz
ori length: 3001 , percent: 1.0 , scale: 3001
train: torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12])
val: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
test: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
TemporalPositionalEncoding max_len: 12
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (src_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
  (prediction_generator2): Linear(in_features=64, out_features=1, bias=True)
)
delete the old one and create params directory experiments/bike/MAE_ASTGNN_h1d0w0_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_diag
Net's state_dict:
encoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
encoder.norm2.weight 	 torch.Size([64])
encoder.norm2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
decoder.norm2.weight 	 torch.Size([64])
decoder.norm2.bias 	 torch.Size([64])
src_embed.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed.1.embedding.weight 	 torch.Size([250, 64])
src_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed2.1.embedding.weight 	 torch.Size([250, 64])
trg_embed.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.1.embedding.weight 	 torch.Size([250, 64])
trg_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed2.1.embedding.weight 	 torch.Size([250, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
prediction_generator2.weight 	 torch.Size([1, 64])
prediction_generator2.bias 	 torch.Size([1])
Net's total params: 1140610
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405]}]
predicting testing set batch 1 / 168, time: 0.63s
predicting testing set batch 101 / 168, time: 28.85s
test time on whole data:47.72s
19.694828878593675 23.6073804501132 783.6322003495357 0.08349309485789194 
17.46110490678251 20.823180163314753 695.4526752381397 0.07133223517366341 
16.09716374483066 19.461084814087478 641.1249289742573 0.060232101184319146 
16.07983923624642 19.487348837550048 639.2959974205116 0.04802746022075684 
17.294229400082447 20.655315287487845 685.8571743891075 0.03676746357334557 
18.531305461488575 21.859580917701056 732.7501624891557 0.02905588206377537 
19.31518634406548 22.639853674398378 761.0668352290107 0.02344463785787661 
19.927387427430865 23.248522266387177 782.2489547366636 0.018310716796108024 
19.835171238510558 23.241492198932008 774.2888281066917 0.013574381100882441 
19.311921026755567 22.953427310716073 748.2032972435659 0.00926695053392234 
19.44099145040945 23.339356757611398 749.7285248213801 0.004850622950888187 
19.905238144666487 23.93316822230941 767.308146112377 0.0008983658436219511 
18.57453060498856 22.157566089520095 730.0816556166071 0.0008983658436219511 
epoch: 0, train time every whole data:186.59s
epoch: 0, total time:245.10s
predicting testing set batch 1 / 168, time: 0.29s
predicting testing set batch 101 / 168, time: 30.10s
test time on whole data:49.84s
2.6352376710411516 4.061268834800195 75.4403517368587 0.4770078823341717 
2.6494722051446638 4.114508555237535 76.16410617578302 0.45154709254321 
2.683499146061728 4.192270083921947 77.59045097221285 0.41296546711234866 
2.733178747710639 4.262885130849667 79.66539582263255 0.3744839663399902 
2.785973933371582 4.339130301160382 81.58358654408158 0.3333929526541453 
2.837877665542155 4.414328775999736 83.32274444549637 0.29266784233083987 
2.8896990188413434 4.484131203409105 84.8772700025361 0.254691404294592 
2.942987812701258 4.534339754590265 86.4478672714347 0.22520488204909556 
2.9853178440091157 4.57298417840836 87.14732021329155 0.20248422577996988 
3.015082431146875 4.606150157993589 86.79631528819522 0.18561378528026362 
3.072091328860837 4.640798673944797 88.17274389256372 0.16929728105558878 
3.096603527267685 4.635032093502653 87.37372727316324 0.1638843354643538 
2.8605851109749194 4.4092871842790125 82.8820973441586 0.1638843354643538 
epoch: 1, train time every whole data:187.65s
epoch: 1, total time:494.66s
predicting testing set batch 1 / 168, time: 0.30s
predicting testing set batch 101 / 168, time: 28.89s
test time on whole data:48.81s
2.31183877947279 3.9350500493727925 56.113746721632296 0.6374954878811989 
2.339706030442069 3.9998851065919263 57.435764259494825 0.5998680666947294 
2.3812242332822864 4.085320065327187 58.80260812168393 0.5562132920187296 
2.427260427357274 4.160399163809134 60.558867176876305 0.5106796860973828 
2.477785037182094 4.2486128949972475 62.05368117761619 0.46360725921633134 
2.5324575857265543 4.328353477173991 63.67608932256717 0.4185464472002232 
2.5877990234197075 4.398852711300659 65.10489692892772 0.37671065261877495 
2.643689407851281 4.4510990556780925 66.51984808420248 0.3402757034115516 
2.7004212557448164 4.501153461451665 67.72440431195342 0.3067947575865777 
2.756642347683864 4.553719920416081 68.58826833872304 0.27629167216576206 
2.8139218987220276 4.591216238351705 69.4100227324563 0.247720030307765 
2.8760190268372674 4.627904183829978 70.6122027879227 0.21873949568680545 
2.5707304211435025 4.329390984967538 63.88364483345126 0.21873949568680545 
epoch: 2, train time every whole data:185.82s
epoch: 2, total time:740.93s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.44s
test time on whole data:47.31s
2.9664967497478107 4.00428070868196 100.73286105840774 0.6254848545555586 
2.9866345502399794 4.0345297684039405 101.99198621507497 0.607243576055431 
3.0009926787632564 4.102668741779613 100.08572899715624 0.5733563268686556 
3.027219621571934 4.142237988367167 101.1831444021361 0.5461480947988241 
3.042421933272499 4.179002553938062 101.2185526443867 0.5153980521477992 
3.0651727405003317 4.21919001616741 101.59814407832906 0.4831314120724749 
3.079477149064165 4.257288710272733 101.34912072401363 0.4475405763875026 
3.1029543029857534 4.297028969444851 101.47062489133303 0.41359983883445844 
3.1252247268031574 4.335266289788471 101.40230955693441 0.3804276589821693 
3.1418879702168385 4.36976842963882 100.71118929396339 0.3481750379961221 
3.1773340519516773 4.4103641205792945 101.02987225688507 0.3165688656308202 
3.209383210098637 4.449775222882609 100.9318753162795 0.28432560449447997 
3.077099973768003 4.235736199397503 101.14212284677792 0.28432560449447997 
epoch: 3, train time every whole data:217.82s
epoch: 3, total time:1017.53s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.23s
test time on whole data:47.04s
2.0185792506526092 3.497853462052341 67.59982361097086 0.6928822393845393 
2.151374935656165 3.7206446242216296 71.36223031843913 0.6457241193401044 
2.3698511527364277 4.157519735450733 72.73772150091254 0.5685221830278906 
2.4153573713951877 4.2574688909933665 73.56533910842278 0.5332454682792699 
2.4898005382180037 4.363311751028572 75.26455178132323 0.5013977435720696 
2.524647489539392 4.431066221212749 75.1067900769888 0.47844832511596647 
2.5516816928015933 4.488819896950971 74.28910138995252 0.4584424264623693 
2.578246461360316 4.545607357976535 72.9279999960462 0.44337475709483404 
2.6002744658920203 4.587694499172374 72.29061798825298 0.42195331527625707 
2.6477985828716664 4.647127255135825 73.15818948599109 0.3931068160765094 
2.7127041621841492 4.716157679991206 74.87314866328202 0.3623455979093034 
2.794610216630623 4.790267968186432 77.35619824085072 0.3323206539197481 
2.487910526661513 4.366593042602982 73.3777247507958 0.3323206539197481 
epoch: 4, train time every whole data:189.85s
epoch: 4, total time:1279.36s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.42s
test time on whole data:47.34s
2.128891483230871 3.2436739164978734 68.98768246354209 0.724500755937552 
2.2231115374362895 3.4231622625462097 71.31744297666653 0.6801900394780572 
2.344890745633149 3.678883385676822 71.39160743223111 0.6218955316481385 
2.4186698607708372 3.7785294237505913 74.2373501777 0.5845713264974731 
2.478051686032897 3.869692858679508 75.16533720110384 0.5524220849074069 
2.5131354992868644 3.9343137257115885 73.3277968847904 0.5368619566741775 
2.5324781418179296 3.9777083174466292 71.1741136652157 0.5257988547450864 
2.554407260928037 3.98532862577051 70.8068942362519 0.5213617298267728 
2.544905716841863 4.003264945126126 67.43304541745613 0.5157963095827407 
2.564286991558348 4.043068563538913 66.84800291380876 0.5002332924308386 
2.590031338729142 4.092185537110979 67.39864474031609 0.47541893106831606 
2.6299201270966304 4.156283532108621 68.47064287135832 0.4408541334761352 
2.4602316991135713 3.857919256223972 70.5464720283275 0.4408541334761352 
epoch: 5, train time every whole data:181.77s
epoch: 5, total time:1521.69s
predicting testing set batch 1 / 168, time: 0.31s
predicting testing set batch 101 / 168, time: 28.46s
test time on whole data:47.32s
2.0006857478013704 3.4215002036196536 55.73041589092485 0.7143357731270544 
2.076779019189466 3.5910314444756737 57.536483505241954 0.6721129285740528 
2.1796391149843557 3.8556557702829797 58.56775437263676 0.6103307421911681 
2.237894965773626 3.951956067661649 59.91348760845603 0.5781175425971596 
2.3043306473113065 4.078196743540434 59.75746530078369 0.5502098621829844 
2.3591597807676132 4.150798367835412 59.90016876273011 0.5316438867203169 
2.3857636164754985 4.170145084557151 60.382894971163225 0.526446247236489 
2.3809483487123533 4.132635854742754 59.991942631422035 0.533918019998974 
2.3705930012816654 4.08784141385506 60.23327731923632 0.5391772927468664 
2.408855805173605 4.145655142627117 60.89232577139594 0.5264186416319223 
2.449106291204425 4.204349546835858 61.92067813863075 0.502531926887189 
2.502340189043876 4.279733788691906 63.38276609465399 0.4667460552980265 
2.3046747106432632 4.013574819309114 59.85089733232719 0.4667460552980265 
epoch: 6, train time every whole data:182.25s
epoch: 6, total time:1765.06s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.52s
test time on whole data:47.44s
1.8487817975841463 3.1626060049707774 53.10903793402644 0.7457252683652705 
1.9413934756041993 3.3548174214061803 55.83087557676173 0.7039793052361605 
2.0542971257550553 3.6241311682193786 55.81853273179331 0.6568792515087091 
2.1138305148798087 3.7354072600440618 56.87241515612732 0.6287777250417036 
2.1793268151503233 3.8600434076290897 56.20111793432183 0.6083781133305016 
2.225319654150201 3.9281208528915164 55.69074964679793 0.5960996005551441 
2.2491010022652116 3.9509701441569005 55.48710287039711 0.5912443060014837 
2.2545617424005733 3.9312051665563956 55.84730460801504 0.5954671092038517 
2.271354347769437 3.952129806331597 55.57913983734015 0.5994697150192042 
2.2758785766073992 3.9567141592038166 55.24936529781671 0.6046108704297846 
2.297320578363325 4.003348227111945 55.77466945639126 0.5990918082115368 
2.326906755207816 4.0561187368591005 56.34663781999637 0.5780293599280463 
2.1698393654781247 3.802333621831641 55.65059245631894 0.5780293599280463 
epoch: 7, train time every whole data:182.67s
epoch: 7, total time:2008.87s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.64s
test time on whole data:47.65s
1.8584515704756515 3.114740237751202 62.222317448357465 0.7370570899154866 
1.931942978993413 3.2760602073437592 64.48116781437588 0.7035275772792353 
1.997201684562046 3.514837031097288 62.007608750066154 0.6624897634061581 
2.0478815980820606 3.6178530164691813 63.18582780618509 0.6395456273069002 
2.1125564646788297 3.7620854397710453 61.983584542900175 0.6147751876405034 
2.1678953438863338 3.88117879649242 61.182572608499285 0.5976657094743306 
2.2242627795074315 4.014034674555162 60.88260526282041 0.5772617741780469 
2.2696273078720663 4.101447507835212 61.35697325415467 0.5641692366533952 
2.3568335037844346 4.2550595789211885 62.744338754483884 0.5458428905453448 
2.420584840621267 4.347001247171897 63.36322296055062 0.5317133644911712 
2.455269128320295 4.375121778046744 63.96031574190817 0.5343430705523459 
2.4631754763943277 4.3810497699789925 63.49945606624002 0.526740855434111 
2.1921402230981797 3.9090230907920906 62.57249145908836 0.526740855434111 
epoch: 8, train time every whole data:182.64s
epoch: 8, total time:2251.25s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.46s
test time on whole data:47.37s
2.257012883335618 3.281728758318841 72.2672475884989 0.7422518891085202 
2.318929319640267 3.430326472783596 72.84700395232639 0.7037471268204728 
2.4095769236904703 3.6573354457512606 72.08015225883824 0.6505874744379225 
2.4567521313802296 3.7576984621535954 71.49890553927173 0.6197697693601191 
2.511489705357967 3.8249113100705494 71.97810580074912 0.588409497882019 
2.533870394728102 3.8204061946593133 72.42292188973116 0.5826857159380471 
2.535796555309601 3.7863598463961567 72.7341807544378 0.5875493689422545 
2.5395348709911287 3.745994697451193 73.75761008958264 0.5942397923468457 
2.543944656819726 3.721866435435361 74.52707631624963 0.5988433103370072 
2.5405732016598894 3.693543417984103 75.01000647595606 0.6075356350127596 
2.527425779569628 3.6976265683314287 73.83282227861922 0.6076841896515068 
2.5420356549616194 3.7356109121965018 74.25498221146492 0.5967780976911756 
2.4764118397870205 3.6827244749416757 73.10097235186177 0.5967780976911756 
epoch: 9, train time every whole data:182.36s
epoch: 9, total time:2494.97s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 29.20s
test time on whole data:48.60s
1.8722329603438044 3.311375286376963 52.02693345547299 0.7330264624752036 
1.9199043050231857 3.4201200578695743 53.98001626515121 0.7105940186904685 
2.0020869621427817 3.6157683153553797 55.63239582720808 0.6770771930292494 
2.031406105528098 3.6691227279425638 57.22682207260824 0.6639952206345653 
2.091331425738122 3.7692007034907746 58.33803917858887 0.641220622131306 
2.1284385374811965 3.8260475963592584 59.10308803861399 0.6305790724955996 
2.162951744627917 3.8878191145312444 60.00897190810369 0.622119508372673 
2.190648110985756 3.9427229572727316 60.93560946977659 0.6130123586271221 
2.241391171162593 4.032999369295591 62.77123117784132 0.6020695453411805 
2.297744074193406 4.119421896853913 64.28975448708782 0.5880349166467493 
2.3663396045020115 4.21898974664015 66.02114165884674 0.5786974194542782 
2.4109990366371794 4.291733382080065 66.59003761732197 0.5594052085825728 
2.1429561698638375 3.8531151479523427 59.74391984000078 0.5594052085825728 
epoch: 10, train time every whole data:183.84s
epoch: 10, total time:2740.35s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.44s
test time on whole data:47.32s
1.9278738522044427 3.0802433982637445 61.39069066436821 0.7552032556905147 
1.9875716373564765 3.212434866802439 62.49021647358282 0.7267321297072051 
2.040342939269241 3.3771885238538464 62.28989053862238 0.6928613001451855 
2.0603941669491608 3.410463038667451 62.81239850421629 0.6813484372689548 
2.0909362591324996 3.4724330826926764 62.25550655860066 0.6611039332177825 
2.123982979075451 3.5160496610194443 62.91308044002165 0.6466679203808758 
2.1427457770443565 3.575379588814179 62.27910252878465 0.6329121877497201 
2.156936086090459 3.620762867341284 62.33349032997467 0.6235988353140887 
2.1726901672061176 3.6817016504727746 62.09535532017077 0.6103196308386154 
2.1938809098178256 3.7255901754755394 62.224254117854514 0.6007084026872379 
2.202339704316198 3.781301843661757 60.65932253083134 0.5945224337252527 
2.2323528312398566 3.843026581538028 60.8643543938986 0.5803524161367537 
2.1110039424751736 3.5315247349870456 62.05062976879309 0.5803524161367537 
epoch: 11, train time every whole data:182.09s
epoch: 11, total time:2981.10s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.44s
test time on whole data:47.37s
1.7588525923432872 2.9027438575419104 59.36776157419581 0.7751879033081968 
1.8262427324754673 3.064990861723819 60.758808137612384 0.7474371572558647 
1.90379891256855 3.2200312044873427 62.4177871737145 0.7160855705661864 
1.9191202198965918 3.2635458941163886 62.677175991047186 0.7067861856751154 
1.9528287876302466 3.318088525858281 63.820937324430346 0.6941777535534628 
1.95330227436826 3.305523376875615 63.80692957699083 0.697591479779783 
1.946084943135136 3.288646244231415 64.09742684523097 0.703156373410007 
1.9321824336933593 3.2696910012087907 63.95801741228956 0.7080768062640362 
1.9336507265798393 3.304282870318258 63.80836539090833 0.7026160228543122 
1.953181460177437 3.3720142732255414 64.02450548947792 0.6908867151372915 
1.978271241223617 3.4606991317631888 63.49088606901672 0.6781891516765023 
2.0087194462501934 3.549639935032238 63.401796418256176 0.6637021852571248 
1.9221863141951654 3.2805848178372132 62.96927530016132 0.6637021852571248 
epoch: 12, train time every whole data:183.74s
epoch: 12, total time:3224.45s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.39s
test time on whole data:47.76s
1.7370153511924935 3.0368345221637725 53.38248821599409 0.7758196151168183 
1.8081170059572906 3.193607942678993 54.73027941194128 0.7515848932476921 
1.8613202567920089 3.317049751756758 55.14328301551357 0.7272896719914594 
1.8843400586713106 3.3485191581171465 55.780732788009715 0.7187053891769304 
1.9137514743349027 3.3763566252416854 56.80211505661267 0.7080527392116798 
1.9297370085977017 3.372529659352474 57.58017234559474 0.7084968605463395 
1.9379036704899655 3.3715532125683114 58.47546969055247 0.7104714562236729 
1.9335437014109145 3.3734524002402337 59.01722472158013 0.7119566076022448 
1.943588789779338 3.4135171288738433 59.32209449707002 0.7085032139436532 
1.9681900437358943 3.4725831265363336 59.89232855476031 0.7004471552147286 
2.0158968602651286 3.5680406913323575 60.972141747350896 0.6856190398481867 
2.0623544293676637 3.6782263497517502 61.40205794803661 0.6614091081364974 
1.9163132208828844 3.3804509555577864 57.70850936434395 0.6614091081364974 
epoch: 13, train time every whole data:182.17s
epoch: 13, total time:3466.59s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 29.00s
test time on whole data:48.58s
1.7296443869317217 2.84084777648095 55.83127680527389 0.7877435909570242 
1.783420018412439 2.937657006986889 57.74016212177828 0.770242818153968 
1.8169713446615885 3.034997964386354 57.71140489749166 0.7571506612970776 
1.842970850505467 3.0715085946652763 57.933981560363016 0.7489033017755299 
1.8741114508919419 3.115749342334542 57.46039843575987 0.7423330264796129 
1.9090515245442234 3.1703846872340566 57.369063957098135 0.7304774201594899 
1.9386164560240826 3.239795092694192 57.00678942311337 0.7158129143881137 
1.9550864596020077 3.284289833420409 56.22623715857553 0.7087113621682108 
1.9711303135358862 3.336038288563406 55.47352534319347 0.7050039797927659 
1.992044741960065 3.392645780064348 55.083592414319746 0.7012415699205986 
2.0223658237988573 3.469466762437555 54.76778769062075 0.6915254703140433 
2.0691095799953634 3.5685287206827057 55.01297554695758 0.6724741698680374 
1.9087102459053036 3.211937125141853 56.46805469342677 0.6724741698680374 
epoch: 14, train time every whole data:182.72s
epoch: 14, total time:3710.24s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.41s
test time on whole data:47.27s
1.8128343039291601 3.0478044105199777 57.49346758635949 0.7887733475032191 
1.8951821852489596 3.1830181955694834 59.942202131637245 0.7727699301842446 
1.9932773847873544 3.4311992514509564 60.478856695430515 0.7517077191749225 
2.0610761377253524 3.544357158561806 61.8918139737425 0.7455693580112963 
2.120442541951491 3.6480259713827508 62.54620743558451 0.7411475503769732 
2.1514882363785235 3.6858462169661372 62.96355086592453 0.7387282790774987 
2.1723636514738733 3.7118759694599905 63.48003362179019 0.7373049697727727 
2.167189554029011 3.717076129524995 63.452552370963346 0.7341434127909351 
2.162781089409299 3.7253484023219503 63.579127418322244 0.7292044442210767 
2.158146831620396 3.7310439653751146 64.15056836790485 0.7196339771032919 
2.172742324322551 3.7639036210022945 64.89856945100658 0.7077560499303894 
2.219096681041288 3.847202129832741 66.24161605948818 0.6847909319728237 
2.090551743493105 3.594108500997287 62.59333849566556 0.6847909319728237 
epoch: 15, train time every whole data:190.51s
epoch: 15, total time:3959.63s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.78s
test time on whole data:47.69s
1.7941400067482498 3.0092534390454646 55.201511891000955 0.7612523232285675 
1.873445165173195 3.175949970001469 57.08300345140314 0.7314108091505236 
1.9438689518212562 3.3830119782085775 56.80482553419104 0.7035029099102276 
2.0160402251652307 3.5382326453748 56.880164696784355 0.6765539859131121 
2.078450295805576 3.659108887231688 57.02320177424536 0.6519857414999846 
2.121910094462956 3.720840844372753 57.473235943916244 0.6329817974741314 
2.157366409415379 3.786061081220689 57.75213471253665 0.6094748794719631 
2.1804317354502247 3.822802025746635 57.64774052384796 0.5980772435948967 
2.1911051346581605 3.843150140832424 57.43658379527403 0.5952549889513553 
2.1902367672899827 3.8384772364480657 57.115859111016135 0.6016726112796054 
2.1900396330581002 3.8501198306348283 55.98264344002689 0.6094341316257966 
2.212220182171535 3.901506591967497 55.37787564130349 0.603386820405467 
2.079104550101654 3.638186589767154 56.814913356688876 0.603386820405467 
epoch: 16, train time every whole data:189.00s
epoch: 16, total time:4225.53s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.44s
test time on whole data:47.33s
1.7137388017514632 2.7539797070604575 57.72967333912098 0.799066563706941 
1.773123951732314 2.910037438170723 58.599573740181285 0.7735098838474835 
1.8061619260602941 2.9941041524014156 57.21867450661948 0.7659076567461974 
1.8349826625079981 3.066045662432905 56.37775148675328 0.7584853943359835 
1.880365974131883 3.1807162165789324 55.18052918965727 0.7474596427085125 
1.9219405784319554 3.271442005907938 54.55027382752873 0.7329902559003196 
1.9515453201538573 3.3421423911228283 54.1705276915701 0.7179646128747408 
1.9654228023610831 3.3637528961157224 53.71999628947896 0.7120473951965438 
1.968247803510832 3.3533938591820993 53.58724007628691 0.7117518039174526 
1.9742932269253015 3.3577848188077737 53.888160816643605 0.7109147447742205 
1.9948891869518197 3.40977441286775 54.08987885732165 0.7012711392545733 
2.0304938819810214 3.4858647463004666 54.569569501791435 0.6903814210519754 
1.9012671763749853 3.2148073995635875 55.30672198630862 0.6903814210519754 
epoch: 17, train time every whole data:181.96s
epoch: 17, total time:4467.16s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.48s
test time on whole data:47.40s
1.7408143016172661 2.864934389749613 53.37393323046196 0.8045858391395546 
1.7824977845349128 2.9692614032404965 54.181666955211185 0.7867049645117896 
1.8079646576109918 3.062512088348517 55.015696662263515 0.7721429712323217 
1.809732277687373 3.0697838235970947 55.125667132984724 0.7683280628681798 
1.8198452366243694 3.073771074424141 55.492994266130324 0.7662526112451302 
1.837691938136926 3.0890236074090325 56.05663402936262 0.7610169746548616 
1.8645302251252744 3.155614793694417 56.64639031058937 0.7472805268204886 
1.891202124723189 3.214269516892831 57.26657815662814 0.7370439104320949 
1.9231593385240329 3.291286073333413 58.00635006784497 0.7241988181789921 
1.947608747663065 3.360866075749202 58.434687239141105 0.7117867402770767 
1.9655065442575586 3.4078728115850665 58.24675092182883 0.7052344202101615 
1.9940048915925657 3.455985106300055 58.35753856975797 0.6937673286431697 
1.865379839008127 3.1726946883038574 56.35050696574323 0.6937673286431697 
epoch: 18, train time every whole data:182.07s
epoch: 18, total time:4710.40s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 29.30s
test time on whole data:49.19s
1.7524788291543012 2.9332761273309855 58.1070266164577 0.7939870222315939 
1.810510855149123 3.046535602520253 60.18766289215293 0.7731404084437324 
1.8701791832705161 3.177268418035643 61.25435497478535 0.7536144080776342 
1.8906432756823266 3.2200247117546428 61.732026293627364 0.7489067273397731 
1.9279185085548531 3.30866818351158 62.59420636182965 0.7356420873488927 
1.9699283720050893 3.3658249396821054 63.52839199619439 0.7283883607772419 
2.006512605847968 3.4253392659766626 64.60386167530706 0.718372095863461 
2.033503597341744 3.4612272809923024 65.56452677262574 0.7091230361717622 
2.067590943279809 3.5177131129536052 66.72389629712534 0.6946206923015381 
2.103453414027446 3.5939339350604596 67.97526337807757 0.6783828172271833 
2.141801460340353 3.6696412619022554 68.25542455852383 0.6661233315760977 
2.171366400963937 3.7432359091598513 68.07223307960923 0.653766803777541 
1.9788239538014556 3.380174262634862 64.05009505530727 0.653766803777541 
epoch: 19, train time every whole data:182.27s
epoch: 19, total time:4954.73s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.43s
test time on whole data:47.31s
1.8089352619610728 2.937109211768684 56.77455088985454 0.772539935660973 
1.8837700867565084 3.12483176804284 57.72774350548813 0.7378358324929423 
1.9343370419353956 3.254840796001554 59.17359611743594 0.7119514550055576 
1.9887638118444455 3.3705913481674576 60.11722507500017 0.6887731286530118 
2.030856226359006 3.4745342942687687 59.78490601127744 0.6685926912650263 
2.068165756641754 3.5626785350679544 59.71781913510708 0.6522569469544222 
2.0937170751796415 3.6169491823373425 59.00533684513505 0.642385722741921 
2.1167035857095784 3.670251527039132 58.24668723682805 0.6327187394422785 
2.1502286370188175 3.734146825133678 57.627523760939816 0.623238808719652 
2.184039013071252 3.7929737085342596 57.012237435052604 0.6165814096888772 
2.218769607964282 3.8561406833922183 56.52827804732781 0.6090014272139622 
2.2554302767142653 3.9106232970039763 56.597645988926324 0.5963890951014837 
2.0611430317630015 3.5373894672505557 58.19277595405817 0.5963890951014837 
epoch: 20, train time every whole data:181.97s
epoch: 20, total time:5196.98s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.44s
test time on whole data:47.33s
1.949749049391154 3.5393242695179294 61.180218895778516 0.7075555885526925 
1.984278015835388 3.572134691734894 63.34424914241333 0.6973041826734139 
2.0248328894629544 3.6444486754771965 65.00868379214768 0.6798148643217192 
2.0452201409386026 3.663095715498854 65.51140149894408 0.6762173259474429 
2.0617710428485383 3.6861727761865017 65.42589815113797 0.6727375777768747 
2.073215590360353 3.7024693933456185 65.17446523408988 0.6696246178124398 
2.0751877207189264 3.718100700836796 64.94852702393428 0.6640091159568823 
2.0643778339016827 3.715893538480041 64.74416646627736 0.6606129529726824 
2.0619480313428102 3.7308214595003872 65.00083728458618 0.6525645901152782 
2.0749202899378503 3.762900031418951 65.58572717599 0.6424552662955166 
2.112364396320922 3.8415865543017076 65.97653202785652 0.6236896425297649 
2.176178507832988 3.9993341715480124 66.59223784237993 0.5840963891445818 
2.058670292407681 3.716477188865069 64.87446670715681 0.5840963891445818 
epoch: 21, train time every whole data:216.05s
epoch: 21, total time:5473.77s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.38s
test time on whole data:48.16s
1.7985690961914758 2.7857412452341954 60.463496848075884 0.7968802702135533 
1.831867486088936 2.8582832796341724 60.781274357028394 0.7855787596508849 
1.8681327885352963 2.97136752516424 58.98666593489159 0.7740223072219788 
1.9035106925244132 3.066521454629297 57.75516723874923 0.7654804351230109 
1.9666239649700445 3.202502051508498 56.95112384017911 0.7517903756626593 
2.0097039482921715 3.2823684019210844 56.52176566118571 0.7414681068213262 
2.0410006153140925 3.3374641396274916 56.39968710260126 0.7320570397425203 
2.058735818462624 3.3684897494276203 56.300457097337386 0.7280028988879022 
2.0713008894358893 3.4085543842481343 56.437538166863945 0.7192767704647094 
2.074191023231085 3.4372624191258105 56.20248146352458 0.7117575881999852 
2.0713811920135328 3.4529993011381253 55.86574484052289 0.7061908078700888 
2.0877253499855066 3.5045867119295573 55.800773708676374 0.6950767746245643 
1.9818952387537556 3.2315684570072105 57.372086667788565 0.6950767746245643 
epoch: 22, train time every whole data:189.35s
epoch: 22, total time:5728.66s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.40s
test time on whole data:48.01s
1.719813296867978 2.90060068595061 55.650663365581096 0.7945095721137533 
1.7727497446647889 3.0289813739591005 56.95590144317259 0.768634515339109 
1.8064010575536107 3.093878140978403 58.337838885197236 0.7514429897033644 
1.8122747538015247 3.092868129289769 57.939995482524964 0.7530046270233917 
1.8328402956725054 3.1392245569885917 57.601347378840074 0.7485646314679821 
1.8530620251645644 3.1851855323534886 57.349940589275015 0.7468713521977747 
1.8794674009380952 3.2325246689256484 57.61494335141314 0.746759221483377 
1.9086332159363444 3.297712491205045 58.14653949284928 0.7409321804405608 
1.9567235695923724 3.406360769181957 58.806362318518744 0.7249288099296282 
2.0083585772542727 3.524720276508047 59.51365958160161 0.7066590855413827 
2.0591415426339066 3.641419699583909 60.11194877200524 0.687943590515656 
2.1038571714336674 3.747703450156591 60.179759453945515 0.6708463384668187 
1.892776887626136 3.283577242891097 58.184131806197136 0.6708463384668187 
epoch: 23, train time every whole data:182.11s
epoch: 23, total time:5971.53s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.66s
test time on whole data:47.65s
1.7466806798279286 2.6909782190842044 53.05700301384005 0.8111788867601009 
1.7851991685886113 2.795990345052863 53.35619099256026 0.7951060421273609 
1.8198868537869837 2.895726626317954 52.79515016887698 0.7824394742175235 
1.848669220466965 2.966775688750585 52.57888323867667 0.7732776543920566 
1.8966362415422642 3.0916728566289136 52.22391184732036 0.7554906451245944 
1.9413661555188397 3.203641891345984 51.96569826218885 0.7367385417618876 
1.981386121619404 3.30639244104226 51.80186645359443 0.7189872983786001 
1.998908555136018 3.334579690223504 51.62415498494695 0.7156177451660637 
2.0100563609449282 3.3546968825735384 51.50994869135742 0.7134655473322415 
2.01121670066379 3.3675455317761642 51.432787664152194 0.7131332858026809 
2.026340760757437 3.4148985988181924 51.85246797409351 0.704840400075646 
2.049288747086855 3.475941004852639 52.12749015267506 0.6939827525770902 
1.9263029638283353 3.168298491355924 52.19376165503608 0.6939827525770902 
epoch: 24, train time every whole data:182.36s
epoch: 24, total time:6214.47s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.44s
test time on whole data:47.32s
1.6365334646629082 2.761334482111757 56.58815573522723 0.8157014255960121 
1.6820730968355424 2.869974407828231 57.59196300888137 0.8022758465315245 
1.7266446362322285 2.9630684068249615 57.87483827300556 0.7958495302899413 
1.7579675758488122 3.0232819158768516 58.183161972425125 0.7897255428363156 
1.7992661833343584 3.09892527361007 58.68361166841415 0.7813880854133611 
1.833334333722524 3.153988148027328 59.26464294090146 0.7718705087367441 
1.8633179733448972 3.2217709633917955 59.84598685671031 0.7594610949033277 
1.8762450025857738 3.2477039142020576 60.17885452077451 0.7552225478836554 
1.892841843096983 3.289702595880252 60.25307351378668 0.7472385846896288 
1.909745278760791 3.325018526289103 60.539747190346326 0.7417382642836304 
1.9256015058815301 3.3682350125135576 60.67016234966521 0.7335115850731372 
1.9620989113963607 3.454983785651327 61.047178679768066 0.7145944632782045 
1.8221391504752258 3.1546250242358105 59.226863621033665 0.7145944632782045 
epoch: 25, train time every whole data:182.22s
epoch: 25, total time:6456.33s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.49s
test time on whole data:47.38s
1.8063776672358314 2.649542645217353 62.77364841329251 0.820870746161086 
1.8266590936693052 2.7181991474369775 62.55563527446748 0.8084245227920484 
1.8290561108484509 2.7643377796285913 60.661669504534544 0.8005150396328408 
1.8380688943070846 2.794554238922078 59.73044309656481 0.7961604993589089 
1.8573691373368992 2.848060766712113 59.1120772825025 0.7874725271822802 
1.8845040382766831 2.896365629309235 59.425223903333645 0.777323178476533 
1.9036484526935078 2.958293420714414 59.44679927919411 0.764971113701063 
1.9126121819999424 2.9812291724825104 59.567970390941774 0.7604489803027644 
1.9199429455892671 3.008776988169202 59.69325986433599 0.7552764472830266 
1.9285335482159363 3.039704329728981 59.76591532997262 0.7499803547539812 
1.9372052009971368 3.080674551105621 59.71193677378083 0.7431965491421519 
1.9742443400838723 3.179187943763903 60.057197827405595 0.7249028905542104 
1.8848518009378263 2.9138995059292485 60.20843034554185 0.7249028905542104 
epoch: 26, train time every whole data:194.45s
epoch: 26, total time:6710.26s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 29.30s
test time on whole data:49.11s
1.7512397951071283 2.882761995176441 58.04118231813463 0.8087092392980346 
1.797099899130208 2.9742673445292658 59.330232819272 0.7963560743537254 
1.8198291585602397 3.0116106815196053 60.75796490048554 0.7886288794084868 
1.828391674731459 3.0021939219886797 61.75028436174631 0.7879742168561907 
1.8627840959719781 3.0575258122051743 62.829202084120084 0.7784033037197147 
1.901200173516802 3.138688731382201 63.9280459155335 0.7636611684269724 
1.940138667805209 3.2313582136684507 64.85329506405162 0.7477591254787916 
1.9703799908046744 3.2931224995258455 65.60662024813635 0.7380080613858404 
2.0047827182359814 3.370056144341077 66.05072321648512 0.7262605438523585 
2.0333279726515925 3.4439437685841843 66.26289639175678 0.715725880272001 
2.0694161796792456 3.531815991790036 66.50696126129138 0.7026123719274533 
2.116168893236844 3.6344506434575776 66.79067428535144 0.6866504422148126 
1.9245632682859468 3.222663180809492 63.559181455163696 0.6866504422148126 
epoch: 27, train time every whole data:182.57s
epoch: 27, total time:6953.78s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 29.23s
test time on whole data:48.18s
1.6904384949032571 2.8448494350277644 55.672368890618216 0.8023301107956268 
1.7330927933278006 2.9240131213496725 57.275983337995505 0.7918133335544519 
1.7582508084452046 2.944240515643843 58.7690428232165 0.7878926142830979 
1.7595602370991061 2.9221119978549974 59.72753168351653 0.7872623491430253 
1.7811166006138637 2.9669031995597575 60.42320633377681 0.7799804806022981 
1.8132988472140084 3.0384377534516696 61.07872986520444 0.7689358767304086 
1.864316139356189 3.1559529930575905 61.62581057113186 0.7511354646127075 
1.9147511491983065 3.278070976797266 61.71056316823251 0.7363722708539188 
1.9757063576282845 3.4316020326520205 61.38881946401988 0.7186035223113794 
2.0377099387688298 3.585312616085578 61.00640376655699 0.7028204439247744 
2.0798225785328874 3.6845469023016433 60.58334017538611 0.6942659780942982 
2.117752770312663 3.763269858575113 60.81537619726765 0.6788722806819725 
1.8771513929500334 3.226958887682568 60.00653058200266 0.6788722806819725 
epoch: 28, train time every whole data:183.87s
epoch: 28, total time:7197.58s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 29.04s
test time on whole data:48.69s
1.682418520629672 2.8419218687972445 53.91357801892867 0.8109691074773774 
1.7128718861060306 2.8939036363412574 55.213295768305514 0.800310809204385 
1.747262701617465 2.95739112572512 56.320926416325094 0.789228482874805 
1.769279613424536 2.991230846868509 56.82102881172224 0.7860535031215768 
1.8050477578584105 3.05875019022108 57.26198116422948 0.7804866703138206 
1.8507476515399204 3.155685104453968 57.889700938953034 0.766599325652659 
1.9086449136866168 3.302426407357536 58.6591989950873 0.7413261877795093 
1.9538352663514338 3.415310286841021 59.09928776640796 0.721837258019398 
1.992479830394898 3.5050207161320137 59.401773607460264 0.7080673513071837 
2.0218251345350984 3.5753135569529326 59.41452623833303 0.6994706510883695 
2.0421023948479977 3.6251053244817277 59.156890750053094 0.6945506714719294 
2.0660601405245917 3.6773617595126664 58.884004379145125 0.6873380030438242 
1.8793813176263892 3.2629473323665414 57.66978709237337 0.6873380030438242 
epoch: 29, train time every whole data:182.20s
epoch: 29, total time:7440.51s
fine tune the model ... 
epoch: 30, train time every whole data:370.85s
epoch: 30, total time:7811.37s
predicting testing set batch 1 / 168, time: 0.30s
predicting testing set batch 101 / 168, time: 28.71s
test time on whole data:47.66s
1.6224153023841126 2.804232599115868 55.70765338641554 0.8044720193347886 
1.6698104172618615 2.897628298086453 56.48605594258142 0.7926243089087144 
1.704305115783942 2.9533862750125643 56.69831691939292 0.786881573831796 
1.7150913777420564 2.9397085819157565 57.22829007560454 0.7896562711652386 
1.7308372956053133 2.938244753944785 57.59068111232284 0.7919494139567919 
1.7431685254529474 2.9448614089809504 58.012236194051766 0.7902689639739299 
1.7597430639586278 2.9856611508891544 58.26057283232914 0.7842110840960671 
1.7711125244866348 3.0137667023830272 58.56988252115598 0.7788015150130213 
1.790356349361617 3.061805908463379 58.82621565702381 0.7705896368484647 
1.8143364220572014 3.1215240675534925 58.8165448530097 0.7631934056508395 
1.85197536329384 3.220945283388762 59.25860813081444 0.7484567342492071 
1.9117474672318924 3.362300547911021 59.944844652361994 0.7249690853540347 
1.757074935385004 3.023862346332804 57.95006168943231 0.7249690853540347 
epoch: 31, train time every whole data:371.89s
epoch: 31, total time:8257.51s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.56s
test time on whole data:47.55s
1.6024564023979364 2.732971189757022 56.641407600512395 0.8091275616216211 
1.6399954018671774 2.7883370594168175 57.46845920482624 0.8022877785339119 
1.6691258545069112 2.8322071640936235 57.51255181712068 0.7979302433711734 
1.684981418964054 2.8414983552570696 57.753089254683886 0.7977159064480838 
1.7081394136279289 2.8652818133278277 58.01534825033969 0.7970682802427267 
1.7336028862614186 2.9022917070392213 58.39780017844245 0.7923939973072299 
1.7603039743551718 2.9792292479638713 58.62520361694642 0.781121964702685 
1.7786503121153052 3.0196272454245774 58.9541222606161 0.7747946353011876 
1.7997862956139836 3.0689745646223883 59.07289377284394 0.7666051609532065 
1.8193518673483993 3.111003560861766 59.131741659296125 0.7611874134938944 
1.8494654693125436 3.184371270634042 59.64786402803796 0.7492330349217278 
1.898203702372898 3.3036424482917224 60.20601891685037 0.7278292953938589 
1.7453385832286439 2.97374315032683 58.4522640046072 0.7278292953938589 
epoch: 32, train time every whole data:366.64s
epoch: 32, total time:8684.25s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.78s
test time on whole data:48.68s
1.5961002916214722 2.698689681729308 57.892040724613715 0.8123119870227145 
1.6304484925412883 2.749828996627147 58.706032945605955 0.8059862061477324 
1.6546281066463284 2.7872933508489703 58.656649276697124 0.8026428378207262 
1.6707274462770494 2.792570292749494 59.14685485830231 0.8037716153733271 
1.6962872621083542 2.8237193342429308 59.64485928128479 0.803091536757939 
1.7229074245959166 2.8746798793711434 60.22172947027946 0.7974815107846739 
1.7563618649376467 2.960967885611379 60.645650717269085 0.7864390749448122 
1.7856047694091464 3.028488720541129 61.07775038043759 0.7769045642564713 
1.8205819528629736 3.1099198178078495 61.463846592739124 0.7650893898160576 
1.8583658860751562 3.196399800164123 61.739367853692286 0.7534175404535437 
1.9025178984734452 3.299541757636345 62.292703120733385 0.7373292782328413 
1.9587922049971918 3.430756376899492 62.91053469253469 0.7149782966632027 
1.7544436333788307 2.987945597086856 60.366589502887976 0.7149782966632027 
epoch: 33, train time every whole data:367.53s
epoch: 33, total time:9112.95s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.55s
test time on whole data:47.49s
1.5931823898545865 2.706097527373427 56.975422230307515 0.8154611082333388 
1.6229642031159075 2.749582220970168 57.8107098139299 0.8101169773654963 
1.6508906160521188 2.7869576588138814 57.93336105977165 0.8064390588713526 
1.6649803007953756 2.785482710798232 58.45254625205391 0.8068148534362476 
1.6890160068704614 2.8170821101806927 59.07870026777835 0.8039982870485606 
1.7096524915786548 2.8544601198021335 59.56292159437265 0.7997878099891701 
1.737569441679333 2.9244314604013906 59.737339817085356 0.791830104226207 
1.7608330829940027 2.976216159072031 59.917976888349365 0.7860672651331859 
1.7899882871989339 3.041425326196296 60.19479454042105 0.7778948980583678 
1.8225695481994855 3.113495838428058 60.5225195378377 0.769140742114125 
1.863307879608568 3.2103838181963074 61.231827064288666 0.754207106679553 
1.9208817580248274 3.3446448439263223 62.025112883630904 0.7321173570721557 
1.7354863338310211 2.948737117645076 59.45368268647307 0.7321173570721557 
epoch: 34, train time every whole data:366.82s
epoch: 34, total time:9537.23s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.54s
test time on whole data:47.48s
1.5819476839921305 2.69532531293535 57.07715414918497 0.8148142472660265 
1.6126000289367068 2.7403879575275685 57.924848830495264 0.8100457529054073 
1.64385703231891 2.786192836557449 58.20621882749407 0.8056494088781051 
1.662471559759052 2.7988523395264595 58.69698185260223 0.8055454512930469 
1.6935390694595518 2.842420042201123 59.234713582692855 0.8024634788100109 
1.7221208463094775 2.901733169667389 59.840575418995954 0.7943985884318452 
1.755666486245181 2.9952369445165963 60.42918031136853 0.7798963913623932 
1.7810265410106096 3.048439450234565 61.041773975452216 0.7715651263946574 
1.8079449852336908 3.1104025034890603 61.436534164999 0.7616410134220826 
1.8349127225986726 3.169212771306676 61.658951718264355 0.7535458304872642 
1.8650942029114812 3.242977799958779 62.11654360742246 0.7414653291510536 
1.912574620572584 3.355636390967391 62.53536728758648 0.7222554376386643 
1.739479648279004 2.980918455834284 60.01667232758678 0.7222554376386643 
epoch: 35, train time every whole data:364.48s
epoch: 35, total time:9960.79s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.55s
test time on whole data:47.50s
1.5841258568689227 2.6788370975738487 55.63986296213831 0.8175330148530493 
1.6163988887297789 2.73321228905496 56.251060423834396 0.8118861328428532 
1.648260633271631 2.7786082413348723 56.25537174520707 0.8089453978594694 
1.6692351736429014 2.799451226912533 56.47177640217882 0.8087945533955212 
1.7022014642507726 2.859523824194265 56.87625194452258 0.8038705237462638 
1.7291615489421501 2.9128086977146412 57.381644044015886 0.7971492169540327 
1.753479094717563 2.980188729886012 57.71020501363021 0.7871840460815859 
1.7685384944317242 3.0136682412087694 58.0874634287819 0.7810215550542162 
1.7871817350417731 3.0540732497454024 58.46280926444023 0.7731165462341103 
1.809205306642201 3.102286081203378 58.88934561958602 0.7649210787847371 
1.834011675097137 3.1589395693338447 59.715304447819065 0.7540004169187192 
1.8783666462526611 3.253355025346866 60.530466191764475 0.7373022545582977 
1.7316805431574347 2.9487853594824025 57.68937423652042 0.7373022545582977 
epoch: 36, train time every whole data:365.52s
epoch: 36, total time:10386.83s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.47s
test time on whole data:47.36s
1.5792734513473474 2.678329431614326 55.36949024208162 0.8203414684356007 
1.6100889567426806 2.7250625699700026 56.10572132529113 0.8155203446977672 
1.6389555932659479 2.7572556373379196 56.49992677334323 0.8122165240324377 
1.6533971388834927 2.761412237112477 56.77567064897122 0.8121108243205709 
1.6779599669797434 2.7975840946243236 57.07235428854817 0.8097452536693116 
1.702822199579417 2.846803232729015 57.58486288421963 0.8030583731487018 
1.7318892994281792 2.9284317016465726 58.06527125489042 0.7906029428333834 
1.752448787088373 2.975475640837676 58.61286370071056 0.7829159251203038 
1.7755876080502888 3.026394417413874 58.99136187520806 0.7739445068053443 
1.7961117071786985 3.069951331731777 59.286972438016804 0.7671533367035547 
1.8205026346183426 3.1259907845888972 59.86025247661792 0.7571763052242024 
1.8649765218846677 3.226842622626786 60.44482313959206 0.739955682355502 
1.7170011554205982 2.914835611798415 57.88921770521958 0.739955682355502 
epoch: 37, train time every whole data:379.10s
epoch: 37, total time:10827.12s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 29.01s
test time on whole data:48.34s
1.5698358969465784 2.651430432810033 56.45237989039141 0.8207802266565376 
1.6017442896564802 2.6991284800415847 56.92261694588488 0.8164297640649453 
1.628998062237565 2.7302920303459213 57.17578390908293 0.8132639920009548 
1.6429068788352998 2.7338383153784886 57.38615558945415 0.813702145749219 
1.6673173285560416 2.7711373797552916 57.69771120316645 0.8106262457500454 
1.6906135136853193 2.8188765125528055 58.15063453480397 0.804093859899591 
1.717619853121805 2.892752651089079 58.59672082445484 0.7928002746748929 
1.7386713630340638 2.93592101410672 59.2761495122194 0.7854939757001783 
1.7655220438225105 2.9953163060099746 59.83794310880685 0.7752124285077707 
1.7901431371665426 3.0552189073424008 60.20100025083238 0.7657549177321259 
1.8188899518663861 3.1240205124339178 60.83065340172531 0.7540515318549377 
1.8638123811132141 3.230457932603687 61.382033174534456 0.7363432853557295 
1.708006225003484 2.8919184323245486 58.6592354292386 0.7363432853557295 
epoch: 38, train time every whole data:386.04s
epoch: 38, total time:11274.36s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.81s
test time on whole data:48.82s
1.5623239008307102 2.6089588312125978 57.040804680687906 0.8263927574145005 
1.589300226721824 2.649779708533515 57.408806649386754 0.8230216631572169 
1.6192699020735564 2.6980131176498103 57.47832622164138 0.8189690796781683 
1.6366805802141982 2.7153751887083803 57.988627207645585 0.8178504257091955 
1.6674881267594617 2.7719412582964083 58.62803209030415 0.8121298592932149 
1.6971720725030062 2.833273840992363 59.37615566457364 0.8041103050631708 
1.7311866416375907 2.913839461758293 60.061648877451034 0.7927320448070049 
1.7599530598383752 2.977482597090999 60.83166880844384 0.7824403509344102 
1.7939758332253744 3.061852295338173 61.46177452310098 0.7685532666648245 
1.8213392583858221 3.132160854939216 61.754130907048356 0.7577750619469037 
1.8509561623217805 3.2032548764211546 62.35435996632596 0.7460007278307709 
1.8947949661869734 3.2969276612739424 62.86011263050402 0.7313345318844079 
1.7187033942248895 2.913523680102685 59.77048618065923 0.7313345318844079 
epoch: 39, train time every whole data:410.17s
epoch: 39, total time:11746.07s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.49s
test time on whole data:47.39s
1.558212429355209 2.582785155390196 56.825500883717176 0.8310450022138512 
1.5844484131696324 2.6228315385794247 57.39432734889087 0.8267696309674282 
1.6156232951893692 2.6845325800952002 57.51483603108182 0.8201219726529447 
1.6381284465768509 2.7243602012309998 57.702551953371504 0.8167442261163226 
1.6709806302506476 2.7946385536264935 57.96370198121114 0.8109209077762418 
1.6983460517748303 2.851263576246048 58.45085621670112 0.8047143548210474 
1.7238696818260388 2.913026602464517 58.932634939263416 0.7955624945610927 
1.7389413884454186 2.93990914956107 59.53953912787402 0.789587901197539 
1.7586563909474229 2.9822441037419654 60.19870438151321 0.7808596202178 
1.7784804736166482 3.0327766959227054 60.7930250536737 0.7719002046696429 
1.8073077894537044 3.0948291129805834 61.85407104080968 0.7608590355515433 
1.8528317261784382 3.1937132791132608 62.631740596386564 0.7453509389331491 
1.7021522263986841 2.8739731886459445 59.15021736177298 0.7453509389331491 
epoch: 40, train time every whole data:372.78s
epoch: 40, total time:12177.83s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.56s
test time on whole data:47.50s
1.5541854644894069 2.552074487178611 56.56257680507165 0.8344751755088885 
1.5847441979414296 2.6318003659728966 56.966478436032844 0.8252157311118732 
1.6171514810170269 2.700304483749184 57.18971563051275 0.8171533915095002 
1.631619264741916 2.7160353981048995 57.643010591260655 0.8158095329645764 
1.6563820796890982 2.759722229046353 58.24465965334619 0.8115850155549537 
1.6825821356759185 2.8158270984989207 59.07161438234155 0.8037727331726499 
1.7115652373092516 2.8918304048594305 59.837091159138225 0.792298216439092 
1.735516214898477 2.9438297353666054 60.638779803538824 0.783743641706526 
1.7678013321408736 3.0207185513627977 61.16437587122759 0.7712720587014388 
1.7942466528113221 3.0803847582853177 61.33689094605535 0.7626621459688137 
1.827489416079507 3.16030216732964 61.92076931164002 0.7493092424031723 
1.8719412696501683 3.2633569756423557 62.27457927843576 0.7329044736048534 
1.702935395537033 2.8857730757908855 59.4043285002934 0.7329044736048534 
epoch: 41, train time every whole data:366.24s
epoch: 41, total time:12603.45s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 29.85s
test time on whole data:49.74s
1.5546431722748315 2.5467163981544507 56.71624957629149 0.8361460346649865 
1.5826868635583669 2.603347667441326 57.43472653947409 0.8291736116639636 
1.6108668841925404 2.653748061347544 57.720398542294745 0.8231628255514432 
1.6226269549364667 2.670737476430268 58.22267714264326 0.8209271329534719 
1.641609010497463 2.7038958567476086 58.78283134209309 0.8172241681786792 
1.661787817346553 2.74636671020668 59.574889684440954 0.8104210974877146 
1.682919858269837 2.7983118934775577 60.12935886055736 0.802302844060777 
1.7012192930739727 2.840366323778628 60.72275473055665 0.7955163925169354 
1.7258939756784766 2.8973231365515115 61.04271950292508 0.7872904323020004 
1.7509352353318994 2.9700680233396635 61.148241734194976 0.7771308812398326 
1.7847983498384379 3.0493353867521815 61.57166873146189 0.765879971412197 
1.836765085412899 3.164733881023881 62.00197599478394 0.7499720986911078 
1.679729375034312 2.809507465854099 59.58914352481488 0.7499720986911078 
epoch: 42, train time every whole data:367.02s
epoch: 42, total time:13033.13s
