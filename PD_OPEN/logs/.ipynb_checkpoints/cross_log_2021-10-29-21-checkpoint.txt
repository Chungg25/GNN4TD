CUDA: True cuda:0
Read configuration file: configurations/bike.conf
total training epoch, fine tune epoch: 100 , 50
batch_size: 4
folder_dir: MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross
load file: data/bike.npz
ori length: 3001 , percent: 1.0 , scale: 3001
train: torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12])
val: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
test: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
TemporalPositionalEncoding max_len: 13
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
EncoderDecoder_ms(
  (encoder): Encoder_ms(
    (layers): ModuleList(
      (0): EncoderLayer_ms(
        (layer): EncoderLayer(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): ModuleList(
            (0): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
            (1): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (layer2): EncoderLayer(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): ModuleList(
            (0): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
            (1): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (layerc): EncoderLayer_cross(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): SublayerConnection2(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (sublayer_): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layerc2): EncoderLayer_cross(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): SublayerConnection2(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (sublayer_): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer_ms(
        (layer): EncoderLayer(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): ModuleList(
            (0): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
            (1): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (layer2): EncoderLayer(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): ModuleList(
            (0): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
            (1): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (layerc): EncoderLayer_cross(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): SublayerConnection2(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (sublayer_): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layerc2): EncoderLayer_cross(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): SublayerConnection2(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (sublayer_): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer_ms(
        (layer): EncoderLayer(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): ModuleList(
            (0): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
            (1): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (layer2): EncoderLayer(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): ModuleList(
            (0): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
            (1): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (layerc): EncoderLayer_cross(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): SublayerConnection2(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (sublayer_): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layerc2): EncoderLayer_cross(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): SublayerConnection2(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (sublayer_): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): EncoderLayer_ms(
        (layer): EncoderLayer(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): ModuleList(
            (0): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
            (1): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (layer2): EncoderLayer(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): ModuleList(
            (0): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
            (1): SublayerConnection(
              (dropout): Dropout(p=0.0, inplace=False)
              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (layerc): EncoderLayer_cross(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): SublayerConnection2(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (sublayer_): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layerc2): EncoderLayer_cross(
          (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward_gcn): PositionWiseGCNFeedForward(
            (gcn): spatialAttentionScaledGCN(
              (Theta): Linear(in_features=64, out_features=64, bias=False)
              (SAt): Spatial_Attention_layer(
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (sublayer): SublayerConnection2(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (sublayer_): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder2): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (src_embed2): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed2): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
  (prediction_generator2): Linear(in_features=64, out_features=1, bias=True)
)
create params directory experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross
Net's state_dict:
encoder.layers.0.layer.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.layer.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.layer.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.layer.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.layer.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.layer.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.layer.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.layer.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.layer.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.layer.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.layer.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.layer.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.layer.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.0.layer2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.layer2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.layer2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.layer2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.layer2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.layer2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.layer2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.layer2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.layer2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.layer2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.layer2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.layer2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.layer2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.0.layerc.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.layerc.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.layerc.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.layerc.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.layerc.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.layerc.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.layerc.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.layerc.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.layerc.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.layerc.sublayer.norm.weight 	 torch.Size([64])
encoder.layers.0.layerc.sublayer.norm.bias 	 torch.Size([64])
encoder.layers.0.layerc.sublayer_.norm.weight 	 torch.Size([64])
encoder.layers.0.layerc.sublayer_.norm.bias 	 torch.Size([64])
encoder.layers.0.layerc2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.layerc2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.layerc2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.layerc2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.layerc2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.layerc2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.layerc2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.layerc2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.layerc2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.layerc2.sublayer.norm.weight 	 torch.Size([64])
encoder.layers.0.layerc2.sublayer.norm.bias 	 torch.Size([64])
encoder.layers.0.layerc2.sublayer_.norm.weight 	 torch.Size([64])
encoder.layers.0.layerc2.sublayer_.norm.bias 	 torch.Size([64])
encoder.layers.1.layer.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.layer.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.layer.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.layer.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.layer.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.layer.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.layer.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.layer.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.layer.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.layer.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.layer.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.layer.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.layer.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.layer2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.layer2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.layer2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.layer2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.layer2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.layer2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.layer2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.layer2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.layer2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.layer2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.layer2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.layer2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.layer2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.layerc.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.layerc.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.layerc.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.layerc.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.layerc.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.layerc.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.layerc.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.layerc.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.layerc.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.layerc.sublayer.norm.weight 	 torch.Size([64])
encoder.layers.1.layerc.sublayer.norm.bias 	 torch.Size([64])
encoder.layers.1.layerc.sublayer_.norm.weight 	 torch.Size([64])
encoder.layers.1.layerc.sublayer_.norm.bias 	 torch.Size([64])
encoder.layers.1.layerc2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.layerc2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.layerc2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.layerc2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.layerc2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.layerc2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.layerc2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.layerc2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.layerc2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.layerc2.sublayer.norm.weight 	 torch.Size([64])
encoder.layers.1.layerc2.sublayer.norm.bias 	 torch.Size([64])
encoder.layers.1.layerc2.sublayer_.norm.weight 	 torch.Size([64])
encoder.layers.1.layerc2.sublayer_.norm.bias 	 torch.Size([64])
encoder.layers.2.layer.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.layer.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.layer.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.layer.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.layer.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.layer.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.layer.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.layer.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.layer.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.layer.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.layer.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.layer.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.layer.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.layer2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.layer2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.layer2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.layer2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.layer2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.layer2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.layer2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.layer2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.layer2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.layer2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.layer2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.layer2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.layer2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.layerc.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.layerc.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.layerc.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.layerc.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.layerc.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.layerc.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.layerc.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.layerc.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.layerc.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.layerc.sublayer.norm.weight 	 torch.Size([64])
encoder.layers.2.layerc.sublayer.norm.bias 	 torch.Size([64])
encoder.layers.2.layerc.sublayer_.norm.weight 	 torch.Size([64])
encoder.layers.2.layerc.sublayer_.norm.bias 	 torch.Size([64])
encoder.layers.2.layerc2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.layerc2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.layerc2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.layerc2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.layerc2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.layerc2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.layerc2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.layerc2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.layerc2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.layerc2.sublayer.norm.weight 	 torch.Size([64])
encoder.layers.2.layerc2.sublayer.norm.bias 	 torch.Size([64])
encoder.layers.2.layerc2.sublayer_.norm.weight 	 torch.Size([64])
encoder.layers.2.layerc2.sublayer_.norm.bias 	 torch.Size([64])
encoder.layers.3.layer.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.layer.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.layer.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.layer.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.layer.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.layer.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.layer.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.layer.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.layer.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.layer.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.layer.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.layer.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.layer.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.layer2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.layer2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.layer2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.layer2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.layer2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.layer2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.layer2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.layer2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.layer2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.layer2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.layer2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.layer2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.layer2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.layerc.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.layerc.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.layerc.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.layerc.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.layerc.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.layerc.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.layerc.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.layerc.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.layerc.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.layerc.sublayer.norm.weight 	 torch.Size([64])
encoder.layers.3.layerc.sublayer.norm.bias 	 torch.Size([64])
encoder.layers.3.layerc.sublayer_.norm.weight 	 torch.Size([64])
encoder.layers.3.layerc.sublayer_.norm.bias 	 torch.Size([64])
encoder.layers.3.layerc2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.layerc2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.layerc2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.layerc2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.layerc2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.layerc2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.layerc2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.layerc2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.layerc2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.layerc2.sublayer.norm.weight 	 torch.Size([64])
encoder.layers.3.layerc2.sublayer.norm.bias 	 torch.Size([64])
encoder.layers.3.layerc2.sublayer_.norm.weight 	 torch.Size([64])
encoder.layers.3.layerc2.sublayer_.norm.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
encoder.norm2.weight 	 torch.Size([64])
encoder.norm2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.0.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.0.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder2.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.1.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.1.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder2.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.2.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.2.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder2.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.3.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.3.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder2.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.norm.weight 	 torch.Size([64])
decoder2.norm.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 13, 64])
src_embed.2.embedding.weight 	 torch.Size([250, 64])
src_embed2.0.weight 	 torch.Size([64, 1])
src_embed2.0.bias 	 torch.Size([64])
src_embed2.1.pe 	 torch.Size([1, 1, 13, 64])
src_embed2.2.embedding.weight 	 torch.Size([250, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([250, 64])
trg_embed2.0.weight 	 torch.Size([64, 1])
trg_embed2.0.bias 	 torch.Size([64])
trg_embed2.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed2.2.embedding.weight 	 torch.Size([250, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
prediction_generator2.weight 	 torch.Size([1, 64])
prediction_generator2.bias 	 torch.Size([1])
Net's total params: 1230594
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415]}]
validation batch 1 / 168, loss: 0.46
validation batch 101 / 168, loss: 0.50
validation cost time: 55.3389s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_0.params
epoch: 0, learning rate 0.001000
epoch: 0, train time every whole data:223.21s
epoch: 0, total time:278.75s
validation batch 1 / 168, loss: 0.08
validation batch 101 / 168, loss: 0.05
validation cost time: 56.0323s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_1.params
epoch: 1, learning rate 0.001000
epoch: 1, train time every whole data:224.30s
epoch: 1, total time:559.33s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.03
validation cost time: 56.2626s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_2.params
epoch: 2, learning rate 0.001000
epoch: 2, train time every whole data:224.08s
epoch: 2, total time:839.83s
validation batch 1 / 168, loss: 0.08
validation batch 101 / 168, loss: 0.03
validation cost time: 56.4654s
epoch: 3, learning rate 0.001000
epoch: 3, train time every whole data:224.17s
epoch: 3, total time:1120.47s
validation batch 1 / 168, loss: 0.11
validation batch 101 / 168, loss: 0.04
validation cost time: 56.3277s
epoch: 4, learning rate 0.001000
epoch: 4, train time every whole data:224.44s
epoch: 4, total time:1401.25s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.04
validation cost time: 56.2724s
epoch: 5, learning rate 0.001000
epoch: 5, train time every whole data:224.44s
epoch: 5, total time:1681.96s
validation batch 1 / 168, loss: 0.08
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2136s
epoch: 6, learning rate 0.001000
epoch: 6, train time every whole data:224.46s
epoch: 6, total time:1962.64s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.03
validation cost time: 56.3033s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_7.params
epoch: 7, learning rate 0.001000
epoch: 7, train time every whole data:224.42s
epoch: 7, total time:2243.54s
validation batch 1 / 168, loss: 0.08
validation batch 101 / 168, loss: 0.04
validation cost time: 56.3592s
epoch: 8, learning rate 0.001000
epoch: 8, train time every whole data:224.27s
epoch: 8, total time:2524.18s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4526s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_9.params
epoch: 9, learning rate 0.001000
epoch: 9, train time every whole data:224.26s
epoch: 9, total time:2805.06s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2893s
epoch: 10, learning rate 0.001000
epoch: 10, train time every whole data:224.54s
epoch: 10, total time:3085.90s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.04
validation cost time: 56.4322s
epoch: 11, learning rate 0.001000
epoch: 11, train time every whole data:224.69s
epoch: 11, total time:3367.03s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4073s
epoch: 12, learning rate 0.001000
epoch: 12, train time every whole data:224.59s
epoch: 12, total time:3648.03s
validation batch 1 / 168, loss: 0.04
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1482s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_13.params
epoch: 13, learning rate 0.001000
epoch: 13, train time every whole data:224.73s
epoch: 13, total time:3929.12s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4140s
epoch: 14, learning rate 0.001000
epoch: 14, train time every whole data:226.34s
epoch: 14, total time:4211.88s
validation batch 1 / 168, loss: 0.04
validation batch 101 / 168, loss: 0.03
validation cost time: 56.4935s
epoch: 15, learning rate 0.001000
epoch: 15, train time every whole data:226.16s
epoch: 15, total time:4494.54s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.5000s
epoch: 16, learning rate 0.001000
epoch: 16, train time every whole data:226.82s
epoch: 16, total time:4777.86s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.03
validation cost time: 56.4819s
epoch: 17, learning rate 0.001000
epoch: 17, train time every whole data:224.53s
epoch: 17, total time:5058.88s
validation batch 1 / 168, loss: 0.07
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1991s
epoch: 18, learning rate 0.001000
epoch: 18, train time every whole data:224.54s
epoch: 18, total time:5339.63s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3547s
epoch: 19, learning rate 0.001000
epoch: 19, train time every whole data:224.43s
epoch: 19, total time:5620.42s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1634s
epoch: 20, learning rate 0.001000
epoch: 20, train time every whole data:224.56s
epoch: 20, total time:5901.15s
validation batch 1 / 168, loss: 0.04
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2021s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_21.params
epoch: 21, learning rate 0.001000
epoch: 21, train time every whole data:224.67s
epoch: 21, total time:6186.20s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2775s
epoch: 22, learning rate 0.001000
epoch: 22, train time every whole data:224.34s
epoch: 22, total time:6466.82s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2256s
epoch: 23, learning rate 0.001000
epoch: 23, train time every whole data:224.43s
epoch: 23, total time:6747.48s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3817s
epoch: 24, learning rate 0.001000
epoch: 24, train time every whole data:224.56s
epoch: 24, total time:7028.42s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1818s
epoch: 25, learning rate 0.001000
epoch: 25, train time every whole data:224.71s
epoch: 25, total time:7309.32s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4110s
epoch: 26, learning rate 0.001000
epoch: 26, train time every whole data:224.59s
epoch: 26, total time:7590.33s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3694s
epoch: 27, learning rate 0.001000
epoch: 27, train time every whole data:224.80s
epoch: 27, total time:7871.51s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2082s
epoch: 28, learning rate 0.001000
epoch: 28, train time every whole data:224.47s
epoch: 28, total time:8152.18s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3058s
epoch: 29, learning rate 0.001000
epoch: 29, train time every whole data:224.59s
epoch: 29, total time:8433.08s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4138s
epoch: 30, learning rate 0.001000
epoch: 30, train time every whole data:224.72s
epoch: 30, total time:8714.22s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4568s
epoch: 31, learning rate 0.001000
epoch: 31, train time every whole data:224.71s
epoch: 31, total time:8995.39s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3262s
epoch: 32, learning rate 0.001000
epoch: 32, train time every whole data:224.57s
epoch: 32, total time:9276.30s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4186s
epoch: 33, learning rate 0.001000
epoch: 33, train time every whole data:224.66s
epoch: 33, total time:9557.38s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4282s
epoch: 34, learning rate 0.001000
epoch: 34, train time every whole data:224.35s
epoch: 34, total time:9838.17s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4451s
epoch: 35, learning rate 0.001000
epoch: 35, train time every whole data:224.31s
epoch: 35, total time:10118.93s
validation batch 1 / 168, loss: 0.07
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0992s
epoch: 36, learning rate 0.001000
epoch: 36, train time every whole data:224.69s
epoch: 36, total time:10399.73s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3616s
epoch: 37, learning rate 0.001000
epoch: 37, train time every whole data:224.69s
epoch: 37, total time:10680.79s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2310s
epoch: 38, learning rate 0.001000
epoch: 38, train time every whole data:224.63s
epoch: 38, total time:10961.66s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2563s
epoch: 39, learning rate 0.001000
epoch: 39, train time every whole data:224.67s
epoch: 39, total time:11242.58s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2273s
epoch: 40, learning rate 0.001000
epoch: 40, train time every whole data:224.40s
epoch: 40, total time:11523.21s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4025s
epoch: 41, learning rate 0.001000
epoch: 41, train time every whole data:224.59s
epoch: 41, total time:11804.21s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4631s
epoch: 42, learning rate 0.001000
epoch: 42, train time every whole data:224.56s
epoch: 42, total time:12085.23s
validation batch 1 / 168, loss: 0.07
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2963s
epoch: 43, learning rate 0.001000
epoch: 43, train time every whole data:224.70s
epoch: 43, total time:12366.23s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3390s
epoch: 44, learning rate 0.001000
epoch: 44, train time every whole data:224.64s
epoch: 44, total time:12647.21s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4287s
epoch: 45, learning rate 0.001000
epoch: 45, train time every whole data:224.68s
epoch: 45, total time:12928.33s
validation batch 1 / 168, loss: 0.08
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3363s
epoch: 46, learning rate 0.001000
epoch: 46, train time every whole data:224.58s
epoch: 46, total time:13209.25s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4652s
epoch: 47, learning rate 0.001000
epoch: 47, train time every whole data:224.51s
epoch: 47, total time:13490.23s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3566s
epoch: 48, learning rate 0.001000
epoch: 48, train time every whole data:224.47s
epoch: 48, total time:13771.07s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4566s
epoch: 49, learning rate 0.001000
epoch: 49, train time every whole data:224.64s
epoch: 49, total time:14052.17s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3076s
epoch: 50, learning rate 0.001000
epoch: 50, train time every whole data:224.75s
epoch: 50, total time:14333.24s
validation batch 1 / 168, loss: 0.07
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3562s
epoch: 51, learning rate 0.001000
epoch: 51, train time every whole data:224.63s
epoch: 51, total time:14614.23s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3483s
epoch: 52, learning rate 0.001000
epoch: 52, train time every whole data:224.64s
epoch: 52, total time:14895.23s
validation batch 1 / 168, loss: 0.07
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3372s
epoch: 53, learning rate 0.001000
epoch: 53, train time every whole data:224.38s
epoch: 53, total time:15175.95s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3631s
epoch: 54, learning rate 0.001000
epoch: 54, train time every whole data:224.48s
epoch: 54, total time:15456.79s
validation batch 1 / 168, loss: 0.07
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3802s
epoch: 55, learning rate 0.001000
epoch: 55, train time every whole data:224.58s
epoch: 55, total time:15737.76s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1501s
epoch: 56, learning rate 0.001000
epoch: 56, train time every whole data:224.48s
epoch: 56, total time:16018.39s
validation batch 1 / 168, loss: 0.07
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2456s
epoch: 57, learning rate 0.001000
epoch: 57, train time every whole data:224.70s
epoch: 57, total time:16299.35s
validation batch 1 / 168, loss: 0.08
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2428s
epoch: 58, learning rate 0.001000
epoch: 58, train time every whole data:224.60s
epoch: 58, total time:16580.19s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4136s
epoch: 59, learning rate 0.001000
epoch: 59, train time every whole data:224.24s
epoch: 59, total time:16860.85s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4409s
epoch: 60, learning rate 0.001000
epoch: 60, train time every whole data:224.50s
epoch: 60, total time:17141.79s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3644s
epoch: 61, learning rate 0.001000
epoch: 61, train time every whole data:224.64s
epoch: 61, total time:17422.80s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2644s
epoch: 62, learning rate 0.001000
epoch: 62, train time every whole data:224.49s
epoch: 62, total time:17703.56s
validation batch 1 / 168, loss: 0.07
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3166s
epoch: 63, learning rate 0.001000
epoch: 63, train time every whole data:224.71s
epoch: 63, total time:17984.59s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2347s
epoch: 64, learning rate 0.001000
epoch: 64, train time every whole data:224.69s
epoch: 64, total time:18265.52s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.4726s
epoch: 65, learning rate 0.001000
epoch: 65, train time every whole data:224.42s
epoch: 65, total time:18546.42s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3599s
epoch: 66, learning rate 0.001000
epoch: 66, train time every whole data:224.38s
epoch: 66, total time:18827.17s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2422s
epoch: 67, learning rate 0.001000
epoch: 67, train time every whole data:224.62s
epoch: 67, total time:19108.04s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2274s
epoch: 68, learning rate 0.001000
epoch: 68, train time every whole data:224.58s
epoch: 68, total time:19388.85s
validation batch 1 / 168, loss: 0.08
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1793s
epoch: 69, learning rate 0.001000
epoch: 69, train time every whole data:224.58s
epoch: 69, total time:19669.62s
validation batch 1 / 168, loss: 0.07
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3320s
epoch: 70, learning rate 0.001000
epoch: 70, train time every whole data:225.02s
epoch: 70, total time:19950.98s
validation batch 1 / 168, loss: 0.07
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2636s
epoch: 71, learning rate 0.001000
epoch: 71, train time every whole data:223.78s
epoch: 71, total time:20231.03s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1881s
epoch: 72, learning rate 0.001000
epoch: 72, train time every whole data:223.54s
epoch: 72, total time:20510.77s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0567s
epoch: 73, learning rate 0.001000
epoch: 73, train time every whole data:223.70s
epoch: 73, total time:20790.53s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 55.9460s
epoch: 74, learning rate 0.001000
epoch: 74, train time every whole data:223.52s
epoch: 74, total time:21070.00s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1735s
epoch: 75, learning rate 0.001000
epoch: 75, train time every whole data:223.71s
epoch: 75, total time:21349.89s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0462s
epoch: 76, learning rate 0.001000
epoch: 76, train time every whole data:223.78s
epoch: 76, total time:21629.72s
validation batch 1 / 168, loss: 0.07
validation batch 101 / 168, loss: 0.02
validation cost time: 55.9891s
epoch: 77, learning rate 0.001000
epoch: 77, train time every whole data:223.73s
epoch: 77, total time:21909.44s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1391s
epoch: 78, learning rate 0.001000
epoch: 78, train time every whole data:223.73s
epoch: 78, total time:22189.31s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1271s
epoch: 79, learning rate 0.001000
epoch: 79, train time every whole data:223.31s
epoch: 79, total time:22468.75s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2157s
epoch: 80, learning rate 0.001000
epoch: 80, train time every whole data:223.79s
epoch: 80, total time:22748.77s
validation batch 1 / 168, loss: 0.07
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1059s
epoch: 81, learning rate 0.001000
epoch: 81, train time every whole data:223.79s
epoch: 81, total time:23028.67s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0979s
epoch: 82, learning rate 0.001000
epoch: 82, train time every whole data:223.58s
epoch: 82, total time:23308.35s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 55.9932s
epoch: 83, learning rate 0.001000
epoch: 83, train time every whole data:223.77s
epoch: 83, total time:23588.12s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0989s
epoch: 84, learning rate 0.001000
epoch: 84, train time every whole data:223.83s
epoch: 84, total time:23868.05s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0710s
epoch: 85, learning rate 0.001000
epoch: 85, train time every whole data:223.69s
epoch: 85, total time:24147.82s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0393s
epoch: 86, learning rate 0.001000
epoch: 86, train time every whole data:223.79s
epoch: 86, total time:24427.66s
validation batch 1 / 168, loss: 0.07
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1916s
epoch: 87, learning rate 0.001000
epoch: 87, train time every whole data:223.77s
epoch: 87, total time:24707.63s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2120s
epoch: 88, learning rate 0.001000
epoch: 88, train time every whole data:223.80s
epoch: 88, total time:24987.65s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1321s
epoch: 89, learning rate 0.001000
epoch: 89, train time every whole data:223.79s
epoch: 89, total time:25267.58s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2499s
epoch: 90, learning rate 0.001000
epoch: 90, train time every whole data:223.78s
epoch: 90, total time:25547.62s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1336s
epoch: 91, learning rate 0.001000
epoch: 91, train time every whole data:223.76s
epoch: 91, total time:25827.52s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1468s
epoch: 92, learning rate 0.001000
epoch: 92, train time every whole data:223.74s
epoch: 92, total time:26107.41s
validation batch 1 / 168, loss: 0.07
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2188s
epoch: 93, learning rate 0.001000
epoch: 93, train time every whole data:223.73s
epoch: 93, total time:26387.36s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 55.9621s
epoch: 94, learning rate 0.001000
epoch: 94, train time every whole data:223.64s
epoch: 94, total time:26666.97s
validation batch 1 / 168, loss: 0.05
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0817s
epoch: 95, learning rate 0.001000
epoch: 95, train time every whole data:223.82s
epoch: 95, total time:26946.88s
validation batch 1 / 168, loss: 0.07
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2016s
epoch: 96, learning rate 0.001000
epoch: 96, train time every whole data:223.77s
epoch: 96, total time:27226.85s
validation batch 1 / 168, loss: 0.08
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2175s
epoch: 97, learning rate 0.001000
epoch: 97, train time every whole data:223.87s
epoch: 97, total time:27506.94s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2141s
epoch: 98, learning rate 0.001000
epoch: 98, train time every whole data:223.84s
epoch: 98, total time:27787.00s
validation batch 1 / 168, loss: 0.06
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1783s
epoch: 99, learning rate 0.001000
epoch: 99, train time every whole data:223.79s
epoch: 99, total time:28066.98s
best epoch: 21
apply the best val model on the test data set ...
load weight from: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_21.params
predicting testing set batch 1 / 168, time: 0.33s
predicting testing set batch 101 / 168, time: 33.78s
test time on whole data:56.20s
input: (672, 250, 12, 2)
prediction: (672, 250, 12, 2)
data_target_tensor: (672, 250, 12, 2)
current epoch: 21, predict 0 points
MAE: 1.66
RMSE: 2.74
MAPE: 51.22
PCC: 0.84
current epoch: 21, predict 1 points
MAE: 1.74
RMSE: 2.87
MAPE: 53.85
PCC: 0.82
current epoch: 21, predict 2 points
MAE: 1.82
RMSE: 3.04
MAPE: 55.69
PCC: 0.79
current epoch: 21, predict 3 points
MAE: 1.92
RMSE: 3.24
MAPE: 58.35
PCC: 0.75
current epoch: 21, predict 4 points
MAE: 2.02
RMSE: 3.46
MAPE: 60.81
PCC: 0.70
current epoch: 21, predict 5 points
MAE: 2.11
RMSE: 3.65
MAPE: 63.05
PCC: 0.66
current epoch: 21, predict 6 points
MAE: 2.18
RMSE: 3.82
MAPE: 64.88
PCC: 0.62
current epoch: 21, predict 7 points
MAE: 2.22
RMSE: 3.89
MAPE: 65.84
PCC: 0.61
current epoch: 21, predict 8 points
MAE: 2.24
RMSE: 3.94
MAPE: 66.29
PCC: 0.60
current epoch: 21, predict 9 points
MAE: 2.25
RMSE: 3.97
MAPE: 66.13
PCC: 0.59
current epoch: 21, predict 10 points
MAE: 2.27
RMSE: 4.01
MAPE: 66.04
PCC: 0.59
current epoch: 21, predict 11 points
MAE: 2.30
RMSE: 4.08
MAPE: 66.08
PCC: 0.58
all MAE: 2.06
all RMSE: 3.59
all MAPE: 61.52
all PCC: 0.58
1.655301105323292 2.740443409908004 51.21776711978042 0.8411135491517717 
1.7364667947285233 2.8721620674505073 53.84840957637304 0.8173553619280881 
1.8216446542294606 3.0360882206238347 55.69238542882814 0.786792533563968 
1.9169497904993948 3.2393193695805946 58.351576363677424 0.7455728389389039 
2.017906796297502 3.4575269626518024 60.81394266738614 0.7008794583136269 
2.1063712024233703 3.6546864616482746 63.05079030258537 0.6591667786176528 
2.1775230445047575 3.815778242772149 64.88350354011226 0.6249048233937273 
2.2152323655165023 3.894511779313377 65.8431949392129 0.6078416745919629 
2.236636015535199 3.9366512346398106 66.29210269579718 0.5994347999836064 
2.2531119542987574 3.968750430435118 66.1331788441546 0.5948513145514208 
2.2711050770581656 4.006571602919889 66.04314267908515 0.5918276943428097 
2.3037782552817037 4.081747598218207 66.08233866592519 0.5764283972234223 
2.059335587974719 3.5878319881343166 61.521346267900334 0.5764283972234223 
fine tune the model ... 
epoch: 100, learning rate 0.000100
epoch: 100, train time every whole data:440.59s
epoch: 100, total time:28574.67s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2048s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_100.params
epoch: 101, learning rate 0.000100
epoch: 101, train time every whole data:440.93s
epoch: 101, total time:29071.95s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1813s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_101.params
epoch: 102, learning rate 0.000100
epoch: 102, train time every whole data:441.03s
epoch: 102, total time:29569.31s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1352s
epoch: 103, learning rate 0.000100
epoch: 103, train time every whole data:440.69s
epoch: 103, total time:30066.14s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0935s
epoch: 104, learning rate 0.000100
epoch: 104, train time every whole data:441.00s
epoch: 104, total time:30563.24s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0801s
epoch: 105, learning rate 0.000100
epoch: 105, train time every whole data:441.00s
epoch: 105, total time:31060.33s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2797s
epoch: 106, learning rate 0.000100
epoch: 106, train time every whole data:441.00s
epoch: 106, total time:31557.61s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1019s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_106.params
epoch: 107, learning rate 0.000100
epoch: 107, train time every whole data:441.05s
epoch: 107, total time:32054.92s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0666s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_107.params
epoch: 108, learning rate 0.000100
epoch: 108, train time every whole data:441.14s
epoch: 108, total time:32552.27s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2910s
epoch: 109, learning rate 0.000100
epoch: 109, train time every whole data:441.36s
epoch: 109, total time:33049.93s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3482s
epoch: 110, learning rate 0.000100
epoch: 110, train time every whole data:441.47s
epoch: 110, total time:33547.74s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1581s
epoch: 111, learning rate 0.000100
epoch: 111, train time every whole data:440.85s
epoch: 111, total time:34044.75s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2103s
epoch: 112, learning rate 0.000100
epoch: 112, train time every whole data:440.71s
epoch: 112, total time:34541.68s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0789s
epoch: 113, learning rate 0.000100
epoch: 113, train time every whole data:441.29s
epoch: 113, total time:35039.05s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3176s
epoch: 114, learning rate 0.000100
epoch: 114, train time every whole data:440.92s
epoch: 114, total time:35536.29s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3030s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_114.params
epoch: 115, learning rate 0.000100
epoch: 115, train time every whole data:441.18s
epoch: 115, total time:36033.89s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1327s
epoch: 116, learning rate 0.000100
epoch: 116, train time every whole data:441.18s
epoch: 116, total time:36531.21s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0577s
epoch: 117, learning rate 0.000100
epoch: 117, train time every whole data:440.39s
epoch: 117, total time:37027.66s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2834s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_117.params
epoch: 118, learning rate 0.000100
epoch: 118, train time every whole data:441.30s
epoch: 118, total time:37525.42s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1831s
epoch: 119, learning rate 0.000100
epoch: 119, train time every whole data:440.56s
epoch: 119, total time:38022.17s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0946s
epoch: 120, learning rate 0.000100
epoch: 120, train time every whole data:441.10s
epoch: 120, total time:38519.36s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 56.0944s
epoch: 121, learning rate 0.000100
epoch: 121, train time every whole data:441.14s
epoch: 121, total time:39016.60s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 56.2647s
epoch: 122, learning rate 0.000100
epoch: 122, train time every whole data:441.31s
epoch: 122, total time:39514.19s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2172s
epoch: 123, learning rate 0.000100
epoch: 123, train time every whole data:441.05s
epoch: 123, total time:40011.46s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1756s
epoch: 124, learning rate 0.000100
epoch: 124, train time every whole data:441.77s
epoch: 124, total time:40509.41s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2826s
epoch: 125, learning rate 0.000100
epoch: 125, train time every whole data:441.38s
epoch: 125, total time:41007.08s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.2983s
epoch: 126, learning rate 0.000100
epoch: 126, train time every whole data:441.56s
epoch: 126, total time:41504.94s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0999s
epoch: 127, learning rate 0.000100
epoch: 127, train time every whole data:440.77s
epoch: 127, total time:42001.81s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0282s
epoch: 128, learning rate 0.000100
epoch: 128, train time every whole data:439.82s
epoch: 128, total time:42497.66s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 56.1973s
epoch: 129, learning rate 0.000100
epoch: 129, train time every whole data:439.54s
epoch: 129, total time:42993.41s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 56.2461s
epoch: 130, learning rate 0.000100
epoch: 130, train time every whole data:443.03s
epoch: 130, total time:43492.69s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 56.3478s
epoch: 131, learning rate 0.000100
epoch: 131, train time every whole data:441.43s
epoch: 131, total time:43990.47s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 56.2010s
epoch: 132, learning rate 0.000100
epoch: 132, train time every whole data:441.41s
epoch: 132, total time:44488.08s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.0787s
epoch: 133, learning rate 0.000100
epoch: 133, train time every whole data:442.18s
epoch: 133, total time:44986.35s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 56.1680s
epoch: 134, learning rate 0.000100
epoch: 134, train time every whole data:440.83s
epoch: 134, total time:45483.36s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3795s
epoch: 135, learning rate 0.000100
epoch: 135, train time every whole data:441.73s
epoch: 135, total time:45981.47s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 56.1785s
epoch: 136, learning rate 0.000100
epoch: 136, train time every whole data:440.72s
epoch: 136, total time:46478.38s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3742s
epoch: 137, learning rate 0.000100
epoch: 137, train time every whole data:441.72s
epoch: 137, total time:46976.47s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1412s
epoch: 138, learning rate 0.000100
epoch: 138, train time every whole data:441.30s
epoch: 138, total time:47473.91s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 56.1896s
epoch: 139, learning rate 0.000100
epoch: 139, train time every whole data:441.33s
epoch: 139, total time:47971.44s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 56.4374s
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_139.params
epoch: 140, learning rate 0.000100
epoch: 140, train time every whole data:441.50s
epoch: 140, total time:48469.52s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3038s
epoch: 141, learning rate 0.000100
epoch: 141, train time every whole data:440.58s
epoch: 141, total time:48966.41s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1719s
epoch: 142, learning rate 0.000100
epoch: 142, train time every whole data:441.65s
epoch: 142, total time:49464.24s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 56.1744s
epoch: 143, learning rate 0.000100
epoch: 143, train time every whole data:441.10s
epoch: 143, total time:49961.52s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 56.1919s
epoch: 144, learning rate 0.000100
epoch: 144, train time every whole data:441.09s
epoch: 144, total time:50458.81s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.3398s
epoch: 145, learning rate 0.000100
epoch: 145, train time every whole data:441.05s
epoch: 145, total time:50956.21s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.5737s
epoch: 146, learning rate 0.000100
epoch: 146, train time every whole data:441.35s
epoch: 146, total time:51454.14s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 56.2196s
epoch: 147, learning rate 0.000100
epoch: 147, train time every whole data:441.71s
epoch: 147, total time:51952.07s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 56.1738s
epoch: 148, learning rate 0.000100
epoch: 148, train time every whole data:442.15s
epoch: 148, total time:52450.40s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.02
validation cost time: 56.1939s
epoch: 149, learning rate 0.000100
epoch: 149, train time every whole data:442.54s
epoch: 149, total time:52949.14s
validation batch 1 / 168, loss: 0.03
validation batch 101 / 168, loss: 0.01
validation cost time: 56.0789s
best epoch: 139
apply the best val model on the test data set ...
load weight from: experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross/epoch_139.params
predicting testing set batch 1 / 168, time: 0.33s
predicting testing set batch 101 / 168, time: 33.72s
test time on whole data:56.09s
input: (672, 250, 12, 2)
prediction: (672, 250, 12, 2)
data_target_tensor: (672, 250, 12, 2)
current epoch: 139, predict 0 points
MAE: 1.48
RMSE: 2.42
MAPE: 53.00
PCC: 0.86
current epoch: 139, predict 1 points
MAE: 1.55
RMSE: 2.55
MAPE: 54.44
PCC: 0.84
current epoch: 139, predict 2 points
MAE: 1.58
RMSE: 2.62
MAPE: 54.79
PCC: 0.83
current epoch: 139, predict 3 points
MAE: 1.60
RMSE: 2.67
MAPE: 55.29
PCC: 0.83
current epoch: 139, predict 4 points
MAE: 1.62
RMSE: 2.71
MAPE: 55.76
PCC: 0.82
current epoch: 139, predict 5 points
MAE: 1.65
RMSE: 2.77
MAPE: 56.31
PCC: 0.81
current epoch: 139, predict 6 points
MAE: 1.68
RMSE: 2.83
MAPE: 57.16
PCC: 0.81
current epoch: 139, predict 7 points
MAE: 1.70
RMSE: 2.86
MAPE: 58.10
PCC: 0.80
current epoch: 139, predict 8 points
MAE: 1.72
RMSE: 2.91
MAPE: 58.96
PCC: 0.79
current epoch: 139, predict 9 points
MAE: 1.73
RMSE: 2.94
MAPE: 59.82
PCC: 0.78
current epoch: 139, predict 10 points
MAE: 1.75
RMSE: 2.97
MAPE: 60.61
PCC: 0.78
current epoch: 139, predict 11 points
MAE: 1.77
RMSE: 3.02
MAPE: 61.26
PCC: 0.77
all MAE: 1.65
all RMSE: 2.78
all MAPE: 57.12
all PCC: 0.77
1.4804587376136333 2.4222391165350707 53.00090806609834 0.8563782789658944 
1.5492993230346945 2.550299284989403 54.43848514925567 0.8418656140979885 
1.5778628098333165 2.615719401602939 54.79282201103941 0.834168015095263 
1.6006432504288497 2.6654858314226404 55.290030539546194 0.8281160653951971 
1.6247043737864033 2.7103570224662277 55.75911832156909 0.8228462633705379 
1.6543939314699243 2.774080082375577 56.30764524248729 0.814406247362572 
1.6804524380884887 2.8308683341945455 57.16289225310036 0.8057214320135709 
1.6986274964191197 2.8645777352315855 58.09770365440612 0.7989410105918227 
1.7189365763532973 2.907228677638844 58.96066871863472 0.7904689309461197 
1.7344948134449798 2.9382761914286304 59.81678774943806 0.7836769209907563 
1.7509085552138055 2.968107031584439 60.60747583972172 0.7779867880562056 
1.7745419760106043 3.0191014011268105 61.25640953383803 0.7696344284374748 
1.6537770234747597 2.7777287827760566 57.124386365282746 0.7696344284374748 
