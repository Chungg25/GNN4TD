total training epoch, fine tune epoch: 30 , 40
batch_size: 4
folder_dir: MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_ed_gts_tcn
load file: data/bike.npz
ori length: 3001 , percent: 1.0 , scale: 3001
train: torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12])
val: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
test: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
TemporalPositionalEncoding max_len: 12
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder2): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (src_embed2): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed2): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
  (prediction_generator2): Linear(in_features=64, out_features=1, bias=True)
)
delete the old one and create params directory experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_ed_gts_tcn
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.0.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.0.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.1.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.1.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.2.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.2.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.3.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.3.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.3.sublayer2.norm2.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
encoder.norm2.weight 	 torch.Size([64])
encoder.norm2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.0.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.0.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.tcn2.filter_convs.bias 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.1.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.1.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.tcn2.filter_convs.bias 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.2.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.2.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.tcn2.filter_convs.bias 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.3.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.3.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.tcn2.filter_convs.bias 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.3.sublayer2.norm2.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
decoder.norm2.weight 	 torch.Size([64])
decoder.norm2.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.0.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.0.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.0.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.0.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.0.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.0.tcn2.filter_convs.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.1.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.1.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.1.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.1.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.1.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.1.tcn2.filter_convs.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.2.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.2.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.2.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.2.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.2.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.2.tcn2.filter_convs.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.3.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.3.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.3.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.3.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.3.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.3.tcn2.filter_convs.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.norm.weight 	 torch.Size([64])
decoder2.norm.bias 	 torch.Size([64])
decoder2.norm2.weight 	 torch.Size([64])
decoder2.norm2.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed.2.embedding.weight 	 torch.Size([250, 64])
src_embed2.0.weight 	 torch.Size([64, 1])
src_embed2.0.bias 	 torch.Size([64])
src_embed2.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed2.2.embedding.weight 	 torch.Size([250, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([250, 64])
trg_embed2.0.weight 	 torch.Size([64, 1])
trg_embed2.0.bias 	 torch.Size([64])
trg_embed2.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed2.2.embedding.weight 	 torch.Size([250, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
prediction_generator2.weight 	 torch.Size([1, 64])
prediction_generator2.bias 	 torch.Size([1])
Net's total params: 761986
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483]}]
predicting testing set batch 1 / 168, time: 0.70s
predicting testing set batch 101 / 168, time: 36.92s
test time on whole data:61.12s
38.42083641371069 40.46798404984502 1562.4807166230942 -0.004728021568559233 
29.663894758487654 35.84069480871488 1190.197841584357 -0.010335867176809468 
28.572511636781286 34.387754685766176 1144.9838939361564 -0.01193811311657562 
26.58224118498633 33.22611401156205 1051.6345149325277 -0.013004620530380704 
25.362373978298955 31.69835336844867 993.9120646452018 -0.0143615530587907 
24.405895252598803 30.18129032897095 950.6298029214541 -0.01518583875228217 
23.864877211273516 29.631677743249107 931.7547061491259 -0.013995760857140624 
24.34191523009308 30.25668767240047 958.5357868298805 -0.011618959064905524 
25.579949388246007 31.41087467297868 1016.4917845307883 -0.00950147288978918 
26.23880182412329 32.10097676384787 1045.19195799249 -0.008265468317499696 
25.134145616299694 30.732302989877144 999.7956699617093 -0.007488504048160625 
38.68319663426236 39.52062291409233 1571.8713146303267 -0.004282406150294996 
28.070886594096805 33.46811252090005 1118.1184692427798 -0.004282406150294996 
epoch: 0, train time every whole data:221.89s
epoch: 0, total time:297.02s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 37.57s
test time on whole data:62.32s
2.8556833699240927 4.787544095548363 62.564476914274024 0.3883985928843586 
2.850135120245761 4.748957796739361 62.28433734388322 0.3447787585802036 
2.912001112327423 4.829106395321909 63.30337417661862 0.2555976502129302 
2.9261635606829075 4.852573902901006 63.75239785488432 0.24172435528861463 
2.938284528312495 4.87449211520119 63.885876144654 0.23172502095546574 
2.9369217464203636 4.86726003078814 63.65533246813069 0.22764496351753774 
2.939441429464324 4.865520767467089 63.92482052440393 0.22274401330920598 
2.9463801370622322 4.878733232022758 64.36773590476291 0.22099794403526554 
2.952764421719437 4.895475171543201 64.63943887016194 0.22273551327679444 
2.9643837256608085 4.932755188205467 64.90321145139819 0.23200054379092688 
2.949715391492471 4.882109341752715 64.59771150548666 0.220468430183816 
2.959033460902582 4.928254105826109 63.41208861862269 0.21830096517925146 
2.927575667017908 4.862163232701693 63.77427567645637 0.21830096517925146 
epoch: 1, train time every whole data:223.79s
epoch: 1, total time:596.77s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.93s
test time on whole data:59.60s
2.644840566723829 4.075463443653414 71.07398675940892 0.4733034484075944 
2.725520123726023 4.121399582359203 76.89263960913381 0.4391250475122485 
2.816524925815385 4.1918880521206265 80.38213339561368 0.405433339418839 
2.883886787303413 4.2679074184480195 82.7533394093188 0.3641990144096926 
2.9404340661828896 4.335818415777529 84.17485041407156 0.32522092348958687 
2.9939283493506235 4.390250672734711 85.84320184647221 0.29251109535094105 
3.035583293029861 4.431300395110039 87.14495472443356 0.2676398599192316 
3.0659449256207973 4.462920506046723 87.79318448752935 0.24891289013984452 
3.080546573426486 4.483394178569351 87.6932672214719 0.235824395561767 
3.0659733574111017 4.49084176063461 85.61561232943536 0.22814830330792768 
3.0845678192333628 4.493226896113445 86.3358179662009 0.22304128272442686 
3.056213759345845 4.493823045038891 82.88364972101598 0.2111122337668364 
2.949497045597468 4.355666974285609 83.21580788330478 0.2111122337668364 
epoch: 2, train time every whole data:229.08s
epoch: 2, total time:899.96s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.60s
test time on whole data:59.15s
2.514495150402454 3.984338977510128 67.5115668722987 0.4960397258474737 
2.696509392983768 4.131460372552317 77.27056328423708 0.4446556816222843 
2.8923121104628913 4.315509714851784 86.65905982091631 0.3981406281977135 
3.1030052739869625 4.558455299393449 96.45385082815568 0.347408402669927 
3.3130111706644474 4.808855323947099 106.14304080291097 0.30173765348130305 
3.531805317860274 5.079785398451 116.5864589421616 0.26318503343439664 
3.761780192416782 5.372620735211389 127.59734941098093 0.2322367747825491 
3.978924092081065 5.652873287691493 137.3902621636437 0.20618647287630462 
4.138131622565466 5.860515625610173 144.33994618405902 0.18666969122363958 
4.201461799648812 5.9447094983879385 145.95537311906216 0.17268243502253455 
4.15512598574401 5.824502807735023 143.44081237000128 0.16421663888976987 
3.799970904438712 5.247084240931684 126.7206545652072 0.16291193021917194 
3.5072110844379707 5.109128597522822 114.6740176231584 0.16291193021917194 
epoch: 3, train time every whole data:222.90s
epoch: 3, total time:1195.09s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.95s
test time on whole data:59.42s
2.566495911257164 4.310431186770475 67.85557560721016 0.43754376383884414 
2.7879464000282543 4.503814829725744 79.77128516597651 0.3885589561072609 
3.0283984122353473 4.7208624075084735 91.25611932742748 0.3431038573555043 
3.3083167243515628 5.012403215533486 104.43312795770274 0.2964626052284176 
3.607690071645503 5.347665602499852 118.46531698776481 0.2543963477723961 
3.9050343270640644 5.702243616507325 132.4319771316661 0.21934113651206058 
4.191969486991387 6.059633026296273 145.82270233721522 0.1913022176422908 
4.454569020906375 6.390977371842872 157.4361887909432 0.16656826392217078 
4.655422294448795 6.639602130924295 166.0385065461232 0.14774359935403936 
4.740733494553361 6.732416422214867 169.13396619440846 0.1340427241081598 
4.601723702850175 6.5194356314342725 161.75900505170674 0.12450015138121111 
4.0538614644543935 5.783970187465247 135.42238609471684 0.11665763413873817 
3.8251801092321984 5.703207037579742 127.48754347177336 0.11665763413873817 
epoch: 4, train time every whole data:228.13s
epoch: 4, total time:1502.71s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.65s
test time on whole data:60.26s
2.5619622999560088 4.1623866530156395 67.32918294971924 0.4572775372018062 
2.9250078633618317 4.448087766072843 85.4990852793153 0.3949993801908792 
3.3289269668400463 4.8532077737016115 104.37152172446618 0.33918072918447095 
3.791879116156715 5.37296184245938 125.83348751071237 0.28601646785402784 
4.279461235598883 5.960080231303022 148.03675361780518 0.23941685785881295 
4.72844999545103 6.537690631393797 168.1003828415742 0.20332679289679323 
5.14151484879053 7.0888121820390095 186.01383953653817 0.17729217798832292 
5.52607841395977 7.5977621165984415 202.02391293323365 0.15583982473557081 
5.836117844871024 7.976824421711464 214.76547166394386 0.1388677238314732 
5.9709632924466085 8.114839815056099 219.71961097421442 0.12665124532581382 
5.771526427182679 7.796221630639821 210.99518277782408 0.12008971708972395 
4.903427882297763 6.563693034851701 174.71794967966028 0.12067464131372452 
4.563776348909408 6.513562681153631 158.9535896806908 0.12067464131372452 
epoch: 5, train time every whole data:223.05s
epoch: 5, total time:1800.15s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.94s
test time on whole data:59.45s
2.6249448834213296 4.052313169240597 74.98357524243562 0.4805743604344193 
2.9697142306827007 4.342310001557773 92.24350401465003 0.41477055615632746 
3.3619361556089884 4.741279003130738 110.6313875997162 0.355154016386356 
3.8117846090076046 5.2582905828993995 131.6312117803942 0.2988284835588277 
4.281840787397165 5.843345585977975 152.78823055604227 0.24820998108969286 
4.68970237641143 6.393121898558371 170.37714637962404 0.20855811469121038 
5.0305413269797965 6.869939768709643 184.42985711379492 0.18079336891246797 
5.28776578241968 7.224791073772305 194.35151601711397 0.1589507580803323 
5.422534686542072 7.386187192780689 199.35082479951333 0.14422172490959018 
5.406676515277386 7.332392512173974 197.99702943848325 0.1348270402388778 
5.109547109216629 6.902883262386526 184.86107550068323 0.13251040359938474 
4.285751178121106 5.805263530641832 148.26218362585416 0.14103782098560555 
4.356894970090491 6.1202344830967474 153.4947038770432 0.14103782098560555 
epoch: 6, train time every whole data:228.86s
epoch: 6, total time:2107.74s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.56s
test time on whole data:59.03s
2.592323164252919 4.166274180109981 88.10602594833234 0.5952042709819114 
2.7980843697104247 4.226908046698326 102.99993286528944 0.5702651006455499 
2.961832463329924 4.437075646595637 111.68059331234672 0.5265603711257485 
3.2594571577936766 4.8012883304725955 124.60234390836504 0.472616131226619 
3.516675559851474 5.154810973812316 135.21715561173312 0.41947430815367687 
3.8068787040368965 5.534447516022863 146.13641979641878 0.3690270744719484 
4.116303481676128 5.905560024151022 156.67819206234975 0.3242169434394461 
4.295588935881853 6.122743466105457 160.81114554214085 0.2854321445949549 
4.338209785972056 6.170518247026338 158.79493464303488 0.2554768451729106 
4.379616142122962 6.191397358065101 156.8407495909574 0.22507289319192905 
4.321445026776797 6.111115591324993 150.88164106265572 0.1929845839633119 
4.034759049872469 5.81108431451676 134.44908198425017 0.16165924879429364 
3.701764486773132 5.439466469718276 135.60119117297026 0.16165924879429364 
epoch: 7, train time every whole data:223.48s
epoch: 7, total time:2405.97s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 37.29s
test time on whole data:62.31s
2.1850897237187517 3.483253139346928 74.83826239948328 0.7438349967496982 
3.235881113014494 4.5134716608509935 108.51610039588388 0.5886755818522547 
3.8972696737673664 5.280226966648086 134.75185458109246 0.5074144771375488 
4.400615006865224 5.922680447637008 152.6513740105585 0.42145281412849134 
4.753355890320348 6.38608462413403 162.1871182469976 0.34243380348144964 
5.053494195587046 6.7802791669680715 171.3053591430104 0.2772017040224313 
5.321995579081898 7.130235993340217 180.57289953638687 0.22641312468445104 
5.450342761668687 7.30452120612395 183.0549313116125 0.18316977119941577 
5.493491204664022 7.375021510360732 182.18794634058608 0.1477279726898455 
5.553762980340759 7.471431465371643 183.35699973122698 0.11627607005673785 
5.551699092393741 7.4941413607978955 181.11738738756807 0.08584675010604241 
5.183348247042901 7.092213311058524 165.69671177840232 0.0673722157153581 
4.673362122372104 6.474879998601532 156.6882366660643 0.0673722157153581 
epoch: 8, train time every whole data:229.72s
epoch: 8, total time:2710.10s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.57s
test time on whole data:59.22s
1.627880422083395 2.9503770005528978 56.50186891628488 0.7887712182806165 
2.4236483224477796 3.754596810836404 82.9786762806189 0.6422462758681389 
2.7858547910702014 4.201339577777138 99.81872864733265 0.5817010170673586 
3.16795524068567 4.701031489100038 116.22837466050278 0.5134887870262016 
3.5260857607753326 5.1451356953348135 129.52946072210005 0.44463538711030687 
3.8733357804973743 5.5812690382855275 141.9999336914354 0.3793847593821371 
4.1826666160828125 5.970029387068281 152.84886722367386 0.3228706704084271 
4.346121888249225 6.166337781781911 155.88473178504856 0.2738410628222114 
4.405014613717972 6.238798921376002 153.96806011091212 0.2320214867132583 
4.4386816243712035 6.307803885776964 151.76114832502978 0.19329669273739034 
4.37492261780009 6.24417522836285 144.94407593968148 0.1561222339738095 
4.036033585592838 5.891396128368888 127.27574702449968 0.128371339921234 
3.5990167719478245 5.371606722355903 126.14669702597767 0.128371339921234 
epoch: 9, train time every whole data:222.62s
epoch: 9, total time:3007.70s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 37.13s
test time on whole data:61.23s
1.8904908844189985 2.9438212372406825 66.46385716365357 0.7837555496814111 
3.648644564423089 4.734830857547524 144.67152506643296 0.5913498618466394 
4.8647889874478185 6.075708591063991 202.3724070152047 0.49083079252669193 
5.97530832079186 7.347989642416412 250.43559515599188 0.3927161068639031 
6.823849177215071 8.23528544337581 284.87767333316077 0.3231885272820542 
7.658435667114776 9.056798621092192 319.52852789086 0.27439132656877274 
8.461897128570293 9.846570491804993 353.1704092249956 0.24069906953670725 
8.941969690142377 10.300665478981484 372.5361934608545 0.21863912438259775 
9.125290117059347 10.442019144801513 379.9426271914901 0.204483601026082 
9.292425002734133 10.517781353937984 386.65935759488843 0.19904033732336715 
8.767935693241123 9.902057969643552 364.74821709863187 0.1927575757060133 
7.143322394270538 8.229595385169224 294.57022616342783 0.16485343633322866 
6.882863135619119 8.46725339872819 285.0037110383059 0.16485343633322866 
epoch: 10, train time every whole data:228.72s
epoch: 10, total time:3313.11s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.90s
test time on whole data:59.34s
1.3578969513530887 2.598435087814452 45.65480918448047 0.8236158034626132 
2.374781958430828 3.657534831183262 89.57018345995772 0.6669177235482144 
2.9937371773955723 4.5274210635743195 122.21397747920906 0.5863836704469352 
3.7206588276925365 5.508921353863056 157.9120794794855 0.5165614436131392 
4.2489646615960766 6.15454518798263 182.33884530903845 0.4529933088927656 
4.756455222946104 6.769362983505716 204.32957472325262 0.3915680178189771 
5.239414964722647 7.378689826365605 223.1160561937785 0.334541715155621 
5.514893417728089 7.701985642617701 231.04860749125544 0.2842462295290745 
5.708703169738164 7.852943484021627 235.0025535952441 0.24940175839183865 
5.931137738061448 7.918302163726166 239.82739901671283 0.22762858764256533 
5.653490628181469 7.3173824622486014 222.1396596369347 0.20976224233550061 
4.979170304797235 6.352302626880663 186.2412958825401 0.16886765041775087 
4.373275418553605 6.368592173475452 178.28640702488707 0.16886765041775087 
epoch: 11, train time every whole data:223.20s
epoch: 11, total time:3611.08s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.25s
test time on whole data:60.38s
1.5136777350394321 2.6027857251039674 51.978743444444206 0.8278722366004148 
2.987228607682245 4.150949886612291 116.92202191458918 0.6359744567654437 
4.140668006524 5.589228510838638 170.3287363801273 0.5236488597382526 
5.336288659278836 7.07356581146073 222.59642850707598 0.4426220491737421 
6.184189239955197 8.05545453580821 256.6680340524456 0.3738051022901536 
7.10151886965636 9.066081252957327 292.66062755330637 0.3157752534362205 
8.029221425839921 10.083927015905664 329.34402267695776 0.2678804383962148 
8.681824138427242 10.740777587283295 354.9193304778588 0.2271229259527841 
9.258716448682227 11.227512786734172 378.9966567393839 0.20138356326483514 
9.79735282489338 11.585856188382381 401.85719241006905 0.19161208454733442 
9.297712811410605 10.84272913416239 381.73770100096084 0.19568090302177263 
7.803806015052345 9.003431500617031 320.3858504684235 0.18507184324245746 
6.677683731870149 8.795497959609696 273.2060415664766 0.18507184324245746 
epoch: 12, train time every whole data:228.81s
epoch: 12, total time:3914.68s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.40s
test time on whole data:58.89s
1.3773517435877807 2.5324549137940444 52.79584587197088 0.8436864509394499 
2.246815017311346 3.648700863088945 86.00440068053025 0.6785413045122838 
2.710410824640521 4.454748122043981 108.06008347591512 0.5957680796401638 
3.2211332084539213 5.288749847251838 130.81709674816858 0.5208750655346842 
3.555769257366125 5.838971098072001 143.86163274501502 0.44819644969325917 
3.8591190495388137 6.316172018970387 155.07444235422255 0.38096342900229163 
4.101867737187605 6.70787364872054 162.65771598523614 0.3215314962654837 
4.1487998607320975 6.818953913084432 160.44290669737447 0.2703955987151314 
4.071750915930828 6.688563055666439 151.63938243581703 0.2250780517889751 
3.9790552854619565 6.431708980503979 141.8644156481689 0.18383654083105772 
3.8076869565715037 5.97119877637109 128.79723561165108 0.14657274958476338 
3.5941597960728795 5.564973726507414 113.88052585061712 0.11565977246021945 
3.3894933044046147 5.669738523018902 127.99277229602792 0.11565977246021945 
epoch: 13, train time every whole data:223.05s
epoch: 13, total time:4210.33s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.02s
test time on whole data:59.53s
1.4149467617899534 2.515798197037749 50.44356258093055 0.8443419534302368 
2.9368986535920274 4.493783764222841 119.75917250190228 0.6554476996797125 
4.191873876182451 6.37092449559571 178.3124982821616 0.5582675584130927 
5.455144015755327 8.112503621605208 236.07384071777807 0.49295478634772955 
6.4570862728371505 9.31550541292881 279.9322255852153 0.43709897454695246 
7.445470854472369 10.443504423364487 320.0409105600417 0.3772242347701567 
8.35041099405803 11.491301710834836 354.05109895414034 0.3171018967195542 
8.931573649378494 12.09729185210461 372.7421930840822 0.267247740642085 
9.166865044897333 12.223708502872244 377.42078144799336 0.23074327563475064 
9.203423783289711 12.020964112489755 374.4231115026902 0.20803785509481454 
8.465613382932508 10.850159201427559 339.72235853351333 0.1895333807833251 
7.237322169797495 9.084949050285815 284.5883029964685 0.15953474907713597 
6.604719121581905 9.580142307703001 273.9651624498168 0.15953474907713597 
epoch: 14, train time every whole data:229.41s
epoch: 14, total time:4512.79s
predicting testing set batch 1 / 168, time: 0.38s
predicting testing set batch 101 / 168, time: 37.42s
test time on whole data:62.29s
1.3901122548552673 2.4196329081704433 54.55825951530354 0.8528908278218177 
2.4991532435313397 3.9598656901779625 103.43019455860174 0.6799770537544503 
3.4314621213838636 5.468116725954725 147.99633320865973 0.590556473078175 
4.389900540851145 6.972194751314674 191.47099868635067 0.5218714286117312 
5.139063163478105 8.040805685096363 226.05113568419378 0.4591851378993612 
5.918352340388954 9.037021502866446 261.77355093439616 0.3977206613348336 
6.619814555396636 9.915320044268865 292.0708108957706 0.342173909423652 
6.997362337695967 10.304679712250557 306.325726500813 0.29861777319184646 
7.017521955514327 10.18694725630194 303.7648021356959 0.26185823632060384 
6.827208544739212 9.748874289138286 290.6615887586867 0.22097764497468694 
6.134954278096379 8.630309856035936 253.35545396397526 0.1618154138981481 
5.489281445384469 7.577707856198072 215.69280996863176 0.10283332551108915 
5.154515565109639 8.070458568176015 220.6004513672248 0.10283332551108915 
epoch: 15, train time every whole data:223.46s
epoch: 15, total time:4811.94s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.07s
test time on whole data:59.59s
1.6065198586609746 2.6652830058444605 61.12302802339119 0.8395332291094947 
3.331553904927735 4.920574169877888 139.9029321214072 0.6464164164042457 
4.68513112173602 6.969049927049213 200.98915305694098 0.5520417324995585 
5.8788647752099985 8.745011031782065 254.18830885241044 0.48325715406139397 
6.747164538898106 9.966748227981462 292.2790631500432 0.4194584380632016 
7.687497879967537 11.182070991365599 330.8190121722686 0.35732725426365847 
8.660148587190857 12.393229223777034 367.581118415755 0.29385808732253005 
9.265030655714018 13.078129365974576 385.8097384647869 0.23626227334492655 
9.496373754958222 13.159604721857091 387.7326595082738 0.19420811408664404 
9.489775283029925 12.748160477692531 380.33678775857766 0.1649608014007806 
8.678361939433076 11.163645283475542 341.1209531609734 0.14045684121282287 
7.866249474107066 9.889718222984904 303.1707304058451 0.11009016181135076 
6.949389314486128 10.261121337530145 287.09357916834375 0.11009016181135076 
epoch: 16, train time every whole data:229.18s
epoch: 16, total time:5114.07s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.21s
test time on whole data:60.34s
1.2048126999987734 2.311990801192133 46.1510708626471 0.8680153007438224 
2.640624352879379 4.231336734557859 111.19135292536883 0.6715253807583872 
3.712425267842997 5.982886443547455 161.10743612662844 0.5803832544623027 
4.854149240416608 7.711130315448732 214.03569349590796 0.5111267566620986 
5.800074916767489 8.96174228870444 259.5398880413942 0.4470651975745457 
6.72330247976152 10.097978678617586 301.74642971586434 0.3824497155583312 
7.527394680691528 11.091493041925164 335.9212252319451 0.32633043330528705 
7.939929914406368 11.565162227831276 350.44186199345984 0.2811909488592074 
8.013124267834282 11.538270394862169 349.7586909482679 0.24577823204825314 
7.8608137995772775 11.114549339999199 337.68129619837043 0.21152200968667798 
7.177414248351451 9.93056401681459 300.7381598338798 0.16847287790171636 
6.598898143737799 9.022076281341512 267.7069029132085 0.12718628603573906 
5.837747001022122 9.102385306962844 253.0072315717913 0.12718628603573906 
epoch: 17, train time every whole data:223.22s
epoch: 17, total time:5409.40s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.85s
test time on whole data:59.30s
1.192486800184562 2.3444772817903723 46.52327303985705 0.8590387296448948 
2.4442553747758446 3.873054065197271 102.27177074935791 0.6755455632702247 
3.199746693630215 5.061118399622148 138.8518174515388 0.595093615363316 
4.008784663126316 6.276330879003971 176.9614488166093 0.5304936752021796 
4.666027143487973 7.138619585988343 208.33982359915458 0.4697060918378305 
5.33405884844659 7.967234127928286 239.21885132061539 0.40849140611753665 
5.982424006903544 8.75679738157718 267.13355428530224 0.3524463736895862 
6.317463616515643 9.106471542883476 279.77107988664017 0.307703502012155 
6.372365395304702 9.086445158090374 278.5765080057281 0.26926564900337097 
6.303001391767302 8.86619597069576 270.3174632321939 0.2281787694442278 
5.934831189925117 8.228703878117893 246.8070328347055 0.17442038731815315 
5.667746559390088 7.79335235991203 226.54190692141304 0.12288583157388386 
4.785265973621492 7.355107233422874 206.78044782141515 0.12288583157388386 
epoch: 18, train time every whole data:229.10s
epoch: 18, total time:5710.68s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.46s
test time on whole data:58.93s
1.163079259425047 2.290187364626147 45.189258639783134 0.869566417493523 
2.46419978740492 4.087064118612012 100.60196417420248 0.6773646551946076 
3.2944775043247003 5.572261653227877 138.5641933367913 0.5954671655530678 
4.215742517625558 7.100049257276254 180.26676239412726 0.5245067040758651 
4.9320978901078485 8.193003664846328 213.5558346106497 0.45529966736903077 
5.515635529680887 9.067237122839494 240.04045426015205 0.3906829830927801 
5.9669423475009165 9.753923019908818 259.681002005915 0.33734808074859446 
6.096349371201492 9.927623542213896 264.47525326907714 0.29276230771779205 
5.952729743639007 9.637115621668123 255.7152324018463 0.24561414503997192 
5.706266490428221 9.052543626615776 239.77449635987927 0.18976401602680176 
5.367280278831011 8.209466753507265 217.541431894201 0.12425215160576875 
5.244632351679727 7.959517767323479 203.38604289445072 0.06638411542384881 
4.659952755987445 7.9163253434132 196.5696287149189 0.06638411542384881 
epoch: 19, train time every whole data:224.17s
epoch: 19, total time:6010.55s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.95s
test time on whole data:59.47s
1.3306729951586929 2.3664835213725413 48.43330096886209 0.8642124953706699 
2.9497857648435803 4.228176339157165 116.26337176940457 0.6544276389666661 
3.9996601524954394 5.653483152836224 163.43875685581884 0.5580826899015192 
5.251401347544488 7.19681979313192 218.73009655844174 0.4768210143241475 
6.459649600327991 8.488501121082054 270.67248888218677 0.40180270253507605 
7.625979860796726 9.708644973332175 318.9827036639115 0.3383888401937378 
8.711262507987254 10.853920646090288 362.73233694006717 0.28634337837300455 
9.438223581561996 11.550644717093478 391.95527209958453 0.24481315149352875 
9.896581917812162 11.90400436297298 410.3722645627673 0.21995564340779092 
10.232285536022946 12.05921689318556 423.7263576552329 0.20452718776132126 
10.128343635929483 11.708880848627611 420.1891089705078 0.18718459540670385 
9.977811992923241 11.430678027115558 413.99942696257517 0.18270752474674773 
7.166804907783666 9.485337607840814 296.6322766666276 0.18270752474674773 
epoch: 20, train time every whole data:229.39s
epoch: 20, total time:6312.83s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.56s
test time on whole data:59.08s
1.1383254251928911 2.2605210522787154 43.91205231412538 0.8741360269272016 
2.5952598642062927 4.260198942225425 107.61615073541788 0.6722010932704654 
3.5540932277429493 5.947057356235901 151.53709948150478 0.5886607503983269 
4.597517762992354 7.589740606457362 200.47468968140336 0.5211820741307646 
5.489856296428612 8.809188762891536 243.45470592610704 0.45770859331742164 
6.361717859065692 9.93656577811633 284.0533337491938 0.39212785579726683 
7.18704502501454 10.990924759229644 319.5389399787635 0.3350146905542304 
7.6765814450145475 11.544692142469888 339.18508177078786 0.29255272236116064 
7.885808937632967 11.649640720968732 344.82236873492195 0.26002878590383993 
7.900930326875388 11.387624106905111 339.81645522087746 0.22936845831914932 
7.633475584171003 10.709486275871887 322.0097517336428 0.19388627024313512 
7.56811735553871 10.560801392977618 310.79030470291775 0.1472591204479444 
5.799060759156329 9.30172084464378 250.60679080316865 0.1472591204479444 
epoch: 21, train time every whole data:223.88s
epoch: 21, total time:6609.37s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.06s
test time on whole data:59.56s
1.1985832725091883 2.2884033854965624 44.015116176269956 0.88040023816345 
2.2848009349591143 3.6943090027541503 85.25794231754679 0.6890688996136833 
2.8142903165790654 4.648574791261357 106.98944074046615 0.5896890112553299 
3.4173417950335536 5.677655485237093 129.60726965094017 0.4773591597517524 
3.9206547403193657 6.489290641795451 147.60376951308214 0.3592355794948072 
4.378354842186595 7.155075655583764 163.96924798804986 0.25024303172588597 
4.741524852294297 7.666432830768322 176.76535450435975 0.1556832816188159 
4.846314275477259 7.8046929186943235 178.4215309221422 0.08235592364271666 
4.7854427303082 7.679897726737238 171.94060297108655 0.03033630890091908 
4.69173611223467 7.434375771971702 164.09046177017922 -0.0052008682915400265 
4.559748676697679 7.143889363818941 155.85326215444445 -0.027537100015291662 
4.555525205182976 7.158574597460361 153.869484534666 -0.043513288161718386 
3.8495264794818302 6.469507134486818 139.86748928875681 -0.043513288161718386 
epoch: 22, train time every whole data:229.53s
epoch: 22, total time:6910.61s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.44s
test time on whole data:59.01s
1.1994221801863363 2.251845954136432 45.53918204344493 0.8769941230954358 
2.7975667464860847 4.290595916649107 115.36031403304816 0.6770192265463182 
4.0153068325639305 6.1506045126796876 172.02036909005102 0.5921126990492851 
5.463626413132285 8.165798666175903 239.93777571996122 0.5182120851670331 
6.736998082220731 9.749998417777016 299.1816211550022 0.4522191130240206 
7.930570385052158 11.180741424046696 350.5547516599791 0.3850538122596219 
9.090622042006503 12.559182362360682 395.49593454936826 0.3169620460486722 
9.897522712841452 13.427883441911879 422.3455216596529 0.25689884910964844 
10.361375886797195 13.762063921413711 433.8011209745703 0.20975904646182464 
10.502530476978848 13.574773682548075 430.167966319817 0.17193174128731994 
10.191829614578259 12.862927908679936 408.64826125771066 0.1393485445545106 
10.22175422744036 12.801755972593964 401.1486531882975 0.10385533950719195 
7.367427133357012 10.759149380887074 309.52462951677677 0.10385533950719195 
epoch: 23, train time every whole data:223.51s
epoch: 23, total time:7207.29s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.95s
test time on whole data:59.47s
1.1656937671603547 2.2271875413703506 45.15614798502792 0.8797099377711107 
2.694459094762891 4.274954549527591 113.6223060657245 0.6792058998859704 
4.037772757475043 6.285361226190108 176.69540679816086 0.5981585894753988 
5.70067755183134 8.547590509783006 254.68654607069817 0.5252755319279643 
7.193992428087408 10.361789433996112 324.68980949323264 0.4574227106925862 
8.55978044912707 11.976081921105909 384.0288135537458 0.3861576086070711 
9.795794620475244 13.463061108914573 433.7623015260598 0.31479060493828837 
10.592211126421002 14.3637868738097 460.92509372562915 0.2570961195037068 
10.999490367826075 14.686040075607828 469.777753571527 0.2121031315307927 
11.046514470628596 14.40362345714284 461.37700956782936 0.17554988372164668 
10.638866119598616 13.527408623668446 434.34429466360206 0.14510957516549192 
10.41098379348999 13.21328518392317 414.7570168536068 0.10526107552104116 
7.736353045573636 11.383079941542606 331.1604710752096 0.10526107552104116 
epoch: 24, train time every whole data:229.48s
epoch: 24, total time:7509.22s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.54s
test time on whole data:59.22s
1.0831634247828097 2.086377770318683 41.103452749959416 0.8933847646303754 
2.543094848817392 4.073360083824788 103.01521010038384 0.6948633430887817 
3.6402340617383873 5.842854548788433 153.10785468085078 0.6039936904021255 
5.082264806055863 7.880852283072504 220.61749791219123 0.5235565756829446 
6.520426123630877 9.637929169552578 287.9908617151286 0.45399409722936174 
7.904607526852173 11.249318516024353 348.9391686020051 0.3881718727253584 
9.191401373420709 12.739746954981435 401.25161586164575 0.3218919895406764 
10.078420461741409 13.666862836871099 432.8969969380634 0.2641747167071441 
10.626694171090922 14.082315958890506 446.6184538285418 0.21648641384390985 
10.835680847074038 13.978268279131886 444.6011015148512 0.17629112924934412 
10.638474866047767 13.390346036832689 425.4553533200474 0.14087370157825538 
10.67334511640934 13.262282787921098 416.6990669132442 0.09881808028389584 
7.4014839689718075 10.93391616779447 310.1999343514577 0.09881808028389584 
epoch: 25, train time every whole data:223.11s
epoch: 25, total time:7805.74s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.97s
test time on whole data:59.51s
1.1708348491502305 2.2253603949505094 47.719977730878036 0.8845147001553029 
2.705162942218461 4.522227657217356 110.7538255226643 0.6830150224545912 
3.8104639446338906 6.515604741093884 158.86603571934904 0.5942833893317849 
5.053289094349281 8.62730932300268 214.00876605500915 0.5133914812112663 
6.112690826907133 10.411752232023224 262.0608223186366 0.4361813474110928 
7.10188645302185 12.061676778160747 307.1237247345223 0.3680274381341247 
8.002230168259038 13.546320105982163 347.30862514380306 0.31576284222216544 
8.516961466385052 14.39946104451514 370.3472258939303 0.27715704774905026 
8.666606659757772 14.52919279703017 375.08334745674443 0.2445994409141369 
8.466588234453772 13.816349064943168 362.89571797200324 0.21388286553297203 
8.001727292332 12.524230484451131 337.7645709895456 0.17555825340890643 
7.697161791351757 11.822990328333265 316.89674636621544 0.120654711909639 
6.275466976901686 11.126734454851317 267.5754525675425 0.120654711909639 
epoch: 26, train time every whole data:229.64s
epoch: 26, total time:8108.23s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.67s
test time on whole data:61.57s
1.0499577948549497 2.108237873380791 39.141327816909524 0.892276725045156 
2.3351104514723025 3.8337345984904614 92.52223856940132 0.6942324685073469 
3.0580154125148518 5.164923337776659 125.32278883877417 0.6064929503022184 
3.9409812591647997 6.658754331559741 164.7673265853993 0.5216288226895113 
4.73326188992842 7.918115432320906 199.91861488420201 0.4404439922626075 
5.445071126245938 9.045728407182999 231.97055340545103 0.37167709453223086 
6.080227799032682 10.060361320276312 261.4499301308344 0.3199260092866062 
6.483670625870427 10.668946953645015 279.50546644664985 0.27743526885680486 
6.691226654579153 10.883929050060576 286.46639074538064 0.23697564665004392 
6.703803521063622 10.654995778532045 283.96404637029707 0.19554481782254232 
6.6331093681230255 10.23575708849849 275.398332596703 0.1464197921668542 
6.796986320816424 10.361577657530336 274.82220106240584 0.08800177767593458 
4.995951851972216 8.630558812445374 209.609037712665 0.08800177767593458 
epoch: 27, train time every whole data:222.98s
epoch: 27, total time:8405.96s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.73s
test time on whole data:60.35s
1.1946841456688764 2.3238332279623575 47.086373832046185 0.8854963908370076 
3.2814373478001606 5.416122107343304 135.86685911800512 0.6740116544051408 
5.107296174812796 8.331202111405446 216.02018720396273 0.5880991916429829 
7.271150027953709 11.45785932684045 314.26286961802253 0.5129279120162823 
9.224571398410237 14.15215656347988 404.51619097778445 0.43990437476549776 
10.975163004469747 16.613682004213103 483.0290066818188 0.36570298267646695 
12.53918700410337 18.894680541782936 548.9393667562058 0.29962780227172187 
13.542081720510675 20.36788393660789 586.011554310309 0.24806958984991964 
14.001270608321896 20.82079151445396 595.8497417271195 0.21151647635185322 
13.828902431021932 19.980541052574868 576.533380864615 0.18321372835868516 
12.956223243149175 17.95628174558227 528.7694411670302 0.1578669312054535 
12.265949731662248 16.570334094514678 490.1802058704244 0.11936632949568389 
9.682326403157068 15.58127901432212 410.5996343664759 0.11936632949568389 
epoch: 28, train time every whole data:228.75s
epoch: 28, total time:8709.53s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 35.88s
test time on whole data:59.61s
1.2549966947477134 2.1149476964479073 48.77176268427766 0.8989772489085466 
2.3575854428708554 3.96052015889887 93.18222475487202 0.6931992054934337 
3.1634670550639608 5.502641154849655 128.087547740034 0.6029508309294901 
4.197469231360725 7.4246611675014 173.66099658645396 0.5117367010349263 
5.224361391898511 9.298158898946895 220.32676735296906 0.4246922384596072 
6.233991689334313 11.116988755255774 266.2598196390903 0.351774059021183 
7.2016989149418436 12.839296922723262 309.81853118150696 0.2995106582267553 
7.926753032541346 14.092788528833687 342.2928559769006 0.2615010075064515 
8.419429655656218 14.755367093202455 362.2861879414506 0.2320978016305843 
8.626159784171199 14.612899511937998 366.72718341795303 0.20779671412641693 
8.594103809493904 13.878645123048688 360.09824064888284 0.1800173160264029 
8.699429922828184 13.57264495336933 357.4464977946668 0.13862912458243204 
5.991620552075731 11.132355173040779 252.42004005991916 0.13862912458243204 
epoch: 29, train time every whole data:222.88s
epoch: 29, total time:9004.66s
fine tune the model ... 
epoch: 30, train time every whole data:457.00s
epoch: 30, total time:9461.67s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 37.06s
test time on whole data:61.44s
0.9546337706489222 1.9308636082576722 33.85230449869395 0.907640615883789 
1.8646014863251752 3.213295879325083 63.82970895197818 0.7184398866417044 
2.0003635793129604 3.4888840045988174 67.00338987570906 0.6558799099893516 
2.127575927246894 3.729233769766627 69.24783689797565 0.5943178071671424 
2.2371886922995605 3.926925505056316 70.17302148938838 0.5410559912048899 
2.3331824726951975 4.0867514873458495 70.22178251230406 0.4950476251232098 
2.4139619514435706 4.216629692652559 69.32292811445348 0.4539460818826417 
2.471241844121899 4.3153379021618905 68.16032762122668 0.42542615230544034 
2.5190746068043546 4.403714171672063 66.84794789627402 0.4044526318650893 
2.566282040470856 4.487194946270553 65.24140190689658 0.38631012165183287 
2.6148709520317968 4.575392291804121 64.45206803153323 0.3635386936465361 
2.6787559288599128 4.703862303000828 64.9050402522123 0.3225352975982751 
2.2318111043550917 3.9921333247023516 64.43841094123705 0.3225352975982751 
epoch: 31, train time every whole data:455.09s
epoch: 31, total time:9991.17s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.45s
test time on whole data:59.02s
0.951379749126555 1.8801302764696493 33.39518753467613 0.9123889919986748 
1.8117826875724963 3.099074693677579 62.242961865231074 0.7441943043302969 
1.9575868522384692 3.441757995695493 64.5812622142496 0.6747990693528192 
2.104324903801172 3.718990001176726 66.98386794585896 0.6035265977090825 
2.2283538575834996 3.9329850797515307 68.29260625166229 0.54477070577701 
2.336272703638389 4.080479808196026 68.56491228860108 0.5020585058539888 
2.423867988525668 4.188642892761415 67.68188285715475 0.4679759626071493 
2.493818385148155 4.281312991172676 66.96196212856906 0.4360855815712498 
2.5513079892671002 4.359377686350003 66.1618797703429 0.4106056383079185 
2.5967017927182217 4.416067418141321 64.62490985337345 0.39500346931728225 
2.6253952320922345 4.460826434347387 63.015161136511736 0.38974913613416545 
2.6464558251557784 4.518101778270079 62.08030394152835 0.3823810790403454 
2.227270663905645 3.9330526446533915 62.88251369192839 0.3823810790403454 
epoch: 32, train time every whole data:454.33s
epoch: 32, total time:10517.21s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.73s
test time on whole data:59.39s
0.9353558121783038 1.8706750857246082 33.88033846685881 0.9127157349331143 
1.794302906620538 3.012441484154238 64.10225263111383 0.754788762769414 
1.905916867644127 3.307482529854287 65.67107443816653 0.69828346046554 
2.0232880507279187 3.514690039223896 67.45759316142346 0.651187939255259 
2.1159483983838663 3.6936535646418474 67.79373871726418 0.6095206543929201 
2.1976905422333983 3.828993360997263 67.36297981671188 0.5763587349316973 
2.262207184409279 3.915402334582713 66.17074424646057 0.5553874884825353 
2.3072586648451785 3.9767623912668157 65.01580701708552 0.5427644612751034 
2.340090464639699 4.036258875868284 63.68880581393098 0.5323260379735382 
2.374957313297937 4.098034918075912 62.04857575930276 0.5211871813291571 
2.4017309597159424 4.151143865851731 60.530330527457785 0.5125371374948572 
2.4319800386867887 4.226807045686096 59.8444316626776 0.49492517059577396 
2.090893933615248 3.6911963530425185 61.96407151132334 0.49492517059577396 
epoch: 33, train time every whole data:456.13s
epoch: 33, total time:11046.07s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.54s
test time on whole data:59.15s
0.9068878798566404 1.8598700293620518 32.16614352647233 0.9143987176949722 
1.7800525310349073 2.978482714226876 63.48799485291766 0.7628360789826998 
1.8971535454895347 3.2890920650883886 65.55213141613801 0.7052756266629467 
2.0131508113907737 3.512568595216255 67.4326298297033 0.6565880713701852 
2.1064268511727096 3.714918811012124 67.4232941933762 0.6140290192502764 
2.192382594125523 3.880900053293073 66.47113816905541 0.5793148294897795 
2.2637961130086333 4.009945456378198 64.92766010754782 0.5526950715194119 
2.3215846128513418 4.122854710808074 63.61415690921935 0.5298215291488239 
2.369456708942584 4.231202864452933 62.57930358667202 0.5133097957732943 
2.41188124480063 4.31958747495023 61.358251803561714 0.5047771716108111 
2.4491451151600727 4.387729931376269 60.356787449721374 0.507066113761192 
2.4942420726553314 4.456943504841878 60.41208324258864 0.49585085996965905 
2.1005133400407234 3.798078177067162 61.315313422571805 0.49585085996965905 
epoch: 34, train time every whole data:455.87s
epoch: 34, total time:11573.76s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.75s
test time on whole data:59.48s
0.9057195992906178 1.8506909686903636 32.909121564210686 0.9169559963657278 
1.7565716517503773 2.9910771294315897 62.30308533887053 0.771091552459127 
1.8711011119842351 3.322364291541945 63.39431195364874 0.7209093515825074 
1.9744146493673325 3.5706216257151953 63.94633666621792 0.6819860867242237 
2.09909790427832 3.8323720762610143 63.966538203314904 0.6466421021001599 
2.216208051835231 4.068407623948572 64.26670560191315 0.6112588527301832 
2.305958422792366 4.240896134692229 64.744302794495 0.5818692305691888 
2.3706220796087845 4.349830516921614 65.63612857126738 0.5653930547706371 
2.4337195418999484 4.439101019177893 67.15161537483584 0.5543435841711569 
2.4684186745130767 4.49152647411552 67.44313473138409 0.5453966266770455 
2.4975271662794762 4.524368564224365 67.63860226547588 0.5385892441239944 
2.5363406493854486 4.5680493462216125 68.02218018711869 0.5191864599946503 
2.119641625248768 3.9319280498762206 62.618852897999 0.5191864599946503 
epoch: 35, train time every whole data:456.46s
epoch: 35, total time:12103.95s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 35.56s
test time on whole data:59.12s
0.9066084422156038 1.833261256773164 32.343584577118385 0.9165833284254044 
1.7431929177379324 2.91754982747191 62.56105280493443 0.7748232500475186 
1.8420113679223826 3.2174149486758403 63.71494848504767 0.723651941752237 
1.9311580775289663 3.4238032492500614 64.56227213688507 0.6830697816462218 
2.0063881222365336 3.6059533782814377 63.96516474280348 0.65099247308065 
2.069881349126941 3.741714907056007 63.18968368403488 0.6269853432706807 
2.108022035823603 3.8175955156632515 62.375477671166266 0.6128125929822893 
2.1316441994648248 3.8628977543665117 61.7409353375579 0.6056583427385581 
2.1542783619559236 3.913763026905515 61.543176703877975 0.5998459785354405 
2.178949779958065 3.9680781919461134 61.20214754120193 0.5918116159260798 
2.2089618641692437 4.0146784745516015 61.23957414963209 0.5815403303811626 
2.260476654907866 4.094801124866165 62.16108044581924 0.559400991215511 
1.9617977644206572 3.586952064325592 60.050126147929625 0.559400991215511 
epoch: 36, train time every whole data:455.83s
epoch: 36, total time:12631.61s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.98s
test time on whole data:61.66s
0.9360447302771111 1.849464617833582 32.22939884111788 0.9161261778557236 
1.7578438324842247 2.9922925311861888 58.25864285272547 0.7766131856690606 
1.8972133277537566 3.414059757337794 58.194528236604626 0.7177241804966107 
2.029255840373802 3.7187863635804783 58.56066793603406 0.6648563636463273 
2.1469014417118437 3.9580606240714618 58.932511773883256 0.6290929567859981 
2.2401592225886526 4.119498152801619 59.354676229171176 0.6078378426825586 
2.2963597447660176 4.217026045594233 59.55967306098707 0.5944188950678561 
2.3375647733454548 4.285624549807568 60.13797539903215 0.5849793226415717 
2.3856088286879515 4.367066065834597 61.58358388301156 0.5751963663665678 
2.4293952076761496 4.430961010008189 62.54735950729399 0.5639067333130584 
2.4696743961758023 4.4806040776114875 63.23251746424159 0.5574799064623738 
2.5267828995528676 4.549534026362262 64.7350418187085 0.5442755480376056 
2.1210670204494697 3.9383230319541287 58.11086020484485 0.5442755480376056 
epoch: 37, train time every whole data:455.16s
epoch: 37, total time:13160.74s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.66s
test time on whole data:59.23s
0.8878650077812906 1.8278056660455546 31.768137415688514 0.9179062209946276 
1.724106060560438 2.9010404967452263 61.99356902430222 0.7811601144004082 
1.8203537859396033 3.2091339295860464 62.48670660929889 0.7348497475508875 
1.9106592008718069 3.438033941566373 62.6511940950414 0.6977222342445745 
2.0121049480578375 3.666444697483426 62.38629767890437 0.666292745261767 
2.1070777081740752 3.8561959093983234 62.67281804598896 0.6399934781514517 
2.1777048640559826 3.9861690791551565 63.29229844622347 0.6207729422749809 
2.234431886683706 4.086184489301324 64.21070806282376 0.6066396051036791 
2.296443035424731 4.1882124612717995 65.60724155616563 0.5938276349817155 
2.3421601446868765 4.2651637908708375 66.14496439239097 0.5842508896361839 
2.3826028064970104 4.325408101427279 66.37762873744092 0.5776597100941449 
2.4521739170867063 4.413515007215863 67.7116135085238 0.5646803568932969 
2.0289736138183385 3.749400557023207 61.4422690153072 0.5646803568932969 
epoch: 38, train time every whole data:455.69s
epoch: 38, total time:13688.77s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.76s
test time on whole data:59.40s
0.8858161468029554 1.82735418697317 32.05427715826896 0.9180739130522941 
1.7213198234907219 2.8974875186390574 61.624136185221815 0.781148505937805 
1.8049664875001956 3.174154829823825 62.568203414603154 0.736910168240094 
1.8769558122953667 3.3546322885754125 62.9093988419378 0.7057929044493096 
1.9511480988195786 3.5220649979040797 62.38250184849738 0.6831358985125736 
2.0280887317350578 3.6794991671380015 62.22902192140718 0.6615390578374928 
2.0909053210199233 3.8107421522182423 62.40115051399924 0.6407502176571281 
2.1477686307729176 3.9290410646817175 62.95816061362207 0.6204937850782817 
2.209441237626065 4.04890586403251 64.11941146487784 0.6002983196224605 
2.261548543226151 4.144742647318012 64.94463452400355 0.58283457505131 
2.31334797875255 4.222726994243762 65.55453320147419 0.5700375551888218 
2.367167792589714 4.295548421931484 66.2409799322504 0.5555323462412515 
1.9715395503859332 3.6380161621297717 60.832503434384634 0.5555323462412515 
epoch: 39, train time every whole data:454.47s
epoch: 39, total time:14215.06s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.98s
test time on whole data:60.83s
0.9090366339954947 1.8105117289956867 32.40311256727978 0.9185279892854802 
1.7204999616106174 2.8687533315340707 61.82151145197449 0.7832598106800635 
1.8120235873282489 3.1680771122877447 62.33746887778145 0.7362897538821934 
1.9030809951044974 3.3938113180766942 62.43872992016462 0.6971263936856396 
1.9957583416114073 3.606915618505915 61.86521609189227 0.6630935072875386 
2.0697467155099094 3.7571331221251225 61.65268002851453 0.6394708907038059 
2.1142811749107424 3.8436559033297657 61.56489598026879 0.6246906872842473 
2.1477161754094776 3.917925715466826 61.80723236317315 0.611683814652793 
2.1871188720983588 3.998953699270193 62.70973297569689 0.5999304532334496 
2.2237100619277252 4.06612844767778 63.58316082829458 0.5889264797320051 
2.2636110138847894 4.126096590930234 63.99535337065746 0.581461851697149 
2.317433152629506 4.201670108105351 65.20670723802606 0.568545337707012 
1.972001390501731 3.6230261805405717 60.11575129970305 0.568545337707012 
epoch: 40, train time every whole data:454.44s
epoch: 40, total time:14742.47s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 37.07s
test time on whole data:60.89s
0.9240399467157466 1.827675929933788 34.35874145241161 0.9190574545990933 
1.7264678923206493 2.8933556538957546 62.21044808034982 0.7849224238135762 
1.8259206673752162 3.1872607221755804 63.69302475183489 0.7384351360647101 
1.9136566543615467 3.3955455262209533 64.803341846141 0.7021609782077773 
2.00475142798351 3.596536630425571 64.83380941461584 0.6725338422028229 
2.0889286638536446 3.767700932696381 65.08788526549745 0.648018755059656 
2.1597184142768384 3.904907532348608 65.67169838556094 0.625920376183848 
2.2279323348238 4.035526202260202 66.64093062889057 0.6040286357442072 
2.304097796984017 4.16488107680983 68.01522757943148 0.5844254029264334 
2.362507546008786 4.26044578741312 68.94905071717233 0.5700505172572553 
2.4144836350117944 4.338199660451359 69.42728241313095 0.560909269609738 
2.4863499028725284 4.429901090446207 70.65039328748628 0.552295717062222 
2.036571240215673 3.7193897332160937 63.695521548592446 0.552295717062222 
epoch: 41, train time every whole data:457.69s
epoch: 41, total time:15274.77s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.63s
test time on whole data:59.19s
0.9557432392153534 1.8759433644943777 33.80494947015441 0.9200382381041251 
1.7233255261291882 2.945842707869182 58.58106818642275 0.7911944184943991 
1.812564725039793 3.2317568894570257 59.37037824833561 0.7524218981114654 
1.8826166163319278 3.408102078579724 59.946310477232956 0.7239438179742396 
1.9656357102970636 3.5708670006172927 60.46923466703106 0.7029875066567144 
2.0403667150029823 3.705598324507786 61.29862655707547 0.6841604449370033 
2.0893354413770138 3.7993788802302273 61.79975878506983 0.6687740238825847 
2.1308810239444 3.8840857687769246 62.279121225054766 0.6543215786228184 
2.1770280621808378 3.9725199577033385 63.03479972145778 0.6393627929679536 
2.2052471438039625 4.030597730144246 63.35888423789968 0.6266560855062175 
2.238506812112051 4.082612959519606 63.529380157793 0.6174114241131773 
2.282160432997204 4.145843210975884 63.86295837153221 0.60432845594368 
1.9586176207026482 3.6072022253419602 59.27826611511846 0.60432845594368 
epoch: 42, train time every whole data:456.95s
epoch: 42, total time:15804.23s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.58s
test time on whole data:59.20s
0.9048577505297781 1.8473658292351844 31.336143312437443 0.9202238707966247 
1.705054046417897 2.908990117057239 57.83082962241989 0.7918436266845315 
1.7858321573467304 3.1937887100699065 58.27453875496567 0.7500074868923097 
1.8443203341166179 3.334743870568516 58.756809145776565 0.7228787285331848 
1.8980160656431246 3.442012416441728 58.98702575620432 0.7053267719126605 
1.941269675404809 3.5111718350320467 59.443047385527095 0.6952401977236918 
1.9624421651974497 3.5483022653640672 59.55703873912738 0.6912264616767665 
1.9868931099634086 3.6063519576303347 59.59126404744074 0.684495721906371 
2.023726027824694 3.6978582222001912 59.79727249818103 0.6721240046278385 
2.0633144950118982 3.7869327646953694 60.127734805261724 0.6553512984732022 
2.110972554784002 3.8723236796252944 60.095216875263965 0.6411214738693549 
2.1666410448952975 3.961437683337262 60.35196009639827 0.6229424861549677 
1.8661116189279756 3.435970046778982 57.01267072382473 0.6229424861549677 
epoch: 43, train time every whole data:452.98s
epoch: 43, total time:16329.25s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.47s
test time on whole data:59.07s
0.8704954003088531 1.8027216341764742 30.951857051453814 0.9202248373411726 
1.696254015137131 2.8390799433977607 60.47963431503657 0.7909698981854809 
1.7825942910279013 3.115555441352736 61.37615154036433 0.7480297148602885 
1.8554331663968484 3.290632721165471 61.799296805508085 0.7155747678350592 
1.9209666044302285 3.4337818790019243 61.17510034481797 0.6927122075034373 
1.9717185846645207 3.537643230704727 60.85328742927791 0.6773296595934915 
2.000903286215982 3.599604111199111 60.64258242278837 0.667486017093127 
2.0267521920556293 3.6629459395020136 60.792020278778246 0.6573809748368767 
2.0588793342230574 3.7382075924796405 61.38925456740777 0.6456638346049908 
2.0898559098923135 3.8039024553723606 62.36115192021479 0.6308569409672534 
2.126538561648379 3.8649548454790383 62.748940435393266 0.6196627782499231 
2.1776428556912357 3.9440387213099797 63.66567491930677 0.6045423178820615 
1.8815028501410067 3.4334435732671293 59.01984494706066 0.6045423178820615 
epoch: 44, train time every whole data:456.48s
epoch: 44, total time:16857.05s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.55s
test time on whole data:59.09s
0.8661180884601283 1.7908020813833563 31.650526540081948 0.9210112303089552 
1.693181889489914 2.8165849285631346 61.56401120440768 0.7927611619109712 
1.7676350130418403 3.0675957143963877 62.57585580572853 0.7530364019105246 
1.8262103082111905 3.221004333523069 62.58815905989403 0.7273623666302906 
1.8892592581514092 3.360256352277078 61.70257896269868 0.7094750893400625 
1.95374815935677 3.4847017750025606 61.35052556811935 0.69533313684486 
2.003962855820766 3.5962020222334914 61.127439178668155 0.6805834063434125 
2.0573341587467917 3.7235098698801234 61.21986334292152 0.6618353451277919 
2.1128369252906136 3.8429611010890783 61.74808630686726 0.6448437708788023 
2.1470666629475144 3.9210362280951947 62.180432126286725 0.6296967046909966 
2.1839354525999655 3.981665609452417 62.29929854095305 0.6210337151221962 
2.2219695493569156 4.032536098594003 62.96629272171509 0.6107049321578107 
1.8936048601228181 3.4570541725828186 59.414662670836506 0.6107049321578107 
epoch: 45, train time every whole data:456.77s
epoch: 45, total time:17387.38s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 35.56s
test time on whole data:59.07s
0.8630270398416158 1.788764217014297 31.000848146317374 0.9215223153081087 
1.6936516379033704 2.8356002032234735 61.0558330517715 0.7912087482188317 
1.7900515981876246 3.118216499821395 63.194678630325654 0.74450246455551 
1.8742476549561002 3.308752564770995 64.45319996841793 0.7081985961144271 
1.9482624888264886 3.4702261097240217 64.18729844130536 0.6814283582175062 
2.010402206061142 3.604506060784403 63.83650133306126 0.6595918016522117 
2.057954433048144 3.711152186951741 63.563821855723425 0.6395604819047597 
2.0981435844875165 3.8114536432520505 63.18335410897548 0.6223717025910009 
2.138698860935423 3.9020307047369327 63.10026387004828 0.6108165587091839 
2.1694420359053073 3.9595855966861806 63.38491516952621 0.603843622140247 
2.208367582326134 4.018390931454334 63.21006472410493 0.5998481398893327 
2.2492666488402477 4.074159234643185 63.61218430563515 0.5905486296390257 
1.9251263142765929 3.522506057492591 60.648853343235 0.5905486296390257 
epoch: 46, train time every whole data:456.79s
epoch: 46, total time:17915.47s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.52s
test time on whole data:59.07s
0.9207839485726186 1.784670004453151 32.95145515091152 0.9208634390572249 
1.698596324687114 2.8093989017938 60.50400078136461 0.7920848653206937 
1.7782610828454295 3.0535511458064764 61.57876511400728 0.7526370267908237 
1.8492164368945219 3.219988506759845 62.30255994882086 0.7204079764610842 
1.894714833961711 3.315475221654665 61.960728719124326 0.7021391194317868 
1.9203591596877114 3.3608434521204935 62.004667813382376 0.6930383801762318 
1.9350207830153938 3.3854164547023027 62.39689836570002 0.6863462548815151 
1.9511997611265453 3.420887292348001 62.92934243049202 0.6783569856462865 
1.9701323794830767 3.4688783439423405 63.555110715719486 0.6684854395501406 
1.998617649295323 3.520305328343999 64.99960095190642 0.6551344146486436 
2.035739566422023 3.585964871035536 65.4329674749526 0.6400543920142592 
2.0833601644358466 3.677468200371728 66.20850114692946 0.6173256243502934 
1.8363335075356095 3.2535879154479397 60.569025724553896 0.6173256243502934 
epoch: 47, train time every whole data:456.69s
epoch: 47, total time:18444.07s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.48s
test time on whole data:59.07s
0.9177791197080875 1.8329324150376358 32.50576061029911 0.9228848170277275 
1.7106162323423972 2.919460506913257 57.80940519365071 0.7959964082576867 
1.8239573019970918 3.2661106621123537 58.35701165330812 0.7525260139195066 
1.9321622018181674 3.534878153078847 58.82612449001993 0.7118582219481964 
2.0466933127054503 3.7585252809553245 59.63092206736571 0.6811357128859018 
2.136853227349974 3.9132799600643766 60.79128989608936 0.6595218302164292 
2.1918189106882506 4.009319126973837 61.459064253195294 0.6432551117936902 
2.2400086301519933 4.093417516852651 62.08875287215949 0.6325383430125832 
2.291432926545363 4.182698332684461 62.871892984008014 0.6254929697424402 
2.317535838270737 4.236226628372508 62.83290830938696 0.6210884981467655 
2.361656158331427 4.305050768787672 63.09595635125012 0.6196151410032675 
2.3918626310696736 4.339205927179749 63.12044101565329 0.6179076937985936 
2.0301980409148843 3.765185520616272 58.616122824511116 0.6179076937985936 
epoch: 48, train time every whole data:455.40s
epoch: 48, total time:18972.78s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.59s
test time on whole data:59.11s
0.8843156091981523 1.7762682463785158 31.95560648763458 0.9218953478016858 
1.6957417631071239 2.77027091147804 60.87112471605478 0.7984015153910949 
1.7625893738039193 2.984318319799325 61.92842305761285 0.7640442018572736 
1.8169514403723712 3.1320597477689307 61.819005920515515 0.7378109708016343 
1.865384776027341 3.2522301248092824 60.98824001479712 0.7189614972329703 
1.9035325389780282 3.3284010909434514 60.44643631205333 0.7105928219662206 
1.9304410120476747 3.399672788604745 59.85899741492069 0.7018338178640506 
1.9635693800607252 3.4918153925261626 59.30681389394054 0.6892008746764829 
1.9997438632199274 3.585518041196237 59.21192822020094 0.6759772188298369 
2.0289775551459086 3.661932142214613 59.52097789545674 0.6612155402365179 
2.0732376941882427 3.7491996803343604 59.50020888486027 0.6482130035279399 
2.110343840802443 3.8190170978443923 60.006682442343205 0.6346053487442799 
1.836235737245988 3.2896094599778123 57.95138898703972 0.6346053487442799 
epoch: 49, train time every whole data:457.93s
epoch: 49, total time:19503.62s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.59s
test time on whole data:59.21s
0.883873758817535 1.7867936247337786 31.448157896522194 0.9223735539767562 
1.6855012170197885 2.8184142517131963 59.37826031758377 0.7973625489725584 
1.776472335950516 3.108842511841332 59.93628945873816 0.7563836554401296 
1.8658739384534282 3.3471296756607303 59.70581787552075 0.7171441923049588 
1.9472570842965728 3.5334498663988607 59.008572251325084 0.6901449587339995 
2.007303774387975 3.653818296925649 58.78974117026554 0.6734638222899932 
2.04051995712226 3.7175894666947507 58.70604922416128 0.6626464925884326 
2.0669119846456283 3.773470802636636 58.78240630719313 0.6539885384236384 
2.0927748012258007 3.8334773121815213 59.130259722773594 0.6456816521608256 
2.1097819378391973 3.871351397664498 59.78879251359851 0.6372857206087077 
2.1465408421878127 3.928471857902365 60.08686664564418 0.6322314832537317 
2.1775713818645372 3.973396169225088 60.537434247651575 0.62141296075062 
1.9000319178175875 3.497655843924332 57.10844455597716 0.62141296075062 
epoch: 50, train time every whole data:454.54s
epoch: 50, total time:20030.64s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.89s
test time on whole data:59.73s
0.856919752486405 1.7779165849485026 30.80310612896724 0.9236709774068949 
1.677976235492155 2.8239665379963865 58.651186385834876 0.8008772103503654 
1.7618697999832886 3.0928101960771093 59.02925998811879 0.7651627429266936 
1.835026185947426 3.2820466152006933 59.301843455069836 0.7353780911282447 
1.9055502526109063 3.4228585587672984 59.692766700711964 0.7170658719581917 
1.9637213022523514 3.528726375954997 60.33717503926431 0.7028020869937355 
2.001903401465643 3.610303993585535 60.76431985423169 0.6871395546522489 
2.037682662899739 3.6919596865854403 61.29864991511925 0.6717560425069262 
2.070749070199029 3.7613550256442507 61.882916052342516 0.6611767890103455 
2.091929809264218 3.8092171909457173 62.43979167601352 0.6503227775273267 
2.1275965715675125 3.873275795801269 62.569393792138975 0.6405964035789213 
2.157071997544063 3.9218227352964146 62.51618983103442 0.6302950644737604 
1.8739997534760613 3.4320389624996643 58.27419335491696 0.6302950644737604 
epoch: 51, train time every whole data:454.61s
epoch: 51, total time:20557.39s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.52s
test time on whole data:59.01s
0.8890741433013408 1.7782083586511492 32.855528823355876 0.9248022334247821 
1.6738752299952542 2.8110058833255707 58.97285566550967 0.8041517788942295 
1.7451854900837476 3.0420451548276737 59.44434198805219 0.7750175223086883 
1.8126628660431043 3.220385382777437 59.8909910224829 0.7498214660746232 
1.89803161724373 3.3847061237001634 60.957818177750646 0.7294637262498399 
1.9821101317662924 3.530873937751287 62.380200632247565 0.7102335553064157 
2.0466699175234173 3.6649750424874616 63.243049587753966 0.6879546459767759 
2.1117698887184795 3.809962903587375 64.03649650976853 0.6640768382824926 
2.1831886974118118 3.951331255045244 65.04973868912916 0.6430270296169611 
2.242471720799449 4.068427905867164 66.17164936973614 0.6212939898825505 
2.3155126988272228 4.194997480451494 67.02732393010176 0.6030490152179933 
2.3779191912297337 4.279711795613847 67.84128106411045 0.5950603315319314 
1.9398726327452986 3.54305572804282 60.65632487375401 0.5950603315319314 
epoch: 52, train time every whole data:452.69s
epoch: 52, total time:21084.49s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.62s
test time on whole data:59.31s
0.8786659552631456 1.7515800534583117 32.7092532761992 0.9252401397448926 
1.6697075773037615 2.784877240043807 58.76908997518338 0.8009250511269589 
1.7547571561858057 3.0496392769286396 59.03429286964831 0.7643763177262055 
1.8360889603789186 3.2642138668722875 58.918927275796875 0.731380928634301 
1.9157450300730943 3.432286943797626 58.70375587492196 0.7110503456703331 
1.987068401661746 3.5690994226034487 58.901522186288155 0.6967363708094151 
2.042382136937142 3.6852960818283838 59.182896152363654 0.6806836658556326 
2.09534808763906 3.802824848342965 59.67970345808439 0.6634952463821165 
2.1408215077753576 3.900756976544126 60.13412157544854 0.6523161330703352 
2.163562782084392 3.95815480305666 60.17439017743265 0.6444659318485544 
2.2000884858963214 4.019622847247879 60.162085390253864 0.6392513251916562 
2.2170224594656553 4.038122084403034 60.12459340840318 0.634855002039739 
1.9084382117220333 3.496279051153754 57.20812702226225 0.634855002039739 
epoch: 53, train time every whole data:455.71s
epoch: 53, total time:21612.72s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.60s
test time on whole data:59.17s
0.8725011056954307 1.7438499142709327 32.300170675127944 0.9245912123433266 
1.6710493271432463 2.7336627268547926 62.6319210217791 0.8040662511389745 
1.738606481944876 2.9170534585088324 64.75969387873708 0.7737102726338208 
1.7965403084399267 3.0651530666433615 65.42093229734576 0.7478407181168327 
1.8514705514809382 3.198320772615784 64.8784183912429 0.7273768858000799 
1.9174492130586434 3.3443675916486173 64.41358599206849 0.7072407176399149 
1.9894674135181343 3.5309599526991744 63.88162062687803 0.6786774840829143 
2.071418815475933 3.7385605932641823 63.63357567886743 0.6470371332962096 
2.1439258143582514 3.897216114732426 63.79350979167183 0.6280133917695618 
2.1896007985510817 3.9906509583198 63.78117663171052 0.6192554424213197 
2.2394534823113963 4.075714047058054 63.57673963737701 0.6176369546005381 
2.2741123044200773 4.124248971854543 63.59781085502698 0.6188729690511326 
1.896299634699828 3.4279648429065053 61.389343163060786 0.6188729690511326 
epoch: 54, train time every whole data:457.49s
epoch: 54, total time:22143.87s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.70s
test time on whole data:59.37s
0.8787832061224395 1.742021881334157 32.09331220287003 0.9247517389483184 
1.665537754180442 2.728294039239966 61.274678093360066 0.804766417291782 
1.728129844521128 2.9027128290259085 62.87885311225162 0.776367569021241 
1.778571509091361 3.024753840686653 63.60611677996594 0.7545514073307611 
1.8147961061252724 3.100132888394534 63.59304672876104 0.7415718682891334 
1.8483974354215675 3.1533480672600946 63.756845502221935 0.7323745006230507 
1.8828519536065205 3.2362534400131056 63.917142253177815 0.7160163160521332 
1.9125650436660895 3.335779312259852 63.67449647353304 0.6972645815047954 
1.9390428194855118 3.4210255554768927 63.63246222787422 0.6817232383715948 
1.9685237350899372 3.49050886417433 64.6511743330234 0.6669616411236301 
2.008470981532619 3.5835127705058176 64.61753066631194 0.6493109167526312 
2.058390546120171 3.6914405474404575 65.0031466011392 0.6269846084899874 
1.7903384112469216 3.156523128710433 61.05852758189372 0.6269846084899874 
epoch: 55, train time every whole data:456.87s
epoch: 55, total time:22675.75s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.58s
test time on whole data:59.22s
0.8815486697408238 1.807198615866697 31.438473159521873 0.9240741836696899 
1.7070812579247037 2.914508407060164 57.35085687594581 0.8014765840075984 
1.8315719756653444 3.279795163378769 57.87969987370667 0.7578577114992029 
1.9456556644230372 3.5691208889949344 59.03818718928823 0.7112118196150599 
2.0465202248546164 3.7522166637012204 60.83432860324758 0.6840873187779622 
2.120570057251801 3.861536039296587 62.590808801493814 0.6670336401464829 
2.1661369893267928 3.9279293003166327 63.62875059724864 0.653040990296906 
2.2110364282985353 4.003422305140047 64.73382054510938 0.6376908211151812 
2.253471846340845 4.079390497636183 65.66338778416561 0.6251154879948542 
2.273390024933432 4.12223605320933 66.0636781390174 0.6174737995733172 
2.3084344200213396 4.184634746678981 66.14693760541927 0.6100909697583212 
2.343341220282018 4.227283861288985 66.62725067431167 0.6085110661833482 
2.0073965649219407 3.7049288034019696 60.166771102021585 0.6085110661833482 
epoch: 56, train time every whole data:456.21s
epoch: 56, total time:23204.95s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 35.54s
test time on whole data:59.22s
0.8994729304708363 1.7455952783484288 32.62555939767618 0.9245982697581385 
1.6635556994900107 2.717991489359558 59.125300465779794 0.8094185884862438 
1.7260099711886474 2.936624899462294 59.16691138224596 0.7811811026485517 
1.7861984977242316 3.1061366361023617 58.8088145667896 0.7573163993920069 
1.8485057104106637 3.244658780398291 58.81975435301511 0.7407042117050896 
1.9140452129329955 3.367818414152728 59.28406250283517 0.7272339765285276 
1.978248404594227 3.519964185406498 59.756043947729076 0.7018175951088711 
2.0432681648607054 3.682732410012696 60.36280665139942 0.672052576252656 
2.0986109438060474 3.806899991619422 61.0612504382657 0.6511860056644471 
2.1361906631032803 3.8929659615705523 62.047836379022904 0.6344347498399536 
2.191169065327073 3.98939295279501 62.564164774139606 0.6201427583436018 
2.2299416800173266 4.051442296023219 63.289082741453015 0.6093293888747913 
1.8762680786605037 3.3973791478442377 58.07624760235661 0.6093293888747913 
epoch: 57, train time every whole data:454.88s
epoch: 57, total time:23732.06s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.47s
test time on whole data:61.15s
0.8495525588468604 1.7326270249573625 31.182711678655213 0.9258455362851403 
1.6581491575440659 2.717441450090979 60.782998432657095 0.8073972900569908 
1.712289552453107 2.8891686821848097 61.86758422688974 0.7803563705644284 
1.7617991264658492 3.0277462380229774 61.861177155952454 0.7580524114203699 
1.808215103727721 3.1415486021748125 61.369097930774764 0.7422075244368596 
1.8548492485537593 3.2338984785691296 61.025811297624145 0.7320474611320679 
1.900434135407032 3.345433392062409 60.64910131923102 0.7173812256387201 
1.9546490988493674 3.4910329994165092 60.559403563842906 0.6956606117387618 
1.9988278230530698 3.601100575816768 60.67583964295253 0.6798737233995202 
2.0194516981160713 3.6590510386430877 60.696776702421374 0.6693165489541538 
2.0484782746111354 3.7187289082589547 60.32511013211289 0.6623395591532351 
2.0635176502343797 3.747860616776858 60.08724018034351 0.656609978292792 
1.8025177856552015 3.23830812472572 58.42379074027853 0.656609978292792 
epoch: 58, train time every whole data:454.77s
epoch: 58, total time:24258.85s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.46s
test time on whole data:59.04s
0.8415198327165452 1.7278845832154497 30.995486253950627 0.926436647768088 
1.6549836739050667 2.7209338101985274 60.70010335360075 0.8080139817415035 
1.7190014030670835 2.8976374171976875 62.003099609211674 0.780899903985318 
1.7806566510201387 3.056785886607239 62.21251015017406 0.7566356329547153 
1.8424075629932009 3.200682374961981 61.955363427078325 0.7371768919225148 
1.907144684417262 3.334423314042279 61.84845082583321 0.7194933410753669 
1.9657075888187225 3.479669218034448 61.78523039265589 0.6956420713453146 
2.020804140393223 3.621269907011532 61.79039373831298 0.6726264378625451 
2.0620033320175217 3.7226141774935346 61.774468271213834 0.658910543797272 
2.0866757922805848 3.7839152669652694 61.7474495003146 0.6503798405399633 
2.1253855658902654 3.8606701954215716 61.39990849106829 0.6438579356513466 
2.151903307511693 3.9063388776510166 61.21256125977956 0.6369434855355177 
1.8465161279192757 3.3298383024605243 59.11900172451402 0.6369434855355177 
epoch: 59, train time every whole data:457.32s
epoch: 59, total time:24789.70s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 35.73s
test time on whole data:59.29s
0.8435030045718664 1.7341414041033063 30.806234878648546 0.9263689079881938 
1.6482731432327558 2.707511824250565 59.549703675478746 0.8107194437125328 
1.7043240126379366 2.8814097525730236 60.528484233904464 0.7844680786482688 
1.7557183164918706 3.0218786120925856 60.55850152337264 0.7620632701692114 
1.803781844408207 3.134932706658233 60.44364849419496 0.7454678375088982 
1.8488749322276563 3.2155385292917806 60.450809786435364 0.7354982204277197 
1.887329055435512 3.298419481797837 60.23606464343126 0.723721259195119 
1.9277423607872002 3.410263414292392 60.00593724417234 0.7061216038855356 
1.959343703087774 3.501249425146748 59.899743926063586 0.691175081049496 
1.9773139402318214 3.558572874508608 59.96552180203519 0.6793690169859308 
2.0116597585776557 3.6378461716899317 59.54411471161192 0.6676245758458543 
2.041567203817534 3.6986894764991067 59.4853413489311 0.6578558475694173 
1.7841192729589825 3.192249924823803 57.62307165148708 0.6578558475694173 
epoch: 60, train time every whole data:456.47s
epoch: 60, total time:25319.06s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.49s
test time on whole data:59.02s
0.8734634866272765 1.7588643800584007 31.23134539267094 0.926507709196751 
1.6512151283396497 2.7434253610746233 58.163083172363116 0.8109946009502843 
1.7138856857703733 2.9274295893617657 59.28015115515402 0.7841138715653697 
1.7616969985685178 3.0352725624132177 59.896434644384755 0.764756032574045 
1.7959314855955364 3.0941739742218504 60.07066408388093 0.7552149523116368 
1.8282316622222285 3.1457261417761417 60.249815307173314 0.7473398298926046 
1.8623940923669864 3.2209174204225954 60.44671874747406 0.7335900409223893 
1.8963889862533128 3.325913918649615 60.34157458620613 0.714800003935375 
1.9299280696835015 3.425919429090487 60.38305481678762 0.6970298666921272 
1.9621952351121499 3.5104687734147926 60.877850280427936 0.6803928781996093 
2.008293467391105 3.61705092527721 60.52794944106109 0.6638320659221498 
2.05955646249306 3.7147244627520073 60.922207240028406 0.6456719715349035 
1.7785983967019747 3.165580229689949 57.69950497918197 0.6456719715349035 
epoch: 61, train time every whole data:455.08s
epoch: 61, total time:25846.57s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 37.15s
test time on whole data:61.89s
0.8840927670967011 1.7276828823982553 32.80974903847723 0.9260432417992331 
1.6513632428081084 2.7136087267894475 60.2284272278267 0.809194998385999 
1.7176325354153024 2.9356806598369247 60.22661655888323 0.7787835559907202 
1.7966322582433267 3.149038625730538 59.935194299804785 0.7489488639992155 
1.881285239501191 3.3331192605699873 60.036654200487206 0.7258177095454046 
1.9634058423459175 3.4939778038666964 60.528326089316444 0.7053665161152103 
2.0333814047504037 3.649978873736349 61.018385619478074 0.6791908700328056 
2.0981637744350095 3.803123831634598 61.52740958401694 0.6522512302309624 
2.1481857597170664 3.9125220883387444 61.93843886144399 0.6356382882497752 
2.1775396021028004 3.9791605389031135 62.42461402903328 0.6245614706431738 
2.2190591328677143 4.047682663233118 62.52174216411542 0.6187142004570619 
2.241371555316661 4.069887243365389 62.63739348080845 0.6249205760947129 
1.9010094262166837 3.4650174605829465 58.81968585048594 0.6249205760947129 
epoch: 62, train time every whole data:452.39s
epoch: 62, total time:26375.27s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 37.24s
test time on whole data:62.01s
0.8499697946114022 1.7119604099631396 30.581873343030853 0.9275839440059993 
1.6480635674037927 2.683391875851698 59.09390403316246 0.8124894479711472 
1.6985892478758027 2.8555786357054815 59.37505640324021 0.787989761520697 
1.7503126487135001 3.0050056101609446 59.12854353303211 0.7660406492298364 
1.798246114099487 3.1186595234935806 58.92731232607573 0.7509051083249594 
1.846474819454675 3.2180451182650427 59.16811587019535 0.7373665296097593 
1.893491982919563 3.3374648442971613 59.561739087120046 0.7165732980338809 
1.9438985817239043 3.4692822429954 59.977283900926416 0.6934002013670608 
1.9863701399908889 3.5793126950379843 60.39350335329505 0.6746730959391756 
2.020806538960231 3.6680626167206642 61.11269327809868 0.6590165795049463 
2.0741663781446182 3.773572708232198 61.08506348270413 0.6446880860847786 
2.116311898241884 3.8478003155134086 61.49415216582859 0.634249769803288 
1.8022251426783125 3.238875987167942 57.491874068148185 0.634249769803288 
epoch: 63, train time every whole data:454.47s
epoch: 63, total time:26905.74s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.63s
test time on whole data:59.13s
0.9437558096125722 1.753204453645805 35.841567676394554 0.9283197783449708 
1.6562229132921036 2.7481774535328265 58.711899733419536 0.811964055025268 
1.7163158739127573 2.9351102155154325 59.308955938139164 0.7882968595026435 
1.7862021883171761 3.115680679800956 59.67024738706387 0.7667275001260322 
1.8710327601085994 3.2915390210970474 60.35594732507714 0.7485867522297396 
1.9555999846944496 3.4545289697769066 61.087985539376646 0.7318063548798728 
2.0209396417344077 3.606226347142249 61.44072800231953 0.7083764903972406 
2.072009346114294 3.724713174363754 61.89137075506257 0.6881554483945159 
2.101836984069397 3.791286435716184 62.24969160154236 0.6780426924101352 
2.1123707696217155 3.82338808062809 62.56566170814065 0.6713327531471635 
2.1460436572117643 3.8896731642893876 63.019201990502026 0.6610695908103877 
2.174005628468912 3.928531360549982 63.53793659390209 0.6530602790461262 
1.8796946297631791 3.393007082038784 59.14037894226044 0.6530602790461262 
epoch: 64, train time every whole data:455.31s
epoch: 64, total time:27431.99s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.18s
test time on whole data:60.33s
0.8409373997905779 1.710416476955351 31.336663801738922 0.9277302516964216 
1.6475681539811193 2.6849604760773103 62.230132010372344 0.8115396597084868 
1.7086782605351791 2.834645333612503 65.15495382664355 0.7868956491710871 
1.759064704097009 2.951368198564699 66.58763150212351 0.766928316543349 
1.7945697219513712 3.03840557216471 66.82783467944932 0.7517719867725119 
1.8256264353720029 3.0981733994940623 66.49856442534173 0.741653309430112 
1.8564692025328322 3.1770203068233593 65.65462721858879 0.7285770442965338 
1.896553381835687 3.310607036068834 64.55792757498179 0.7059661509325651 
1.9353044809284843 3.4305740594063874 64.0599362650766 0.6845529577825187 
1.9641184038848927 3.5005438095873074 64.22868188537161 0.6697970465572228 
1.9947365898592841 3.568544860969206 63.74627076171886 0.6573347779464548 
2.021916158911462 3.6131737154612305 63.620957628810274 0.648105224153478 
1.7704619078066584 3.1168449802096916 62.042278737888225 0.648105224153478 
epoch: 65, train time every whole data:454.10s
epoch: 65, total time:27957.18s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 35.74s
test time on whole data:59.31s
0.8881400336583278 1.7196643847487885 33.57430970199791 0.9287109002592927 
1.654661653723479 2.7394110680021173 59.38128608647657 0.8122716024687054 
1.7252489797741708 2.9567339779151514 59.449968710319745 0.7867301728329965 
1.8105940879140758 3.1723296084409927 59.856912157655714 0.7614599689955994 
1.9047409867390634 3.371677162140402 60.76213162853223 0.7386099496299743 
1.996192618838467 3.551729983331323 61.9190836634844 0.7150823770143309 
2.0737358265552848 3.7234130530910283 62.76846747671235 0.6859666223534396 
2.150318913752834 3.8886357720971856 63.58023369458395 0.657882461224085 
2.2135295676834703 4.022011834588043 64.2588764205135 0.6376439835330887 
2.2569736165646463 4.113105093380839 64.5770363511777 0.6242106458781027 
2.3105849250215327 4.215588248893871 64.92193113262877 0.6102330176736889 
2.3408776091235204 4.248505479951716 65.14114580651446 0.6104950456915982 
1.9437999016124061 3.549073248195992 60.0162883589092 0.6104950456915982 
epoch: 66, train time every whole data:456.30s
epoch: 66, total time:28484.78s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.45s
test time on whole data:58.97s
0.8352971870240179 1.7187685052951915 30.338456407596485 0.928050126315405 
1.6421928466168187 2.7421182800750232 57.87495062803867 0.8121381925455703 
1.713062793657361 2.97108830462992 58.08817800150729 0.7815467281600278 
1.7808085427731275 3.147209551157324 58.29545129566639 0.7563607006911194 
1.847579320918325 3.2872401147084993 58.59907258098658 0.7402043229753823 
1.9150225561955678 3.4124681426784895 59.113922590509425 0.7291251697482352 
1.982451939162133 3.5669863845845944 59.57965604509808 0.7087260528313732 
2.0570222307833 3.7416425719254067 60.44243326341593 0.6824036734481215 
2.12269337389423 3.883879987165607 61.471240589671204 0.6611831203352121 
2.1692785472399776 3.9764926297163217 62.42883483012438 0.6465313645883053 
2.2188887756663775 4.059589155134559 63.237147140082975 0.6360954812847854 
2.25187857865214 4.091713884866792 63.846120354582915 0.6351835551224889 
1.878014724381948 3.4460030339862975 57.77661754782019 0.6351835551224889 
epoch: 67, train time every whole data:456.28s
epoch: 67, total time:29013.32s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.68s
test time on whole data:59.25s
0.8335280849597461 1.6988499816320917 30.913157094259187 0.9287228678682173 
1.6336040021764735 2.663145067163726 60.63437996211166 0.815036096683539 
1.6797203154888536 2.811091768259627 61.509751046774355 0.7924723369370195 
1.7300600324357371 2.9522246932497493 61.50572580503594 0.7710037338152651 
1.7834279062740859 3.0764121889559606 61.17868468213376 0.754470458138286 
1.8389545131905802 3.186468664501708 60.98583980787211 0.7422535961962787 
1.8931024308675635 3.321703969890191 60.8419273700772 0.7240955961524682 
1.9550109944742706 3.490880762837466 60.96496071902329 0.6981816449706859 
2.003323948510346 3.6113528452184895 61.30341117749705 0.6811463268061178 
2.0341640394294545 3.680462524413364 61.89807918483788 0.6720305795380147 
2.0733796802623465 3.753431784255087 62.22548470469207 0.6641457044637784 
2.1101616128018748 3.810471703259407 62.82678188326438 0.6583964438705115 
1.7973697967392777 3.2227030486156254 58.8992739043938 0.6583964438705115 
epoch: 68, train time every whole data:456.54s
epoch: 68, total time:29540.32s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.63s
test time on whole data:59.27s
0.851172248056602 1.7013240965383951 32.48856219096435 0.928721438370302 
1.67156274497509 2.7000329172735533 64.57705847621995 0.8107535933262455 
1.7398665331474372 2.8556073656982215 67.4310818230922 0.7849892711694071 
1.7932470494160162 2.978752112655895 68.48133014728973 0.7632503193013795 
1.8284897975620946 3.0613218737333487 68.18419180115902 0.7481868978110563 
1.864317294412869 3.1328927512324083 67.55733318971794 0.7354821359316428 
1.9077122387363619 3.250080325484603 67.06263378423455 0.71370675604853 
1.950446780340125 3.3852397036216635 66.61673990303481 0.6874367489922908 
1.9818791263223225 3.4660530631982196 66.76253789642472 0.6704487645210249 
2.00230244923383 3.493475003966013 67.58258221822446 0.6622790209570526 
2.0305118956367174 3.539542693803452 67.84527210708498 0.6518349387613256 
2.0693235046131804 3.5993382170125794 68.1501115992389 0.6376392647509433 
1.8075693052043873 3.1374512892879047 64.39524806753583 0.6376392647509433 
epoch: 69, train time every whole data:457.31s
epoch: 69, total time:30068.89s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.61s
test time on whole data:59.25s
0.8603978928656628 1.7002787081763062 31.440009477307257 0.9285200752177505 
1.630403712094451 2.6628216525249475 58.99977397251879 0.8165897698911683 
1.6794866660708296 2.8285685479469405 59.23538398231263 0.7938165236669052 
1.7309999301093852 2.9712971520288836 59.24337447181578 0.7740377741231447 
1.7862006294202237 3.0930485666965404 59.364288514108075 0.7587806002894237 
1.8432646765847291 3.2010487870950324 59.86236255386004 0.7459522867407165 
1.899142409819666 3.3258175454789063 60.320430674865776 0.7276007716077231 
1.9621865925891768 3.489982730289454 60.829930439329495 0.6992084769330319 
2.0120707410845373 3.6147984723884843 61.452208646989035 0.6764458738772218 
2.0414239837920203 3.6857451876088665 62.34786379075261 0.6620083943942296 
2.0805350237395612 3.7587402865842203 62.673995808790906 0.6497968459784234 
2.114287910783486 3.8177488579634633 63.354921498711924 0.6399527498972957 
1.8033666807461441 3.230331493309414 58.26067694360268 0.6399527498972957 
