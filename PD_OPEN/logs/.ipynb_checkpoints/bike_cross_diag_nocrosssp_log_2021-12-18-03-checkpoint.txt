total training epoch, fine tune epoch: 30 , 40
batch_size: 4
folder_dir: MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_diag
load file: data/bike.npz
ori length: 3001 , percent: 1.0 , scale: 3001
train: torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12])
val: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
test: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
TemporalPositionalEncoding max_len: 12
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (src_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
  (prediction_generator2): Linear(in_features=64, out_features=1, bias=True)
)
create params directory experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_diag
Net's state_dict:
encoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.3.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.3.sublayer2.norm2.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
encoder.norm2.weight 	 torch.Size([64])
encoder.norm2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.3.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.3.sublayer2.norm2.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
decoder.norm2.weight 	 torch.Size([64])
decoder.norm2.bias 	 torch.Size([64])
src_embed.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed.1.embedding.weight 	 torch.Size([250, 64])
src_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed2.1.embedding.weight 	 torch.Size([250, 64])
trg_embed.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.1.embedding.weight 	 torch.Size([250, 64])
trg_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed2.1.embedding.weight 	 torch.Size([250, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
prediction_generator2.weight 	 torch.Size([1, 64])
prediction_generator2.bias 	 torch.Size([1])
Net's total params: 1498242
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535]}]
predicting testing set batch 1 / 168, time: 0.75s
predicting testing set batch 101 / 168, time: 39.82s
test time on whole data:66.23s
21.433975208398753 27.567985152222665 842.9474260967415 0.032429739703722475 
22.597441172506038 29.034284462152804 892.0495520780646 0.02572712035078103 
21.0320819761636 26.609967101621834 833.1740195487088 0.02159208207629483 
17.85347031961071 22.325908854567558 706.4440919384965 0.021665493219931543 
14.255810613272622 18.029211355146984 559.4227048004949 0.02439052372384771 
12.343285433942984 15.840926779650404 480.8250333533204 0.027241738057147663 
12.355457130777072 15.83801526171706 482.8663208202083 0.02709509882065441 
12.852109253427813 16.33581107934469 505.7838424600656 0.025733083816439267 
12.231313430195586 15.556087885295886 481.8637181887343 0.025986128116583636 
10.432115090593163 13.435794352885441 407.95179126081314 0.02822425382061667 
8.847635459567288 11.438102459435274 343.18152454585186 0.03020517845479858 
8.365440035788342 10.735478625257953 327.38431012588205 0.029002280614631722 
14.550011260353665 19.518798713602248 571.9802462559078 0.029002280614631722 
epoch: 0, train time every whole data:253.94s
epoch: 0, total time:330.97s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.50s
test time on whole data:67.45s
2.4744827854933127 4.221536716343073 61.39692847363648 0.56692019973388 
2.5007585766828133 4.293660253295116 62.31097862224535 0.5308557515436335 
2.5270488712398245 4.367183980958608 62.870977620160474 0.49402669820938383 
2.5822520311649533 4.460142645724552 64.71072693630855 0.4480046813325116 
2.637827679598704 4.55639891584549 66.20183374238749 0.40524381628594003 
2.6694460227805235 4.621327437252995 66.65817055871585 0.367427575666491 
2.7244197025506622 4.687663974724758 68.2583566275401 0.32312472281862636 
2.7865063060122055 4.742880976991305 70.21426470965082 0.28117749291219274 
2.835795879271946 4.780939064566961 71.52913077244298 0.2490345620000357 
2.871977462246393 4.811719607223402 71.92540437853165 0.2294253286955167 
2.907463866527148 4.832992982868777 72.19972282523463 0.21466130557224147 
2.9527473607387926 4.826284769652035 73.23082508437028 0.19651626703159927 
2.7058938786922733 4.604974489562432 67.62584971691209 0.19651626703159927 
epoch: 1, train time every whole data:254.88s
epoch: 1, total time:663.89s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.63s
test time on whole data:67.59s
2.6150048862193667 4.493826906747302 70.14111249358949 0.6122511197603828 
2.628642509132446 4.529309886485292 70.20606347922687 0.5856436402855502 
2.6789022400146796 4.609192938508922 71.55729285027891 0.5507122384237716 
2.7420776405691036 4.699540093483613 73.44246628037058 0.5115001925686111 
2.8251400550911647 4.804694269145915 75.89443210938383 0.46640537777172836 
2.9005007497237196 4.897840084984862 77.95081257304082 0.418432953992183 
2.9896032106474575 5.003505375081341 80.64183908413919 0.3740972223455697 
3.081848663833081 5.102225200291094 83.55111146509968 0.333837950022751 
3.1612685057457006 5.180416415169148 86.10053759226123 0.29777841321815013 
3.264910627548716 5.270954874090047 89.78566887271823 0.26611548754992503 
3.353284776656578 5.341180689776048 93.01847751516001 0.23694237381721747 
3.376165917668048 5.350207470190101 93.54229035040919 0.21411102483612365 
2.9681124819041718 4.949358276209855 80.48646918736623 0.21411102483612365 
epoch: 2, train time every whole data:254.92s
epoch: 2, total time:997.23s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.53s
test time on whole data:67.43s
2.4917797071909797 4.429348213315714 60.66578089207444 0.6246686399791119 
2.4979703248929943 4.465594111537851 61.24094310859582 0.6072471524104248 
2.5320098087830205 4.533792846614698 61.331546828620574 0.5787111200803537 
2.5567399768314902 4.585952045787836 62.20096525622895 0.5453111643908174 
2.5904775257553196 4.644611998452354 63.24513088083389 0.5058537573426564 
2.6248347686589475 4.6962415168646325 63.93387597588488 0.46441474929653453 
2.678702581081628 4.767652657615898 65.15313591376088 0.42187764735163036 
2.7261668505890384 4.816321965625557 65.86091852005296 0.38345813899073766 
2.7694870115448498 4.85209599855334 66.81751711781601 0.3469648294155788 
2.8153060307922284 4.88825745941194 68.00682381990326 0.31608866280784026 
2.8704068982256135 4.930563780044906 69.67884897493133 0.287957538253886 
2.908042518971133 4.937355269529046 70.0208268425404 0.26178783034118486 
2.671827000276437 4.715464779694973 64.84653334234169 0.26178783034118486 
epoch: 3, train time every whole data:255.26s
epoch: 3, total time:1330.68s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.58s
test time on whole data:67.55s
2.3707283926387097 3.583297552708963 75.83277691552281 0.6448878622647817 
2.414704874086327 3.663017046758712 76.92835651603093 0.6161237704032367 
2.480399783307509 3.8667580355918285 71.77540798710541 0.5682773402455409 
2.5243341601844875 3.934279757378589 73.24547842259639 0.5359188592892496 
2.5773654081352233 4.005437758852532 74.96387444473721 0.5011269948905747 
2.632542476016407 4.071886394715085 76.69561132183719 0.46803804067861876 
2.6721897093182694 4.141470403629499 76.00384871896179 0.4373611250478583 
2.704972072369286 4.20330513034147 73.95856110411157 0.4123224359610828 
2.73917906868786 4.258381090848922 72.82105424306428 0.38517722534465365 
2.781609150964678 4.310315044362913 73.33360613579542 0.35281602124926015 
2.8244623256048986 4.362415623140805 73.73818435589904 0.31972536623354375 
2.8814336703958077 4.403364467612919 75.33396490999851 0.28967755044081694 
2.6336600909757886 4.074967747580436 74.55254135287122 0.28967755044081694 
epoch: 4, train time every whole data:255.29s
epoch: 4, total time:1664.29s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.56s
test time on whole data:67.54s
2.1712478204593064 3.8429886597923875 57.93651319880676 0.6987792977733823 
2.255321713988625 4.019561432580231 59.609485556303234 0.6584398889810618 
2.4438697370170126 4.348231434770428 62.306841927618365 0.5877040276330248 
2.496070985630066 4.438118767160898 63.620930042109045 0.5580913004088319 
2.5851798168788886 4.562719295331236 65.46512457674983 0.5213229160296983 
2.640828933430392 4.6321641414282215 66.11277794608746 0.49920561853773654 
2.6760152307980296 4.667106353076658 67.04812577758632 0.4972651488451679 
2.7322587208937676 4.717626913381697 68.83298100930877 0.4834495940700383 
2.7439992081824514 4.734703289259706 69.1098989949914 0.47625257120873343 
2.7705847649980515 4.764143764716708 70.19232984668837 0.45779246324906236 
2.791141218688428 4.7969608911502215 70.85000009181184 0.4354155466297384 
2.789339507066689 4.80959581475897 70.2263998789542 0.40998100945157895 
2.5913214715026425 4.537806141169537 65.94286432309895 0.40998100945157895 
epoch: 5, train time every whole data:255.19s
epoch: 5, total time:1997.80s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.55s
test time on whole data:67.53s
2.2837654314423426 3.8081236569686054 69.69840839955602 0.7097380887608058 
2.354096111708276 3.947264576774834 71.70936384759766 0.6697973466478878 
2.4383707319289623 4.1363874501643405 73.36750419554714 0.6207505026336776 
2.5203815674008356 4.259282611417872 75.4544737874191 0.5821512990285672 
2.578869775793648 4.358677276985815 76.31072792891095 0.5504741399976391 
2.6084827724532 4.399328607226793 76.54736666441887 0.5321464669902182 
2.6491724661476024 4.448151703700153 77.078085322414 0.5183436049915643 
2.6539438397275905 4.445002927602825 76.87897926993081 0.5161444940023145 
2.6614269505379102 4.45262612297589 76.53752344962848 0.516014839138956 
2.677375713817598 4.472603547885053 76.75335419679594 0.5123004151194784 
2.7284967791569374 4.530834231383815 78.5298149801012 0.49235718549163787 
2.7727881542141772 4.588944795450871 79.76136473584879 0.45869305918482145 
2.57726419119409 4.326729831215585 75.71905850674214 0.45869305918482145 
epoch: 6, train time every whole data:255.29s
epoch: 6, total time:2331.80s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.69s
test time on whole data:67.63s
2.0785475157162265 3.2175290667833547 71.1701594004125 0.7237365056853287 
2.1383201569936103 3.3567643135112344 72.26290738698626 0.6908407420856634 
2.188575671868399 3.52824632984847 71.21522601219812 0.6510737774994361 
2.2227698768770234 3.6172890380721023 70.7686355793657 0.6267727353028689 
2.2567946375268733 3.6785624392204066 71.23617810345931 0.6096950155892265 
2.2883522133030705 3.7161550989866985 71.21258015560544 0.5950699454635324 
2.303547576517931 3.7123191516866694 71.06794896403704 0.5917089265322315 
2.311636787819809 3.68948377617978 71.08291229759726 0.5944851778939687 
2.297592356042848 3.668541436335772 69.49138348196217 0.5996919166043343 
2.301488978092603 3.6820872877573523 68.77319721940765 0.595375360577398 
2.313938109425118 3.732845022582277 67.96783230130997 0.5804084505817456 
2.3757715222591624 3.8257625833822755 69.85893381787054 0.551630218024184 
2.256444616870223 3.6225618332719973 70.50894412201735 0.551630218024184 
epoch: 7, train time every whole data:255.50s
epoch: 7, total time:2665.81s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.66s
test time on whole data:67.64s
2.4266430109997414 3.9414060435549008 73.09354553548073 0.7329380200259094 
2.475832829953659 4.06864529951111 73.65672469236945 0.6986569573692559 
2.630674012546206 4.351152505944914 76.52103425767349 0.6406214977188794 
2.7070233935160295 4.469633780978289 78.35478280626896 0.6169329732986678 
2.7681659231067948 4.571095082147951 79.72068863702842 0.5910382868235144 
2.816360078440802 4.637767547522985 80.82056363698817 0.5792492677888854 
2.8492856566844775 4.651208335691436 81.615527408043 0.5944882271905182 
2.9034079132198047 4.685583557185871 83.77539319515688 0.60513040189806 
2.93985817780665 4.695405243521867 85.21923129311921 0.6191111278610263 
2.988396838554669 4.7435057042276005 86.99172631565035 0.6192471636548356 
3.0326559826765385 4.795148961575378 88.42439563232612 0.6135485041289631 
3.038038777346767 4.825235667178063 88.15283164184204 0.5954569351866061 
2.7980285495710118 4.544364370130746 81.36250103657926 0.5954569351866061 
epoch: 8, train time every whole data:255.06s
epoch: 8, total time:2999.48s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.68s
test time on whole data:67.64s
1.896361838100478 3.2369636596178895 55.24827034608896 0.7388021670546074 
1.9467027109490618 3.3608039838667056 56.649467739550595 0.709943610399946 
2.009074162764297 3.5375368867189354 56.12552017199809 0.6798444803521423 
2.045259649038049 3.6087329159383637 56.47747966390069 0.6653193719165205 
2.0900418899998603 3.7108407138787745 55.874167797582494 0.6482027284013047 
2.1176968768894495 3.7700422506939093 55.72482155081294 0.635234284642897 
2.141958616420715 3.824011854701975 55.68218157288547 0.6223537839165352 
2.150804196775137 3.84525945145264 55.41586635679479 0.6168429617556183 
2.144395127694788 3.8405894161641507 55.400009163566644 0.616029782441778 
2.1494936610253617 3.8525661094626353 55.53172378977859 0.6126038430608349 
2.174722412828001 3.8982815349878077 55.521254508118204 0.6039093018680437 
2.2185402298873913 3.953409822972926 56.18526716199877 0.5839918482291228 
2.090420947697716 3.7094602344384033 55.81965655713591 0.5839918482291228 
epoch: 9, train time every whole data:255.44s
epoch: 9, total time:3333.26s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.72s
test time on whole data:67.71s
2.216433722130511 3.5853823168441683 54.85274552034116 0.7036707493242088 
2.256692884967707 3.6937104284440574 56.001543701689826 0.6631386156844681 
2.3207687703365725 3.8512163918677915 56.66988223151974 0.6207892022159707 
2.3586147751048028 3.9265593779834798 57.48639162487069 0.5899123974297072 
2.3997773772655497 4.0130648759820815 57.35012989059035 0.5676912535128854 
2.4306437656996507 4.061121306753238 57.19027394218818 0.5534398458519673 
2.4514480443815034 4.085644221694264 57.00849528026385 0.5485359848218515 
2.4596071155890824 4.085477710887344 56.746217545735014 0.5486317355826048 
2.456380414338694 4.087806187147064 56.397097290783464 0.5492673144805614 
2.469727149278458 4.111159740459151 56.31349866393498 0.54289119555575 
2.487274562054092 4.150699670780245 56.63541212351515 0.5296892595143571 
2.5046219693044467 4.18954359203473 57.295572921882076 0.507567239872772 
2.400999212537589 3.990839398278346 56.6622885095424 0.507567239872772 
epoch: 10, train time every whole data:255.24s
epoch: 10, total time:3667.07s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.60s
test time on whole data:67.54s
2.3162666241431698 3.793506188707346 69.90573953496309 0.7365312916754004 
2.3880209764457705 3.94642853622464 71.24135007084148 0.6965618631395286 
2.457091651583711 4.147113010489495 71.29629438082279 0.6564731766069432 
2.5187595433648675 4.2626847576906455 72.53334080850773 0.6159644256924348 
2.5714046354378084 4.379291399041716 72.91043718124072 0.5890787546249299 
2.6200285886722128 4.467397181139097 73.30519137316716 0.576844131025522 
2.6449693343793705 4.504017129116677 73.68489634725015 0.5798991737822828 
2.6148334032824 4.457868815902855 72.93345311994612 0.5909764171296883 
2.6133583919035184 4.43439574317254 73.86528089818594 0.5939078639497983 
2.6123556413887337 4.4265157407772495 74.49066617458335 0.5918844145466479 
2.6408803731710426 4.453467769640794 75.88941710634273 0.597091321080636 
2.6610932547433213 4.492439878092231 76.28525781485114 0.5859959721060408 
2.5549218682096604 4.319547708917494 73.19520327261672 0.5859959721060408 
epoch: 11, train time every whole data:255.35s
epoch: 11, total time:4000.66s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.68s
test time on whole data:67.67s
1.8536876539787543 3.223519663634389 56.424938326436035 0.7415403262540976 
1.9112539645755398 3.3759092402986948 57.03674702261235 0.7166221102665609 
1.9986195446172996 3.5858441592463652 56.87264284420881 0.6843900113832101 
2.041474434202892 3.6638396904642274 56.57501941922764 0.6728557801608417 
2.093702130293119 3.7514507305983273 56.376912810890545 0.6600395988414034 
2.1139016879008463 3.7684349323664597 56.2772548392136 0.65472708956103 
2.1213093235838625 3.765264844555237 56.03835784003357 0.6526683302207199 
2.1202356916178196 3.740083543395757 56.33081368648806 0.6549842409361326 
2.1208654256622705 3.723189413225681 56.94475022263914 0.6515116961979749 
2.132445952185474 3.7416884735480145 57.404600315519616 0.6434558056591115 
2.1606394275880993 3.8017126707354354 58.00391167282386 0.6311823312641033 
2.2040041994910156 3.8892803389950354 58.658554858950275 0.6103359533830965 
2.072678286308083 3.6736864244602807 56.9120576180977 0.6103359533830965 
epoch: 12, train time every whole data:255.38s
epoch: 12, total time:4334.29s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.69s
test time on whole data:67.70s
2.8708174222044294 4.222417769454981 90.75080091637894 0.7272176383062697 
2.928607530572496 4.337828911578816 91.94515874966584 0.7104457943617112 
3.0735546867093513 4.579874033847189 95.33544323719984 0.6847141492371531 
3.159538027077913 4.696384442944625 97.57428193918759 0.6732267475500949 
3.2477682706376627 4.809396202439704 99.93109181340901 0.6631574769963208 
3.3129208903651506 4.863195222681134 102.28760715756076 0.6634999402524272 
3.3404378184906784 4.891928708371699 103.01178417315744 0.658093972724899 
3.365554102442981 4.920482521981496 104.18678796321383 0.6486957654914866 
3.3801828110705885 4.934582696633692 105.25581987109956 0.6388641667820655 
3.3994329816661777 4.961591643479968 106.29858757295654 0.624954540634946 
3.3963631818228004 4.980494070664025 106.43498119675957 0.6057851956803816 
3.383513150331786 5.005517434548052 105.42561650417124 0.5843400446432024 
3.238224239449335 4.77343292638795 100.70348929728996 0.5843400446432024 
epoch: 13, train time every whole data:255.39s
epoch: 13, total time:4668.30s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.57s
test time on whole data:67.51s
2.052656938591085 3.2013904568391394 68.08530095638498 0.7259752171917839 
2.0969861762120967 3.2837630698768607 69.17619334707194 0.7063978196955173 
2.122424698228992 3.3679741098613145 67.80231061223213 0.6867442947757558 
2.136538271514344 3.3827315714369885 67.45909859265022 0.6837171803605498 
2.1469398818632497 3.426821757404075 65.0505016683596 0.6789431557330237 
2.150511237316809 3.4866129809905457 62.09849159701122 0.6732021804394818 
2.164039898568587 3.579920800958266 58.91438204222451 0.6621191299956961 
2.194130113497288 3.6807908645274434 56.85417274805447 0.6569048219596878 
2.2137987363663103 3.725606179627264 55.940279862513755 0.65548075545897 
2.228954184929441 3.748751593058498 55.80509615077638 0.6447779199749082 
2.2462945845347964 3.784127556589863 56.005936385771484 0.6243372884722233 
2.275316672386513 3.8320278846676925 57.05201507755834 0.6028323698801169 
2.169049282834126 3.5475237137640776 61.686664342675215 0.6028323698801169 
epoch: 14, train time every whole data:255.66s
epoch: 14, total time:5002.39s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.81s
test time on whole data:67.92s
1.9543095669173414 3.5434647487623696 55.93484914226811 0.6930746065705461 
2.0495532202358757 3.733254750396666 57.93481411821033 0.6510563414113669 
2.143211798851245 3.946233201901873 58.61120341911674 0.6144761483362353 
2.2052746449342617 4.056473731129678 59.367177982238736 0.588108882886822 
2.260826112334395 4.1724850311252535 59.45708474043742 0.5729926604182983 
2.3096063136696108 4.250794457492751 59.94773505667427 0.5609891426045168 
2.3514731229855013 4.315240768820092 60.01400518166109 0.5537601699128163 
2.398603561562974 4.379739853844302 60.90276778722592 0.5443857054846031 
2.4342200206999802 4.434161138722706 61.90407161783067 0.5318284043153443 
2.471285112107261 4.486622276598598 62.87940834707694 0.5153216435818258 
2.523520985438533 4.558903409367083 64.37392846094264 0.49779502265094216 
2.560086165439781 4.607524303499826 65.21124823768452 0.484992051593371 
2.30516421876473 4.219081880452813 60.54499069127714 0.484992051593371 
epoch: 15, train time every whole data:255.44s
epoch: 15, total time:5336.35s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.74s
test time on whole data:67.83s
2.4117370765781296 4.216929374760074 60.815061022881046 0.5698343853588872 
2.4140094838927366 4.240890522129957 61.50843794486474 0.5434467619385569 
2.4307615009577326 4.292045463955135 62.630428830645116 0.5113887135861602 
2.4692745241845646 4.368786528375308 64.00929726642694 0.47701891490631687 
2.51876399030359 4.454643569946613 65.23414849333552 0.4404046214845599 
2.5690663744227162 4.5294624040413 66.19694711900735 0.4070088273564716 
2.61548632563624 4.590356824947651 66.86376704497869 0.37757390783857875 
2.6649172087593804 4.645959092391558 67.78609847189104 0.3553073416320969 
2.7178153577049575 4.7043696261274315 68.32125985749565 0.3380989610652129 
2.7728047110444556 4.760206276921977 69.16567230604583 0.3248682735764862 
2.824425558126044 4.805665893085834 70.04358824982043 0.3114876655574208 
2.8634077841179177 4.827641454955613 70.20133876016821 0.29870572535181966 
2.606039157977372 4.54131947155449 66.06485602276184 0.29870572535181966 
epoch: 16, train time every whole data:255.28s
epoch: 16, total time:5670.08s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.63s
test time on whole data:67.60s
2.891540510188345 4.616589811905865 64.04724732321225 0.21346421216074482 
2.8931122359765604 4.606256578775762 64.55447434078256 0.2148015942171779 
2.893527029677782 4.6049435309206395 64.73196084627776 0.2143088337183467 
2.89452500928149 4.607348567086398 64.74746565260936 0.21287283138950658 
2.897064121307096 4.612411187654414 64.77892979248273 0.20914602329426296 
2.8980275392210375 4.616560754217308 64.70177173830598 0.2074801252115346 
2.8980270677596507 4.620269691830352 64.56507131603797 0.20737061566164064 
2.8991035128365317 4.626773659846583 64.38996655175553 0.20536469911770427 
2.900307889324835 4.632307901288386 64.2716082981795 0.20334824837409773 
2.9030101884201702 4.640307772962961 64.19040588845968 0.19929895666725936 
2.9036781793575557 4.654666964594321 63.618007939416486 0.19720566984388743 
2.9077575907897915 4.66395582800746 63.58445984384632 0.19051310827927645 
2.898306739511737 4.625236073485278 64.34843888629919 0.19051310827927645 
epoch: 17, train time every whole data:254.97s
epoch: 17, total time:6003.27s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.58s
test time on whole data:67.52s
2.2334963476485794 3.898028277841528 69.14622421058624 0.5968666207962144 
2.2527488769777118 3.9471453046762126 69.7155646867935 0.575208473998625 
2.2771240113699545 4.048605054829769 68.29900268058002 0.5494439658376098 
2.337083000704025 4.1682034425510315 68.86937427301622 0.515242056561224 
2.404895802687144 4.295321520410904 69.41129352538701 0.4782906846632925 
2.478155138987604 4.420179572643678 68.97632178409263 0.44217346811609237 
2.5497617708825993 4.537103613524756 68.86104096407453 0.40525213960041556 
2.619904970440304 4.632189665179529 69.36602947877873 0.3729345050385158 
2.684295464385301 4.703872945586597 69.20025795900813 0.3469782878181502 
2.7495016121865206 4.770068467605878 68.42638735744988 0.3277238845359121 
2.8197215566808977 4.8364007331225425 68.27598310310887 0.3102830360676465 
2.868004175933078 4.868663873008137 68.31163171252437 0.29366913728398564 
2.52289106074031 4.439825464712326 68.90491926117589 0.29366913728398564 
epoch: 18, train time every whole data:255.21s
epoch: 18, total time:6336.82s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.67s
test time on whole data:67.69s
2.274064434773422 4.012599373056069 63.91083291263203 0.6181703523014204 
2.2931438999554765 4.079247591145678 64.95777603929665 0.5930350581093837 
2.3598548707198352 4.237906126010063 64.76173201945235 0.5596932020986711 
2.4141223466994153 4.3399447657998875 64.94390705930584 0.5273021555508898 
2.4768158037988024 4.434527100847892 65.28685093787817 0.49256307288814005 
2.5345664512745683 4.518190859557514 65.36152042449443 0.45686347855515413 
2.5938248201492464 4.591214340548376 65.84412414972641 0.4203415806780893 
2.6494966167264753 4.648748587790377 66.44227240138557 0.3906999463910801 
2.6995138264473173 4.696776861604041 66.70650966353939 0.3658563287086199 
2.7469336994946714 4.723668470402182 66.76069998776953 0.3453045650984614 
2.7963095209485895 4.758978857942182 66.8948878095056 0.3283884849358833 
2.8399269886729086 4.790466619898378 67.0686807412577 0.31260563300812694 
2.5565477733050606 4.493278148701132 65.74504041269705 0.31260563300812694 
epoch: 19, train time every whole data:255.22s
epoch: 19, total time:6670.43s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.67s
test time on whole data:67.73s
2.6464095462639596 4.44918642739768 77.28687211367902 0.6210949016501851 
2.6842330929487055 4.507462356361918 77.8028370169027 0.6014461896519276 
2.7145925427090734 4.603004156410634 77.20305859439699 0.5715886208178195 
2.7685048056710513 4.691113548969176 77.47664682028709 0.5426421582292665 
2.8295411866299274 4.779765376181058 78.06807421118566 0.5093027584751856 
2.865374921088329 4.84618884901342 77.9653938486434 0.47344951236203225 
2.9020292174868465 4.905305729863078 78.27789045679003 0.43621251146290546 
2.943370100159553 4.955471868814228 79.05029274648345 0.4044143666876655 
2.9814925448955703 5.000237626867821 79.57246022229553 0.3772494361801063 
2.998358069748307 5.021510075066314 79.23651123256849 0.35327940161333843 
3.0224697924022164 5.042234065142654 79.36243788883075 0.33440862755379713 
3.0523949306625875 5.067441453939466 79.68197915857928 0.31653567653796294 
2.8673975625555106 4.8268185392193494 78.41542239199548 0.31653567653796294 
epoch: 20, train time every whole data:255.34s
epoch: 20, total time:7004.26s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.65s
test time on whole data:67.72s
2.365087176891044 4.126437201537266 69.53577998079948 0.6438155634512484 
2.3964701369410113 4.237035560387239 69.74395932074422 0.6143272762594914 
2.4213209816339827 4.332127405229331 69.12197068276492 0.5782338306501478 
2.4561228393187657 4.41831350311711 68.65796411957997 0.5449404515772718 
2.5117441694178573 4.516259591695964 68.55154701761387 0.5133164604555267 
2.5555970980479783 4.5891305511900855 68.22840660001339 0.4814852187421594 
2.604309778549398 4.6545668660846164 68.26969140916358 0.4511548093977347 
2.653025380389854 4.708843200209198 68.57964717603438 0.4226035725651828 
2.6907930066994434 4.74650937866614 68.05911591659411 0.39494778992151786 
2.742507532052696 4.79008236035439 68.42201041219482 0.3711433260910156 
2.7920864662044638 4.82415303669637 68.59302808182078 0.3498222028575491 
2.8396161776015623 4.857048813334253 69.03265133080322 0.32592538239718033 
2.585723395312338 4.572575033947563 68.73295874702224 0.32592538239718033 
epoch: 21, train time every whole data:255.39s
epoch: 21, total time:7338.10s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.85s
test time on whole data:67.92s
2.102643158303574 3.721500178542786 64.86647892003263 0.6531478570247103 
2.1519694756031744 3.8535007147951195 66.0503841407607 0.6234639539412073 
2.2109766748312505 3.9965116208017855 65.5337261258117 0.5916812967331094 
2.270498540591626 4.106464230622001 65.4288652942899 0.562089446424388 
2.341170230554062 4.2272931272033425 65.18795998810134 0.5331326012027757 
2.402503451925924 4.313244925277935 64.6902865761949 0.5051538109292434 
2.4660133626538197 4.398724378548609 64.25599618139479 0.47575628415995913 
2.526162658202861 4.467310011270643 64.18833223973978 0.44940987049294334 
2.584918534405441 4.526729678717729 63.64105607812432 0.42629522106033735 
2.6505348781737545 4.596448554003923 63.26368400678314 0.4026711093252464 
2.7097816514395885 4.6452347608389095 63.009129524694195 0.3787227580174129 
2.7578865857661836 4.68340140956153 63.39697977682993 0.3546571211294316 
2.431254933537605 4.305471174898216 64.45935702654604 0.3546571211294316 
epoch: 22, train time every whole data:255.15s
epoch: 22, total time:7672.05s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.65s
test time on whole data:67.65s
2.223010046516057 3.9266572103557693 65.94310681112884 0.6527126845742638 
2.290546971004811 4.080042289485345 66.80016524038061 0.6260853291325174 
2.391091092086619 4.259568202667907 67.61347772108756 0.5962593577015007 
2.454927709210132 4.373020539838346 67.78914082799197 0.571254439157928 
2.5303600533826365 4.489794629626767 68.20047994870997 0.5409896082294167 
2.5875490608686316 4.577160409692509 68.28011013648056 0.5129471407092913 
2.645761639057171 4.653730127553033 68.68159195978758 0.48447423685205765 
2.7027024036825056 4.721997508308153 69.51537534508823 0.4530112079218282 
2.7577354195432826 4.785443529531092 70.32822226720945 0.4253997629493508 
2.8105880078067558 4.842575726183079 71.06173947724866 0.39914735068266244 
2.856560868788954 4.891104084781039 71.57594026566646 0.37614559791866087 
2.8964108064976477 4.928984672858664 72.21632403257307 0.34872694284067024 
2.595603673203767 4.5548923163184885 69.00057552531587 0.34872694284067024 
epoch: 23, train time every whole data:255.30s
epoch: 23, total time:8005.92s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.73s
test time on whole data:67.78s
2.682890525098624 4.674278279509806 65.22223682978817 0.5388474341613303 
2.611872578267895 4.610940567111742 64.68901222672321 0.545787103249581 
2.563562047357804 4.574052978818764 63.86678343546555 0.5396670922056971 
2.5459560447003513 4.56873176787703 62.83402700844353 0.5209952173460491 
2.539853395887074 4.5740152423845535 62.65854370367106 0.4969018108517531 
2.555956153607794 4.591840961340386 62.31084177731048 0.47097638093310956 
2.5839776148500717 4.622702150465191 62.76384926366069 0.4445076197973421 
2.6191046921695094 4.658684256192935 63.69644494534632 0.41964956094925304 
2.661881940064242 4.6992392270733765 64.55152314909793 0.3977380349498863 
2.706854626774788 4.730165876496471 65.33795572211449 0.37966620425765885 
2.7566497933733323 4.76181199896422 65.97951726028845 0.36158472864829194 
2.798412948920436 4.782866936496188 66.47147336033686 0.34147942381832513 
2.6355810300893268 4.654677237005387 64.19853405055738 0.34147942381832513 
epoch: 24, train time every whole data:255.42s
epoch: 24, total time:8339.94s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.83s
test time on whole data:67.92s
2.095352727032222 3.4735784363185522 66.08376968293813 0.6553476315065496 
2.142037524854587 3.5890208803831363 67.9713100962829 0.6257070162693423 
2.192619173858847 3.719626932406441 67.94742675894868 0.5929760562104082 
2.2462334734198772 3.843801445512989 67.5132505247611 0.5615023878278964 
2.304421738982378 3.9598160831284943 66.88664579097386 0.5302708922741448 
2.369278604253949 4.058425276715219 66.33951481532517 0.5004778850608953 
2.4293485150778933 4.152995954975534 65.84297527248832 0.46969568874177325 
2.4859883238015077 4.233156105164893 65.57591509421769 0.44136077607392077 
2.546055204351388 4.318797483648336 64.7459718168658 0.41353539884525575 
2.6093928416933685 4.397429337212552 63.93837961297787 0.3862835217967885 
2.6693601651770904 4.4662490849250664 63.367468691754304 0.35934405999830027 
2.7218037932333314 4.510824343176344 63.76872551458448 0.33156898525798567 
2.4009910071447034 4.073895667106687 65.83170406749834 0.33156898525798567 
epoch: 25, train time every whole data:255.12s
epoch: 25, total time:8673.75s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.68s
test time on whole data:67.68s
2.1810600253450905 3.5119906590870595 62.12487684167214 0.6524616113868011 
2.1983781902069848 3.616474061251035 62.30138172520188 0.6306104598460378 
2.239913245213972 3.7234591624654088 62.81191426700502 0.6084258866513179 
2.274699620002171 3.8005197958634325 63.83770450689891 0.5861825081753722 
2.311841523201604 3.849782778548772 65.3604946185969 0.5677686702116618 
2.3620508645675367 3.908916535134142 66.80691271459473 0.5464450815163138 
2.4095484815545913 3.9654949092929743 67.9395058561282 0.523190325753523 
2.4549566164131145 4.015867744587366 68.8086712073294 0.5011948371146653 
2.5070425072336304 4.065637152780795 69.79526381226503 0.47867383529540325 
2.5653786082465557 4.123994812813906 70.33853216713143 0.45287158468493355 
2.6206666261679716 4.1880084882478394 70.06208635413024 0.42478358883029865 
2.6751095322500027 4.249998487403075 70.27583217132356 0.39233859169751417 
2.400053820033602 3.9244161172031604 66.70545728128805 0.39233859169751417 
epoch: 26, train time every whole data:255.33s
epoch: 26, total time:9007.44s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.68s
test time on whole data:67.70s
2.93031282696598 4.869819153372601 58.617908125094 0.18844900886439095 
2.931048011594319 4.857179467108909 58.81004089785532 0.18276295471830026 
2.919369222163888 4.816947705816693 59.072573380401764 0.19232203015529328 
2.9083125774251917 4.772115546042085 59.586751648763595 0.205093073708986 
2.9012442269546113 4.742950409704489 59.95313434317736 0.2178788818504231 
2.891023630432075 4.700274508203084 60.58001999567772 0.2313837690041104 
2.8839004415693203 4.673958151295466 60.94523900377226 0.24164369534864724 
2.8806713770990746 4.662438328084733 61.06025826788659 0.2470145672402731 
2.8777831523840627 4.651015500507419 61.26582147842379 0.2502869225561488 
2.8765813798043167 4.647731606434022 61.31596359164592 0.2518986201832409 
2.8753265962259222 4.648903133861197 61.218304071553355 0.25392473463264437 
2.874621604666291 4.649877851708133 61.12748561840572 0.25554575850767486 
2.8958495872737546 4.725134888793115 60.29618713147576 0.25554575850767486 
epoch: 27, train time every whole data:255.05s
epoch: 27, total time:9340.95s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.65s
test time on whole data:67.61s
2.178190068153842 3.908499134781864 58.757743105053564 0.6096344216310922 
2.1893785779921426 3.942210231511588 61.353777629511185 0.5857929573268025 
2.2464225130161006 4.059450175517567 61.08555542804497 0.5616534970040371 
2.308692081110286 4.147293174344305 61.482037397498665 0.5335956090196875 
2.373871187561768 4.229845525588426 61.94652449271747 0.5017007886617236 
2.446807347933274 4.319142281225235 62.10528806276665 0.46809037248545277 
2.515951637368738 4.399772063052694 62.44969722454202 0.43387398929772997 
2.580134013940447 4.463559343891172 62.787742559764645 0.4026067769907373 
2.639450702577918 4.514196373609492 63.06393270983599 0.3750034564122173 
2.694606584577156 4.558357184133353 63.064107430595584 0.353962948356315 
2.742640742936482 4.599939834428038 62.93224733647639 0.3363904032391831 
2.779716247571898 4.6310083286684876 63.17630862770229 0.3197552325307171 
2.474655142061671 4.321330587665935 62.01714685201942 0.3197552325307171 
epoch: 28, train time every whole data:255.18s
epoch: 28, total time:9674.58s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.65s
test time on whole data:67.64s
2.0723424602078184 3.5553797147114614 58.5131034560072 0.652643503313879 
2.119803613829205 3.665721688531113 61.729072762481195 0.6191597904587745 
2.174867557248012 3.7701476755143313 61.562355075274766 0.5926196098355951 
2.211435100514708 3.824054532370194 64.10872595338687 0.5746590618946871 
2.2544238692514065 3.861132914364441 65.90414253735224 0.5587469120518265 
2.297106360030582 3.9154616566220586 65.92545306129458 0.5414438413684913 
2.339770092742872 3.969432734976438 65.77504065699287 0.5240929375940658 
2.3783232241102628 4.023397633072734 65.60292932600564 0.5061960946888822 
2.413636949363475 4.077369092134246 64.84412090736741 0.4902426308604177 
2.4509229539080213 4.13090242387063 63.96528830133772 0.4730144906357383 
2.491747210738027 4.183849290681649 63.2893338798652 0.45557642899080575 
2.5377050495657714 4.241349922934805 63.396378723517486 0.4331464397634312 
2.311840370125847 3.939986217175643 63.718085163099545 0.4331464397634312 
epoch: 29, train time every whole data:255.14s
epoch: 29, total time:10008.15s
fine tune the model ... 
epoch: 30, train time every whole data:526.74s
epoch: 30, total time:10534.91s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.74s
test time on whole data:67.85s
1.980717758104826 3.5252144350922094 60.85001657639616 0.6722788180996823 
2.026977495056444 3.6616191969695535 63.16138661100907 0.644478121774147 
2.091402739122155 3.8115884694560345 61.81554594125727 0.6204020947597912 
2.099552367941698 3.8623917425738665 62.54968222673626 0.6104648986455424 
2.12162620378738 3.9165127768825623 62.45932858784757 0.5993513152420734 
2.1424077209874635 3.964185576166695 62.16941128548093 0.5924321542554947 
2.169143025216868 4.012205923330126 61.83499683343874 0.5861583390774653 
2.190397962996559 4.043921642502782 61.70692337811771 0.5801548035697871 
2.224333303246292 4.099331167020593 61.93543158484829 0.5734639011380034 
2.258857254542233 4.150712246976516 61.9126337763096 0.5654631714872821 
2.302385657467037 4.215564196078563 62.357714475673184 0.5527548357072601 
2.349194422242542 4.282213470195613 62.916177251072924 0.5326165602182318 
2.1630829925592914 3.9678611859955804 62.139105856816414 0.5326165602182318 
epoch: 31, train time every whole data:526.34s
epoch: 31, total time:11139.76s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.74s
test time on whole data:67.79s
1.9815760127973876 3.592522422586485 60.3724006977984 0.6736891830217875 
2.0269603647682817 3.71615009777408 62.708990760914496 0.6460940206967474 
2.0938388906780276 3.855469196891456 62.17364507447722 0.6207670175869225 
2.103957772127042 3.9002086500487776 63.01417077803578 0.6094299567982783 
2.1247989954889 3.945567359525467 62.9801386030146 0.5987994108998792 
2.146136975021678 3.988652895173534 62.71384185853353 0.5906811487053367 
2.168298986792476 4.02811091494244 62.451899845864624 0.5827036282146796 
2.1839238827337644 4.04795353734586 62.16526174533239 0.5767042810912512 
2.2104355937294837 4.087058125673121 62.191157173573075 0.5724796954831622 
2.2362496101432257 4.121892411710051 61.95777187970083 0.5678314899056207 
2.2709742223807567 4.173320262178013 62.20909807264402 0.5576706526164048 
2.319834499626642 4.242902451341024 62.8798578441804 0.5356417958684361 
2.1555821505239723 3.979044929764659 62.31819585137048 0.5356417958684361 
epoch: 32, train time every whole data:527.11s
epoch: 32, total time:11745.61s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.75s
test time on whole data:67.79s
1.9715613195560873 3.5665388111137624 60.591251618320754 0.6778681083593872 
2.019013595922806 3.6964292807241015 62.63409142925636 0.6497097723064498 
2.084604931675164 3.83564488935796 62.18465328522584 0.624953655811576 
2.0849098843163145 3.871798859709463 62.56694204595122 0.6162986380751988 
2.0980284575568957 3.8984356305384 62.801465331354066 0.6098489235426519 
2.1181941198510605 3.9297627711578977 62.77453192432717 0.6043523803950545 
2.1353350025602573 3.959621542189677 62.647750649045776 0.5992429624063033 
2.1481607309366977 3.973541035113725 62.61570993288168 0.5949576951756317 
2.173282699784264 4.0093382294678035 62.80251972194157 0.5919820207030179 
2.1978180162125223 4.047463558160884 62.756795171779025 0.5875479206076071 
2.235119518310009 4.10496276830269 63.15181788935597 0.5761713605935876 
2.285160673224589 4.181160257753818 63.6982792391491 0.5534954605511682 
2.1292657458255557 3.926245325658891 62.60217974252391 0.5534954605511682 
epoch: 33, train time every whole data:526.82s
epoch: 33, total time:12350.92s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.75s
test time on whole data:67.81s
1.9662193626102416 3.5575080482052335 61.27933719729561 0.6831775464933622 
2.022688213514164 3.6990620608055433 62.932020990677074 0.6535625566224447 
2.0864452492955716 3.8318088082061963 62.75707106490385 0.6288218896754317 
2.0853178526399923 3.8625435599000504 62.891438257485696 0.6202306602845002 
2.0947239420723527 3.879229634157249 63.038807684955664 0.6150911755755669 
2.107761163428337 3.905805594109431 62.74682821505459 0.6104889292903649 
2.125529162037497 3.936563042326251 62.72114040791674 0.6055910223140047 
2.139640464980865 3.952594482371501 62.79415323405241 0.6015993321944856 
2.1713064249769918 3.9945280562831575 63.11707691562799 0.5982436202268951 
2.196815385560195 4.035685394856624 63.20494870892864 0.5948280714751231 
2.234365911578937 4.093744581709832 63.72903314933199 0.583778866928635 
2.288205212578798 4.176862088269663 64.4784829062897 0.558967805112736 
2.126584862106162 3.913780454610415 62.97422163919708 0.558967805112736 
epoch: 34, train time every whole data:526.58s
epoch: 34, total time:12956.10s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.79s
test time on whole data:67.85s
1.9273821839901308 3.4339312452520896 61.64695645671012 0.685957262118328 
1.9816211160499426 3.5863126773839813 63.65240208009176 0.6559530559983251 
2.0382558234476793 3.7120497420935537 63.356533414025904 0.632447719973469 
2.0400834092693847 3.7600935114677947 63.724369930776234 0.6237426962957264 
2.0588109864936697 3.79065070577199 64.2243281272448 0.617342237213493 
2.079723553617973 3.827532636247797 64.1865260470642 0.6111549067773433 
2.1012056536877615 3.8608296578908505 64.23539708194068 0.605190813283964 
2.1186670364683406 3.881950721295 64.3654260618735 0.600199095801031 
2.1495710679730844 3.930996109292002 64.40110847290534 0.5943562855434636 
2.178336581457495 3.9838004599312193 64.2677934670323 0.58776471843618 
2.2201459639873358 4.056050877006235 64.62025069638041 0.5738698794707796 
2.279243977355017 4.147253236427276 65.18682449257842 0.5481723767237169 
2.097753946149818 3.8355831745218683 63.98903518571832 0.5481723767237169 
epoch: 35, train time every whole data:526.87s
epoch: 35, total time:13561.50s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.82s
test time on whole data:67.82s
1.9645014305746271 3.5380350493416053 61.61280320055476 0.6911109730453707 
2.0214560952209646 3.6894575854551084 63.09522471260161 0.6607913207388588 
2.081321244076338 3.818509461606188 63.02166013590903 0.6361014482261452 
2.073075530823409 3.84033543155402 62.91094598104887 0.6299494726299822 
2.0885943691291446 3.8670307192969364 62.94960019609833 0.6245734695578562 
2.1028387552516623 3.8915279240268705 62.8263955596598 0.6212741454581259 
2.1183744253449674 3.9141928864239204 62.83854222374442 0.6194331027674083 
2.127267197723722 3.919383565450979 62.845001121993626 0.6201354145222148 
2.155511728162301 3.9547360066718817 63.07608869857248 0.620059982976267 
2.18318379988477 3.9989682374262636 63.285424596610994 0.616623805614351 
2.2235138287201877 4.063351846621281 64.07007207129358 0.6032480604297981 
2.279247756472035 4.158832982163622 64.86671898423721 0.5751459016571769 
2.1182405134486775 3.8909914495170415 63.116564716493386 0.5751459016571769 
epoch: 36, train time every whole data:526.71s
epoch: 36, total time:14167.01s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.75s
test time on whole data:67.76s
1.9163964061208425 3.389969828968511 61.572437340317286 0.6894515460230732 
1.9708440343801463 3.540788031022381 63.52503195056103 0.6588305122249835 
2.0244380772044615 3.6586645613157893 62.71231581905424 0.636779128032611 
2.0216141303469914 3.694642925437378 63.11034787700351 0.6288323414180461 
2.0262923554435726 3.7029075067216075 63.62481267552156 0.6247951130540909 
2.0365479581617705 3.7216058268527776 63.32505836728486 0.6215419319305768 
2.0456728039003376 3.735984317669127 63.32308330743884 0.619968610517778 
2.0549073909773004 3.738096206880664 63.5059899397304 0.6193357258690412 
2.070679662097157 3.7680476079456686 63.32397653184187 0.6171877796424564 
2.0931124927476934 3.811008012205875 63.2168058136538 0.6123153974398179 
2.1296578197041085 3.8786479770224696 63.60272167900967 0.598728363357227 
2.1890417286212602 3.9782276806624424 64.17446525517263 0.5726526870447335 
2.0482670716421367 3.721002043602385 63.25144394135953 0.5726526870447335 
epoch: 37, train time every whole data:526.69s
epoch: 37, total time:14772.29s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.85s
test time on whole data:67.97s
1.921701911873672 3.4563001126138495 60.597398168668114 0.6908036639560574 
1.9771803487408905 3.6059846074469366 62.544660584021486 0.6598878436983712 
2.0421032490186573 3.748271595816335 61.70517239258617 0.6374898373363204 
2.041720392226286 3.791186738256178 61.345814700552715 0.6311538265391081 
2.0502480281588755 3.8073006244679837 61.34650250529059 0.6275217963359198 
2.062867100388876 3.822830317427144 61.019478009653014 0.6259452923195574 
2.0745291378557504 3.8393326585385674 61.01992381944912 0.6252829499361564 
2.081017031036051 3.83663609793345 61.12725713047453 0.6272494226076147 
2.1016535115971986 3.863169820804707 61.24271093193839 0.6281216583136414 
2.1273544206495085 3.902904876557709 61.33325719748402 0.626241236847198 
2.1697753080770017 3.969652956449733 62.019803529140226 0.6130811857373261 
2.2330486859394503 4.070034853165309 62.91347900702976 0.5841958265572659 
2.073599927130185 3.812497755520626 61.51795691734089 0.5841958265572659 
epoch: 38, train time every whole data:526.10s
epoch: 38, total time:15377.11s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.82s
test time on whole data:67.87s
1.9050348031917321 3.3917718645118016 61.75226870070097 0.69258367285207 
1.9567441167441153 3.5384140212696265 63.347789749669246 0.6632536138820259 
2.007791172422114 3.6509646455575564 62.958038068824465 0.6430521500695797 
2.0064270170248513 3.6941468968524087 63.149076802908986 0.6357396102463628 
2.0154576644928506 3.7104465288766675 63.56436597699422 0.6317758082269845 
2.0259504243187783 3.7277743869326945 63.0969929383038 0.6290550209731833 
2.038065353699117 3.7442796592353558 63.20345445918273 0.6262971433651815 
2.0488914105697047 3.746836445527443 63.35907404495138 0.6251854282563203 
2.0676882015614813 3.7780817004206977 63.139812131027405 0.6222800254115198 
2.0874637033275016 3.8139628159604304 62.8385453072548 0.6185000201582274 
2.123087110845106 3.8743799347608054 63.15571039866017 0.6060583344590715 
2.1891594715977885 3.9794435146288096 64.02827870890957 0.5777184581556799 
2.0393133708162616 3.723684190496563 63.132798914177954 0.5777184581556799 
epoch: 39, train time every whole data:525.61s
epoch: 39, total time:15981.41s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.65s
test time on whole data:67.70s
1.916799015614337 3.452697265215251 61.03483735127063 0.6945956246243319 
1.980691839583218 3.6192336439537622 62.29268024936522 0.6648641150401787 
2.0476674794960057 3.751972273644946 62.251558327834914 0.6428111536562482 
2.060345925516493 3.810402428712885 62.359582338670606 0.6349778790857069 
2.080038558456958 3.833362327866797 63.19882723402842 0.6296539672000772 
2.0988887267400114 3.8616739665629685 63.15430302452548 0.626021884281606 
2.11429822219447 3.8812317935097584 63.2018486051839 0.6244848376384047 
2.1184723029720285 3.875709201207822 62.99028623272872 0.628014807208751 
2.138912703063605 3.903180561320334 62.92293664211049 0.6301487491774383 
2.160044499664257 3.9383709320585947 62.871150403670185 0.6295127213452532 
2.1950566248618775 3.9948853312361496 63.502429813514595 0.6197614223957142 
2.254824805902672 4.095760318782157 64.34267895663866 0.5934212110925184 
2.097170058672161 3.838279272507181 62.843630998543155 0.5934212110925184 
epoch: 40, train time every whole data:525.69s
epoch: 40, total time:16585.42s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.77s
test time on whole data:67.75s
1.9307549220111575 3.4908259292764034 59.447663499848545 0.6987307263750927 
1.9866980450433636 3.6476632838641625 60.999297663878494 0.6691561425389556 
2.0452660775223657 3.7669768707612348 61.30672848669923 0.6452368667381987 
2.04591899001146 3.8088496315973748 61.158017835908986 0.6378820175636248 
2.0619900202216312 3.831474870721311 61.72651825891295 0.6328451906473548 
2.084081302549956 3.8646806229543826 61.65604489240567 0.6299525992184913 
2.1058579014969014 3.8921346679188455 61.942220543244986 0.6284963934763895 
2.1178928737301557 3.8979450652366 61.97874395065488 0.6309654771312286 
2.145738956561578 3.9378213059800644 62.07444792611042 0.6315387280672049 
2.1743149860734565 3.984590845016654 62.22097766692761 0.6282271121760831 
2.212170610955606 4.044232731765328 62.864438006724974 0.6153912651569948 
2.2709248595377876 4.1419211533072575 63.51025968236343 0.5882536133081122 
2.0984674621429518 3.862664639835993 61.74049606037752 0.5882536133081122 
epoch: 41, train time every whole data:525.75s
epoch: 41, total time:17189.58s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.75s
test time on whole data:67.79s
1.903047697769123 3.4258298943551053 60.297977447152185 0.7005696668651935 
1.9623703876857068 3.5877489128280904 61.81549465401449 0.6709982643808088 
2.024160425129923 3.7146367142950916 61.990637504954606 0.6468175007163736 
2.0343537527684656 3.770555322636902 62.18752276908026 0.6380166651043611 
2.0611825147334133 3.8089955727433793 63.00284641157252 0.6308322389574126 
2.083845634152137 3.84021192149155 62.75337595537794 0.6272683022341917 
2.0987115702291153 3.861535672338771 62.716266425443536 0.6265719874932647 
2.1035090068328595 3.8575073964233613 62.400953884393 0.630856546762854 
2.123288865739302 3.8879794146518094 62.18535529662621 0.6334288844275715 
2.146930665366795 3.928182755788954 62.17399740200635 0.6314211784557667 
2.1822367261022864 3.983103015632197 62.93748908677771 0.6208423607610853 
2.241663156387529 4.083330567178403 63.6546012719114 0.5955629951625966 
2.080441700241388 3.816161508088402 62.343075349569276 0.5955629951625966 
epoch: 42, train time every whole data:525.69s
epoch: 42, total time:17794.11s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.67s
test time on whole data:67.66s
1.9102469789660756 3.4209624077622887 61.47411859455635 0.6996402556184635 
1.9653911983751293 3.567767166931795 62.4912714227457 0.6735888089450018 
2.019770884527337 3.687223151649249 62.42707869527362 0.651549881068413 
2.0283698930843244 3.73783642866776 62.38688623163146 0.6430962838301066 
2.0474991830013516 3.7632225979194045 62.90522215595558 0.6390473289926137 
2.061123988494277 3.7830694480203912 62.74871573371947 0.6386996333185975 
2.075764502677712 3.7932101826739273 63.029364082059544 0.6406242355152232 
2.081365752418836 3.7882621978831845 63.163745411181125 0.6440133953144425 
2.10731795598673 3.825882396382497 63.29778272016381 0.6445235468683718 
2.138073926077801 3.877569382068114 63.57935469306846 0.6404157608484078 
2.180869670240297 3.947925110882521 64.50713403580531 0.6269490930794155 
2.2471487418175453 4.068290784375026 65.29680448857908 0.596963508444628 
2.0719118896389515 3.7751582510014363 63.10900206610963 0.596963508444628 
epoch: 43, train time every whole data:525.58s
epoch: 43, total time:18398.00s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.78s
test time on whole data:67.86s
1.8972170281266527 3.396465900779374 59.84417329568982 0.7014641570745365 
1.9376753178835802 3.5219800619967203 61.13140486646332 0.6765798016219039 
1.9852033633531205 3.6158976442631965 61.78178706230405 0.6546020344539759 
1.9820028453495886 3.6423213812665964 62.526233426890144 0.6482038290272494 
1.993502348526248 3.6556351178068156 63.508363510965324 0.6423905895436968 
2.0065765079291804 3.6690464108893885 63.4304131962306 0.6389925883034404 
2.0206716911631326 3.6864374874778636 63.88432663232927 0.635182525910515 
2.0327458061339065 3.6923279012944827 64.18320501254277 0.6330481994387384 
2.053316877325376 3.730549594554006 63.86505130854345 0.6290100103679795 
2.0744910951267395 3.773675953481418 63.47004013754931 0.6228514070371541 
2.1074056990481025 3.8318095380681028 63.80323275091742 0.6104524396077957 
2.174902186689366 3.946478759984671 64.64262664782719 0.5811992860486036 
2.022142563887916 3.682676910984643 63.00598201359079 0.5811992860486036 
epoch: 44, train time every whole data:525.59s
epoch: 44, total time:19002.19s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.66s
test time on whole data:67.62s
1.8887196129004338 3.357324166371241 61.480316694929904 0.7006769321122603 
1.9291450188834043 3.481698432930462 63.000815985840184 0.6761541521262068 
1.97818771640983 3.5695537299823847 63.46036417934741 0.655443342074023 
1.9786918855256268 3.6020715469183435 64.17353463708763 0.6498980250281904 
1.9997251166094627 3.6371958790849623 65.0305977145693 0.6438716563570913 
2.016422324858251 3.66815165977884 64.65553717385558 0.6401910416071231 
2.0387445433683515 3.697346886914275 65.0426446347505 0.6365934015800596 
2.0547960380373436 3.7130143548100394 65.21840491958578 0.6347417535780924 
2.078944014581364 3.762740589036296 64.7575805971192 0.6312200094255904 
2.1073402786802147 3.822002789442619 64.32922365431568 0.6253417531097112 
2.1451625924944167 3.891882991765948 64.66732167545881 0.6137461593366775 
2.2114074840052496 4.011004622919999 65.17680650349054 0.5858122039227187 
2.035607218862829 3.6884131211703886 64.24948325077098 0.5858122039227187 
epoch: 45, train time every whole data:525.41s
epoch: 45, total time:19606.01s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.83s
test time on whole data:67.84s
1.9186357658869986 3.428315628871588 57.03000178523189 0.7065954992239304 
1.9545261253802373 3.551522251217761 58.587900277843595 0.6829488182235997 
1.9958743159243215 3.6395300763232363 59.08699902288881 0.661885359603468 
1.99351980454998 3.6664543300168804 60.29665624161973 0.6519977855369212 
2.0042885614438424 3.680099854272722 61.09612041158212 0.6467395920107302 
2.0166708372480104 3.6881719168527787 60.978922425600835 0.6454760943185793 
2.0252936874775305 3.6931511145125864 61.244282307572995 0.6455519216877277 
2.0295544225768674 3.68096550754067 61.39402714047798 0.6477807140189626 
2.0421893058354477 3.7083034018125787 61.04714860445111 0.6473568867227266 
2.060654260666509 3.7458017393713083 60.77577336230314 0.6424550995521804 
2.0887232650100653 3.7986549410518187 61.23710596930467 0.6292364047257535 
2.151275811171248 3.9110076004024297 61.874045977935744 0.6007027777814301 
2.0234338469309217 3.684411404064262 60.387488718604644 0.6007027777814301 
epoch: 46, train time every whole data:525.28s
epoch: 46, total time:20209.79s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.73s
test time on whole data:67.68s
1.9090379320497493 3.447500548284343 59.096681089448296 0.705651180535942 
1.9474829533421212 3.5652186353995274 60.590513680996885 0.6814925508672337 
1.9899771114546096 3.6414117145372216 61.16486490988567 0.6625973397104269 
1.9872293445660423 3.6629754609089673 61.80770501147976 0.6537798846407716 
2.0015538105345554 3.683061504257556 62.23989891406427 0.6498152985543222 
2.016232219877165 3.7025416861303433 61.7333825157607 0.6495210274563754 
2.0300669750437317 3.719586047112285 61.72570431547557 0.6499063852192968 
2.0369648640036937 3.7149207026315154 61.50811749305374 0.6540428879827278 
2.0576224061608137 3.7521529030090766 61.150060535094 0.6552322608444604 
2.0858515653413323 3.8051313140676064 61.053578022209045 0.6502064194841746 
2.1200774303922163 3.8623884694367794 61.660408302506696 0.6387405948476058 
2.183368641633629 3.9755296322387275 62.37973237235311 0.6116105979151112 
2.0304554378666384 3.7133047039385305 61.34258215645103 0.6116105979151112 
epoch: 47, train time every whole data:525.60s
epoch: 47, total time:20813.80s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.75s
test time on whole data:67.78s
1.882277008706083 3.3436862349766416 60.13456995667703 0.7060318846211573 
1.9212220072280615 3.4704105328426436 61.504146274323865 0.6832115605852369 
1.9647007863682118 3.5566084899699755 61.98593659938487 0.6653866879697291 
1.9745041459569086 3.600400573514454 62.72840148327391 0.6557829431570875 
2.0026207089808192 3.6481883191865063 63.577591111535 0.6453112185523514 
2.0288251668678687 3.6844744678005434 63.33303861525237 0.6381730836262732 
2.052140125475469 3.7216872279668167 63.51575200453593 0.6314835524855142 
2.0695221027573125 3.742599156645493 63.35975976964095 0.6293395525579442 
2.0922439327174356 3.793971363485092 62.720818913868605 0.6262477768879737 
2.1202336343203094 3.851488357873825 62.393081366074476 0.6192549076975394 
2.153126897120316 3.9137085936043943 62.708549893690545 0.6075471664798672 
2.216421388827708 4.023091317032017 63.2447834183981 0.5812953112434706 
2.039819825443875 3.700332524584774 62.60057735324545 0.5812953112434706 
epoch: 48, train time every whole data:525.58s
epoch: 48, total time:21417.79s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.67s
test time on whole data:67.64s
1.9445521413474565 3.5053025139823544 58.95385406120957 0.7073912684630683 
1.9933227570982917 3.643407978855896 59.78209701979262 0.6855515900969529 
2.03885938196514 3.742629343355342 60.164515425273976 0.6679391653212444 
2.042880889336002 3.772677877059633 60.445343028746414 0.6621214471814538 
2.0593341995582337 3.7877859927217274 61.108330082449456 0.6590670779556225 
2.0683590977928112 3.797005931206974 61.09492439378178 0.658708506125273 
2.0804146798588334 3.8029326407191286 61.37968600352903 0.6616932725146982 
2.086197262425774 3.7929056964202115 61.57271129272408 0.6669628421535806 
2.1100832375502656 3.823817784061161 61.967262590073766 0.6700434797213841 
2.134983387898565 3.86266917391843 62.408952190782884 0.6667144408877846 
2.164675205147781 3.907594438294341 63.24740074544678 0.656013626415236 
2.2229387921311314 4.018705630851191 64.07639260395584 0.6297677184827346 
2.0788834193425236 3.790075110021472 61.35019679996876 0.6297677184827346 
epoch: 49, train time every whole data:525.59s
epoch: 49, total time:22021.70s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.69s
test time on whole data:67.70s
1.9344869944196903 3.4812611732057333 59.70615584874469 0.7095890902208444 
1.9803083669573422 3.6133172613087905 60.485357419396514 0.6884284028868699 
2.032647621127378 3.7263405047341935 60.72761867861822 0.6700865954367485 
2.035850818325189 3.7505046438291094 61.08804615004362 0.6638412667322613 
2.0469834864060616 3.7607636984888138 61.7452461073612 0.660913807939044 
2.057806585902437 3.773168121232229 61.68709313114377 0.661528042404302 
2.0753324622110063 3.7849026038059614 61.95985987826109 0.6645650452144104 
2.082558378176054 3.780577610139679 62.056785169260664 0.6699426722603445 
2.10835080295198 3.8185898474615807 62.42816385702452 0.6714255600998229 
2.133124036691728 3.861274722641278 62.86904628584008 0.6654583439065637 
2.162903505410201 3.910130954465641 63.71670817264802 0.651987944798183 
2.219724868044612 4.021891482855361 64.4889064919234 0.6244434149226061 
2.0725064938853066 3.775814942597043 61.913317648763254 0.6244434149226061 
epoch: 50, train time every whole data:525.69s
epoch: 50, total time:22625.61s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.76s
test time on whole data:67.80s
1.871814245482286 3.3397447086054215 59.76331513850808 0.7118795217270062 
1.9085835778312314 3.4593661972134164 61.068844687466886 0.6881987936276907 
1.9477663046178364 3.53227237115188 61.3287963167304 0.6720833711579457 
1.9508110470483524 3.5625801802997037 61.92899629435939 0.665541375590672 
1.9636764362166148 3.5887421784783493 62.38518530994248 0.6594680764130385 
1.9720293318196422 3.598536359622482 62.03725100397849 0.6579605101648296 
1.9815971882448842 3.607042517303582 62.24172789387799 0.6573253222983524 
1.9890154731243495 3.603919959118352 62.36190452039811 0.6589586364005358 
2.006437374265155 3.6383768530714007 62.000008773785545 0.6578984736898789 
2.029339644771247 3.6808330191152168 61.68032593363727 0.6548002268634997 
2.059144406287975 3.733474331922702 61.8357566284667 0.6457342727130259 
2.120486443547887 3.8511773710736388 62.29000295514398 0.6204893127650327 
1.9833917894381217 3.6017852005267232 61.743541630722376 0.6204893127650327 
epoch: 51, train time every whole data:525.56s
epoch: 51, total time:23229.68s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.79s
test time on whole data:67.80s
1.8691451622685862 3.323072040699503 60.83100413956216 0.7108196159473202 
1.9086754305029199 3.451998000065423 61.629748665773235 0.6881390381508363 
1.9513551701352532 3.540954765899362 61.62686077614784 0.6703752106785633 
1.9638599638657732 3.594442062907085 61.88892327278239 0.662154604828728 
1.9905463033257318 3.643258727438184 62.06959534351396 0.6572455854033781 
2.0044455865467232 3.6615127416767455 61.58631727506495 0.6588568564582474 
2.0177519941509123 3.676604238526651 61.68071977541105 0.6616166496585639 
2.030242186766561 3.6835712664011417 61.790809249465205 0.6640576786262732 
2.057604527187489 3.7329591246164653 61.88906806548715 0.6625677037254396 
2.0963097698853486 3.800222680455866 62.28943063213927 0.6558451536207834 
2.1419590522864214 3.8779936989822077 63.10889800285891 0.6417489967591883 
2.2163919112388752 4.018198287992755 63.93833451215514 0.6097877502792467 
2.0206905881800497 3.6713420224522078 62.02750324702498 0.6097877502792467 
epoch: 52, train time every whole data:525.68s
epoch: 52, total time:23833.89s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.62s
test time on whole data:67.58s
1.8625969534412559 3.2913596446899995 59.77586023521791 0.7158796280888093 
1.8947959897252953 3.402833557584129 61.10859935611466 0.6943755103982636 
1.9299740803718035 3.4749243320452687 61.650523019227656 0.678523208309972 
1.9474715856100833 3.5347721575057367 62.45876573795135 0.6683193583081598 
1.9745780147533154 3.5851569899342954 62.832924519363196 0.6594950870196665 
1.9969077678277556 3.6153860343676785 62.34017258243645 0.654690007940372 
2.0159651160218885 3.646452779896631 62.381166030623625 0.6504463121849152 
2.0318189846231114 3.661416764145302 62.385060121417204 0.6492158387383731 
2.0522568474647014 3.704980737878041 61.84344444644729 0.6460190139039601 
2.076359950730311 3.756233928148223 61.562530983254746 0.6394046937782627 
2.103208267544795 3.8103579022769662 61.59927546179799 0.6292568256388724 
2.1658666870576284 3.9284443831856213 62.00432928689651 0.6018079379165605 
2.004316687097662 3.6216556236206237 61.828577853850284 0.6018079379165605 
epoch: 53, train time every whole data:525.57s
epoch: 53, total time:24438.48s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.69s
test time on whole data:67.67s
1.8884244241664807 3.3897838171712946 58.15705942550841 0.7143248117553389 
1.9152260617105556 3.4827716478742023 59.37208746586033 0.6943410415990662 
1.9410948547246378 3.5251477183911266 60.429934306672685 0.6789765684375016 
1.9394156531742996 3.53390383723142 62.1976725230874 0.6715654229148964 
1.9515881249446068 3.5493358124791037 63.30142670447786 0.6647527514043974 
1.957582384686119 3.548494275240442 62.89685976123815 0.663761411974463 
1.9653504135237032 3.5514491576573217 63.084345760802144 0.6631589744916652 
1.9687205077296213 3.5379906036841526 63.24402533495012 0.6654601928865479 
1.9777570567845058 3.5531530420146806 62.81196573504536 0.6655483558543447 
1.9944394543187782 3.5814198679767184 62.74009000081401 0.6609307528177542 
2.0204727485934715 3.6269528177261887 63.136149547016075 0.6491332512098239 
2.084500107640223 3.753563008523676 63.79359487739396 0.6201101237543486 
1.9670476493330835 3.55376241022219 62.09719169078758 0.6201101237543486 
epoch: 54, train time every whole data:525.70s
epoch: 54, total time:25042.32s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.74s
test time on whole data:67.85s
1.8742883749584711 3.359101872555474 59.56922551288648 0.71559121825701 
1.9122549814950853 3.481387034309201 60.54358725304664 0.6933503902237917 
1.9543454917519398 3.561859376325685 61.351874628661164 0.6767500660393465 
1.96252866787765 3.5928235025577915 62.7302844409692 0.6676518511356013 
1.9829574399182484 3.6189909512089318 63.50790584326849 0.661874436149329 
1.9945555390688103 3.6282927203356277 62.990232098056595 0.6606800319294144 
2.007516345435221 3.644046003327895 63.03403340178642 0.6593278764883327 
2.0146576650630506 3.64154744450728 62.97443904965109 0.66128579882991 
2.0315734727483776 3.674218873184761 62.6820722541475 0.6596947495209982 
2.055466009435288 3.720888766093028 62.67888833363179 0.6537269487223086 
2.085233331711608 3.7784462405890884 63.05616872676606 0.6416917360770203 
2.1492880535717345 3.9080564103370814 63.54781762087648 0.6126033282408901 
2.002055447752957 3.6365602460928637 62.38893533685275 0.6126033282408901 
epoch: 55, train time every whole data:525.61s
epoch: 55, total time:25646.32s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.76s
test time on whole data:67.76s
1.8661824229545005 3.2961685774674017 59.440792753497405 0.7165198741744015 
1.8969866204552708 3.409669250548327 60.286543443288444 0.6965000382509278 
1.9364677318223709 3.4993015314715823 60.278442725277955 0.6815510229282458 
1.9494545329339092 3.5437902551817926 60.99464920408981 0.6734390998382906 
1.9761374997660694 3.58590600654329 61.479487725230676 0.6650880360891791 
1.9936829147467479 3.598059073953829 61.166358612212456 0.6632054157447121 
2.0056898590825676 3.6145831016602727 61.21675453292662 0.6618320335692671 
2.0178917035596178 3.6191096057612353 61.26739742475223 0.6630931930846063 
2.0365863780589506 3.6557602698091993 60.99958549200415 0.661921181141292 
2.0627456985936456 3.7058827381862276 61.022632599784174 0.6563477330567891 
2.089494554945401 3.7581358689338678 61.30686158138084 0.6470116774590947 
2.1517129963207102 3.883591957478448 61.58933630772647 0.6201514663785399 
1.9985860761033134 3.600533181765187 60.920766086828436 0.6201514663785399 
epoch: 56, train time every whole data:525.59s
epoch: 56, total time:26250.29s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.64s
test time on whole data:67.59s
1.8492287868609032 3.273364504890251 60.17275255022605 0.718144450343109 
1.880214472809895 3.375396338886264 61.210360589965596 0.6982011282063836 
1.9151309793357338 3.4526207633946355 61.210350717642505 0.685005885613234 
1.9251967344016192 3.497308810736678 62.06445490905094 0.6771686028583669 
1.9469538969702664 3.5387543599147278 62.566296378991595 0.6690533848310697 
1.9585650800461216 3.548369810989087 62.237750737816846 0.6675783797598076 
1.9670295579381996 3.556553603981753 62.35412182261373 0.667823847928202 
1.974867155127582 3.550960075980009 62.30424136376127 0.6715868686955339 
1.9915668014703052 3.5839967771772545 61.94795310787664 0.6707672397723247 
2.0156165572322373 3.633841484096597 61.90376941368696 0.6652504762647956 
2.0455856766371676 3.696061444766802 62.12215730095193 0.6543888013275577 
2.111374024163222 3.8302181878732453 62.47342274764297 0.6265832329882572 
1.9651108102494377 3.547456304817123 61.880666279541806 0.6265832329882572 
epoch: 57, train time every whole data:525.73s
epoch: 57, total time:26854.25s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.83s
test time on whole data:67.94s
1.8531827792925317 3.2388410467868116 61.36678112950301 0.7170334181227193 
1.8835672923746918 3.3352310602320303 63.3418264432821 0.695951510498341 
1.9149459081783535 3.3917903172505084 64.1673450950841 0.6824568924567149 
1.9216976166954707 3.4242055927731667 65.15632781702745 0.6759213533404825 
1.93851479913383 3.4634898343025116 65.76415685263532 0.6675686750709584 
1.9519744090297748 3.4747863948100672 65.77481635337534 0.6650457313323883 
1.961959492952075 3.4804490910084844 66.34363344455679 0.6642000263430077 
1.9738831404962 3.4829570868661412 66.82220385473562 0.6644044572279915 
1.9871589548407744 3.5116200691993122 66.43543118635287 0.6606839428576028 
2.0085430975546616 3.5492541544221408 66.21201049630393 0.6547444821701606 
2.0295859601337995 3.5973426843669873 65.9336105144768 0.6457364992384362 
2.0831523510120986 3.7178482636578596 65.7599349739488 0.6199691896465628 
1.9590138168078552 3.4743063094022406 65.25658887232892 0.6199691896465628 
epoch: 58, train time every whole data:525.70s
epoch: 58, total time:27458.41s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.81s
test time on whole data:67.91s
1.8777715767574985 3.3731629372204734 59.00870549167213 0.7218457195816803 
1.9174015100248867 3.4909950116215893 59.57793120766257 0.7025566334016322 
1.962682208846545 3.580526291583453 59.815536747270706 0.6883820519117925 
1.9740057373037119 3.6169171459218137 60.758071709758354 0.6793309871987182 
1.987631012789639 3.6390560016993563 61.077642499863614 0.6729125619290754 
1.995626150885863 3.6435516323792076 60.55654534491575 0.6734098331766093 
2.0003332306277892 3.640274480838466 60.543860369605774 0.6780441616511377 
2.002216877891638 3.6237255862980446 60.32051892548981 0.6858314263911736 
2.0113243244963566 3.635327054937136 60.1870904181721 0.6904425778684915 
2.0255290986273793 3.6575289489747753 60.56218704742543 0.6871607960977857 
2.0525787618723474 3.6964938205987385 61.37211481537814 0.67619900784073 
2.1149433722207767 3.8194240298387125 62.19231541451765 0.6489870845006772 
1.9935036551953693 3.6195522551567154 60.4977408120622 0.6489870845006772 
epoch: 59, train time every whole data:525.25s
epoch: 59, total time:28062.27s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.65s
test time on whole data:67.62s
1.8579459296267125 3.3320047538768174 59.93815519606961 0.7199294153624298 
1.8899739159774922 3.430764955751905 60.78451116494528 0.700937211899952 
1.9197344946363675 3.4847981244660575 60.99922822045166 0.6886405159376409 
1.9333978355164152 3.5295360185501066 61.50739464195878 0.6800881136952817 
1.9551832445037685 3.5678607466298513 61.73997456427244 0.6749292460559665 
1.965183118888044 3.5690304128792754 61.54979797927746 0.6756856461734332 
1.9710943588813146 3.565471419918209 61.81379795976179 0.6783991458449993 
1.9797942665085373 3.558784893999385 62.12671715383512 0.6819197384905084 
1.9952513758150772 3.584690464776298 62.13975209602298 0.6825514793743622 
2.0225227814408107 3.632871571875573 62.55180877799229 0.6746195971334407 
2.0506031635705204 3.686453655478085 63.023061627655586 0.6642781290498064 
2.115675018016515 3.8215882375241508 63.533841193377626 0.6360640119742205 
1.9713632919484645 3.5655905295918284 61.809053484403464 0.6360640119742205 
epoch: 60, train time every whole data:525.71s
epoch: 60, total time:28666.22s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.82s
test time on whole data:67.88s
1.9455700476679596 3.5080142418068463 58.53797923286101 0.7201588497450507 
1.9669332063253968 3.5750145160799547 59.460050164119984 0.7021965709811704 
2.016274097988116 3.6789591859341417 60.105754842433136 0.6859528349934626 
2.0129065927004532 3.686457938164314 61.021929677566725 0.6774319682938463 
2.018274542409483 3.6942647192576583 61.416939308921116 0.6719803496089952 
2.012868421850371 3.6715337423388896 61.40383624556607 0.6705791616088341 
2.009447503300206 3.6412703894211864 61.84206353273205 0.6727928285820235 
2.0040799811281973 3.6075550030856207 62.07039218472193 0.6770612352513464 
2.0076825743109166 3.608583655223625 62.0511807548651 0.6784687874846844 
2.0184490382328986 3.6239046853874384 62.16836005071394 0.6736728666060067 
2.037416368068063 3.6588809885839244 62.50192354263171 0.6629864480670893 
2.094014973232434 3.776356066039692 63.12172015776078 0.6363991727931051 
2.0119931122678745 3.6448079157255813 61.30858399815209 0.6363991727931051 
epoch: 61, train time every whole data:525.58s
epoch: 61, total time:29270.29s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.64s
test time on whole data:67.62s
1.902661701035464 3.437979588015907 57.97661148207022 0.7200118658300657 
1.908016262645168 3.476732160656454 59.00912819618384 0.7043445608722795 
1.928106087065701 3.513571609089097 59.72231075119241 0.6904097312202826 
1.9201541184823783 3.5046133790525604 61.044332477603646 0.6847719879220677 
1.9264778066904595 3.5102288363243233 61.25064488916324 0.6807373301496572 
1.9233616104466575 3.487990035118324 60.79274569106785 0.6831367772359273 
1.9146450347663568 3.4512191369810807 61.12595256274152 0.6900250307105106 
1.90387541242476 3.4048067431120383 61.3214531599895 0.6991575655190014 
1.9025208938645997 3.393935583344206 61.402580421618204 0.7038058575083636 
1.9129538607548568 3.4060174094954845 61.643253035421175 0.7017735507419203 
1.9431563932748424 3.453254299606007 62.50319461616941 0.689180511245165 
2.005055129595278 3.578909161789965 62.982784792582876 0.6636283309257593 
1.9242486925872102 3.4686642889712034 60.897985989486045 0.6636283309257593 
epoch: 62, train time every whole data:525.65s
epoch: 62, total time:29875.29s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.86s
test time on whole data:67.98s
1.8826382042385992 3.379692052245748 58.43171996665275 0.7184295056515578 
1.9067597234726308 3.4674632911423107 58.84693240913709 0.70511484437551 
1.9461265026368202 3.5577530707082325 59.094819837400145 0.692050332551812 
1.949573921830349 3.5728484988391607 60.720489144633895 0.6812651718215086 
1.964072268974302 3.5855529046154193 61.46535047882104 0.6764007270600836 
1.9730826531324004 3.581538348270364 61.341713630643454 0.6756452697092733 
1.9815914480415662 3.5835787615950236 61.65434103677016 0.676380941410242 
1.9940983744184708 3.5884015748134797 61.71805707125221 0.678620916436913 
2.0129163514000497 3.6181223950720067 61.74960341621992 0.6786703954144422 
2.0386632837933445 3.663512265411186 62.1625619716712 0.6704748668581324 
2.0706263167159187 3.723158414490395 62.72806155901138 0.6569575912402286 
2.132749410830261 3.854321687643302 63.17894460935833 0.6277277889529985 
1.9877415382903927 3.5997752078150373 61.09113290975825 0.6277277889529985 
epoch: 63, train time every whole data:525.98s
epoch: 63, total time:30479.98s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.83s
test time on whole data:67.88s
1.856489995993585 3.326263872865524 58.93331125220473 0.7242259623678886 
1.885475876114819 3.4194097763300766 59.68674707703451 0.70756903704967 
1.9203503118032323 3.4872864647177706 59.65398197928731 0.6974799284052857 
1.9238676311229135 3.5051617007188924 60.57086454748423 0.6914098887890594 
1.9315139439591162 3.520511933923067 60.75060852359834 0.6862986971252846 
1.929461571321601 3.5014825783143055 60.39165454744651 0.6892623032608367 
1.9254549963595788 3.476829129067468 60.58155311637936 0.6943784363617534 
1.92183038068341 3.445192891407157 60.66150452471841 0.7024497523725017 
1.9275418079585902 3.451611610537334 60.64583031234461 0.7056395060930735 
1.9472126077891638 3.481920344027739 61.10667707287916 0.7013882813454071 
1.979482901929922 3.5367093264690497 61.9037505080825 0.689760718331473 
2.0458466718611086 3.678211223219037 62.68646811367552 0.6605109859912802 
1.9328773914080868 3.486769541604714 60.63112342925759 0.6605109859912802 
epoch: 64, train time every whole data:525.59s
epoch: 64, total time:31083.99s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.66s
test time on whole data:67.58s
1.8456325419952295 3.2726205729081768 59.76572960930875 0.7208733767377928 
1.8676833092072713 3.357215745160219 60.552945896123056 0.704381105858035 
1.895678422282228 3.400044673110937 61.050789493575884 0.6921680684909228 
1.9051582922967416 3.413871905527038 62.24622390286295 0.6880236004856772 
1.9152570170345937 3.431778306147125 62.67732637729977 0.6840601393486824 
1.920665011065258 3.4315685859743907 62.46285529823275 0.6842253255657418 
1.922051151613483 3.424136132081027 62.778315369128826 0.6874907964213447 
1.9217547728344797 3.4074014774760206 62.58039746587962 0.6949622748984663 
1.9318077902160584 3.4247691197029004 62.398853015672714 0.6971650929229797 
1.9562395617576938 3.4685128302816657 62.6556658866645 0.6924593383702485 
1.990544492094467 3.531501095637077 63.12131898191227 0.681717411807364 
2.0583447695128796 3.6771006297188187 63.532586658929766 0.6531218892719937 
1.9275680943258653 3.437972032190038 62.15197359382588 0.6531218892719937 
epoch: 65, train time every whole data:525.78s
epoch: 65, total time:31688.19s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.70s
test time on whole data:67.68s
1.8675590418611787 3.3435922243730003 58.8520511258627 0.724342330104386 
1.8820675634958204 3.401029578955025 59.861352367933115 0.7102396603789299 
1.8950406452409392 3.418019474361743 60.952972336924574 0.7006756234682756 
1.9006663031591369 3.429420572803017 62.93349357722053 0.6913203091437088 
1.9170945689390813 3.463792693573375 63.381393317495125 0.6822340955210446 
1.9245605130085632 3.4645603691954276 63.187794032350354 0.6810006411247261 
1.9245512374403575 3.450441372263174 63.151061596476744 0.6855844891184039 
1.920117077878632 3.421333772815839 62.6458261843507 0.6951028266749253 
1.9184591689607395 3.413196117331621 62.1441482201249 0.7008468747529498 
1.9327208297381266 3.4343663959889286 62.604930384605 0.6957197767669915 
1.9574489373560284 3.4780233100549283 63.22348523446246 0.6857408228499031 
2.0158247534347966 3.6010704929469766 63.829971188778266 0.660732461588681 
1.9213425533761166 3.443733895232848 62.23077374189665 0.660732461588681 
epoch: 66, train time every whole data:525.70s
epoch: 66, total time:32292.31s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.65s
test time on whole data:67.61s
1.8459573345390992 3.283005576168449 58.49336240037869 0.725689115762176 
1.8675883830001667 3.3640820110792418 59.423558790934194 0.7115887344338808 
1.9016411251307777 3.4309906002087884 59.2759241538569 0.701616693681451 
1.9109387629167842 3.455702741143401 60.552838765687724 0.6950927997659259 
1.9236936138988074 3.478705084492624 61.02020949096809 0.6889889291262242 
1.929619727289304 3.472549126332542 60.855484328605804 0.6900824148876137 
1.9258162449384553 3.4520540140955047 60.99782120371113 0.6966530980899315 
1.925481303314634 3.4287647276453166 61.083813130226694 0.7032557854645773 
1.9311424672973476 3.4372664146117797 61.109376268737414 0.7044817898923901 
1.9507142250586655 3.4753085970293602 61.71392414215274 0.697280560544009 
1.980144891358291 3.532556026302797 62.40412356346815 0.685012025673647 
2.0408894696754536 3.664690067449856 62.905656505815486 0.6576446157918052 
1.9278022957014822 3.45739928651545 60.819738841983614 0.6576446157918052 
epoch: 67, train time every whole data:525.81s
epoch: 67, total time:32896.43s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.61s
test time on whole data:67.57s
1.8497177340372333 3.304550722092037 59.68211000009676 0.7277682776591267 
1.8757816057285028 3.3795792210223308 60.85303749402679 0.7133128706416829 
1.8929911103014434 3.4035390730925292 61.59550394665445 0.7027500584429425 
1.9025201800193283 3.4241983672153666 63.24435362056941 0.6947035666860631 
1.9234204131071233 3.4609082892960714 63.85810342050351 0.6860108269444342 
1.942215510189356 3.482349999798079 63.82258939240222 0.6818612359286791 
1.9534971403945238 3.4916440194665403 63.95979271230392 0.6822981375196927 
1.9642610169128472 3.4920336782270427 63.65078561441359 0.6869591551231592 
1.9753068668821028 3.5088731885503517 63.25341673155421 0.6893268823828023 
1.995987971920254 3.5495698897980437 63.34611754454642 0.6835978049519192 
2.0229873592379013 3.6074758283486466 63.507732704091104 0.674043547281519 
2.0865140976288488 3.7454466948239884 63.92944767945674 0.6463443536879119 
1.9487667505299555 3.48921241106654 62.89198271530103 0.6463443536879119 
epoch: 68, train time every whole data:526.10s
epoch: 68, total time:33500.81s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.63s
test time on whole data:67.59s
1.8206678896484276 3.2078703987593045 60.41408011713193 0.7293753210858559 
1.8460914042701146 3.293605436720178 61.36866964323926 0.7136234503520408 
1.8742924849943567 3.337639473732821 61.314932440448445 0.704726017073355 
1.8917630456412833 3.3864840108187577 61.75473572979 0.6987590908561675 
1.9155932138889495 3.43931305103369 61.902748352520945 0.6903885172366243 
1.9352705892567479 3.472059509857415 61.61438925507593 0.6853893808810222 
1.9440514548543308 3.481580384940052 61.49484890009204 0.6874210423920695 
1.9538486851171724 3.4788418909138734 61.1867061209818 0.6931170045815149 
1.9647996997725041 3.5006568317670297 60.934550137474105 0.6933456559538822 
1.9851170735905568 3.5417859756663557 60.995933671612036 0.6881198853164067 
2.0114844594829315 3.6007508042057994 61.076535592204074 0.678795882676962 
2.073223352092806 3.7368920133452184 61.35537001546674 0.6509185142287292 
1.934683612717515 3.459079950971719 61.284458997149926 0.6509185142287292 
epoch: 69, train time every whole data:525.95s
epoch: 69, total time:34105.26s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.75s
test time on whole data:67.80s
1.8204884878371266 3.224057540036042 61.451021607339605 0.7302365218785757 
1.849433339679525 3.3095391600510635 62.342531539514034 0.7153714955544506 
1.881509427930805 3.376352627867472 61.65431223381649 0.7060118992606667 
1.908626619877205 3.445566217316972 62.12825061883731 0.6959101269533109 
1.9292192142759228 3.4887472004243625 62.25513728904243 0.6888974406450475 
1.9457259899688264 3.5076428152199917 61.97957877157898 0.6865660460543418 
1.949820619868913 3.500433148673085 61.89274344764788 0.6913642837667101 
1.9579927984673884 3.4899908903086128 61.601585177679794 0.6984094896221241 
1.9673503029566435 3.5069691748668586 61.27578051733133 0.6998093040247837 
1.9916929532930787 3.5531279250490586 61.52725544896664 0.6935336616369023 
2.027842685686247 3.6227775621895555 62.03726912787183 0.6821085585666059 
2.0982056693971334 3.77152417449492 62.85568849100927 0.6513632007356642 
1.943992342436568 3.4856817434390495 61.91676137267237 0.6513632007356642 
