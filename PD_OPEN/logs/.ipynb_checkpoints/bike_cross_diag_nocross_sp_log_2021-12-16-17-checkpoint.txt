total training epoch, fine tune epoch: 30 , 40
batch_size: 4
folder_dir: MAE_ASTGNN_h1d0w0_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_diag
load file: data/bike.npz
ori length: 3001 , percent: 1.0 , scale: 3001
train: torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12])
val: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
test: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
TemporalPositionalEncoding max_len: 12
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (src_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
  (prediction_generator2): Linear(in_features=64, out_features=1, bias=True)
)
delete the old one and create params directory experiments/bike/MAE_ASTGNN_h1d0w0_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_diag
Net's state_dict:
encoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
encoder.norm2.weight 	 torch.Size([64])
encoder.norm2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
decoder.norm2.weight 	 torch.Size([64])
decoder.norm2.bias 	 torch.Size([64])
src_embed.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed.1.embedding.weight 	 torch.Size([250, 64])
src_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed2.1.embedding.weight 	 torch.Size([250, 64])
trg_embed.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.1.embedding.weight 	 torch.Size([250, 64])
trg_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed2.1.embedding.weight 	 torch.Size([250, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
prediction_generator2.weight 	 torch.Size([1, 64])
prediction_generator2.bias 	 torch.Size([1])
Net's total params: 1140610
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405]}]
predicting testing set batch 1 / 168, time: 0.63s
predicting testing set batch 101 / 168, time: 28.13s
test time on whole data:46.73s
40.5813251594792 49.523569521415176 1678.6869883402808 0.08227536765620337 
40.32431476269955 51.43040926585563 1664.553612962808 0.06005229980179304 
41.65556046676112 53.405866727804685 1718.067489906371 0.047602004911117664 
43.40933347039218 54.02444511755517 1794.7462898862693 0.04265281785852964 
46.18006911548325 54.14120428913453 1912.1624602745371 0.04136994571058806 
47.5957331193878 53.711575030603086 1969.2082281311493 0.039825497269931974 
46.55417292178608 52.31561062860123 1924.4683472456932 0.035133653306128006 
43.860445725032086 50.06848462950267 1810.7967344488045 0.02900537802248158 
41.47666674631453 47.980872626465576 1709.495685639778 0.02357120497130311 
40.287615686368994 46.33306240982591 1657.9721564174242 0.019529271331205758 
39.88558206631767 44.9444824635305 1639.5513940447688 0.015969164725728877 
39.22478576794098 43.55992137354375 1611.3610817542103 0.01184052785461322 
42.586300417330285 50.245122344977844 1757.5895519405815 0.01184052785461322 
epoch: 0, train time every whole data:182.16s
epoch: 0, total time:241.56s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.68s
test time on whole data:47.68s
4.440123095903102 6.130950029049054 141.048265602417 0.25340573300286 
4.464376818054755 6.178082171612931 141.61803027217871 0.23716457209781733 
4.504512601788289 6.249841754434388 142.53206043394243 0.21703740185369014 
4.5243420184424945 6.290786666084272 142.70876660302292 0.19755874769957116 
4.5823754863699095 6.372234986454316 144.41180195328803 0.176381569660631 
4.566445666237158 6.367697828840264 143.26135915365793 0.15950858780888091 
4.6384871052034375 6.445308548377572 145.9188645664382 0.13865740049586028 
4.703036032882209 6.514341248293041 148.26034095547493 0.12133476590699416 
4.704171671527216 6.520053184605921 147.94261328831547 0.1084378490590482 
4.711304014384126 6.52530772445981 147.91630407987577 0.09589088562569589 
4.695590111858256 6.486649095812559 147.36887195722144 0.08247541395831023 
4.7435060183635605 6.546925904156084 148.64585991333402 0.07282366909738461 
4.606522553417876 6.3871808770397305 145.13625813288994 0.07282366909738461 
epoch: 1, train time every whole data:182.07s
epoch: 1, total time:485.47s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 29.08s
test time on whole data:48.35s
2.8081134426749887 4.459935844142868 85.33639600687944 0.6437871954655499 
2.863169958622594 4.547991376990536 86.40940446364647 0.6090224461384522 
2.982901410529745 4.768648491785048 87.27732202125188 0.563018674590673 
3.022667557761162 4.847765851757565 87.5475853128621 0.5191099870629451 
3.084112664824174 4.944165232240189 88.52035983954785 0.4716754614583316 
3.1116318519784225 5.014123525470788 88.12873899591538 0.42166431794647774 
3.1706547919154344 5.099608067911092 89.15414902877929 0.3779639325314746 
3.2202045733158786 5.164129372817757 90.43745174553199 0.337976474364141 
3.271200262048858 5.219125643569786 91.99204188140058 0.30010540069188885 
3.331955530387926 5.276048104238092 94.0268425487495 0.26597963635474253 
3.3316718430691177 5.274304423366112 93.55123927787122 0.23602056843846247 
3.4335889002766815 5.359870129053436 97.20103596011752 0.20367672988693478 
3.1359893989504153 5.005821492030198 89.96538766681653 0.20367672988693478 
epoch: 2, train time every whole data:182.38s
epoch: 2, total time:726.88s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.60s
test time on whole data:47.57s
2.142283262338312 3.3835093610365554 76.81938546150124 0.679432538606487 
2.251264106087298 3.5474330141182806 81.32383124688158 0.6414879972982428 
2.3219055234649706 3.6915808422199032 77.6061062095826 0.5932372702524135 
2.416228099300127 3.818632570794422 81.15240586217485 0.5546469539706397 
2.5224853639554765 3.9506608229956712 84.98152875285993 0.5129578431947353 
2.631407027868199 4.065756670658346 88.31391498705001 0.47336846873343696 
2.640575933192014 4.11064368073708 83.79267307114604 0.44579770196220764 
2.6775378473702642 4.153793249654771 81.39320400405624 0.42478286371616863 
2.729216423709212 4.209860461095912 80.78359611207343 0.3991322736317446 
2.7704507062739383 4.266943195118589 79.54406911247975 0.3718168331808131 
2.849883346367095 4.333735161919223 81.21818827030386 0.3385304429704821 
2.9117060269753314 4.409801333891947 81.2249880670493 0.3025769645562194 
2.5720786389085197 4.00717030329997 81.51288456231826 0.3025769645562194 
epoch: 3, train time every whole data:182.58s
epoch: 3, total time:967.98s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.71s
test time on whole data:47.79s
2.0932058865172523 3.247107632209321 67.27230735103723 0.7196680966903994 
2.163119652155343 3.429154866961528 68.39915587109682 0.6731923629787289 
2.219390540328675 3.732852115137071 63.71677764214958 0.6073571546393892 
2.2792030832880132 3.8293974253493093 65.67283825123327 0.5712547081421201 
2.3469901584160646 3.9264448091594057 66.71060710706584 0.5374385844367756 
2.4143993483465165 4.0340763693268284 65.36427135014216 0.5058834031097023 
2.458249466411858 4.08842245112923 64.54529522063662 0.48593192951947706 
2.4803629596558 4.123257531361933 62.762892096099336 0.47729289036659517 
2.4887510733788036 4.1252418669578415 61.676082999337275 0.47917736180836895 
2.499452499768475 4.160517480377145 60.01592017400156 0.4755343266855562 
2.518059110822777 4.165267503132523 60.46171119702779 0.46921294365412625 
2.5486562884809185 4.210987811699462 61.28489716734323 0.44934177243181844 
2.375820005630875 3.93401692846763 63.990096103848835 0.44934177243181844 
epoch: 4, train time every whole data:182.46s
epoch: 4, total time:1209.16s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.88s
test time on whole data:47.98s
2.024054822888669 3.252535397945608 61.89911030935967 0.7259687117317783 
2.0992609129884237 3.4049555465613803 63.64530914386434 0.6888176221276185 
2.1661108138105343 3.6434647230739103 61.96191498396493 0.6410402426782883 
2.221954261397943 3.7197257950330793 63.661123513722515 0.6133639421125502 
2.286372043849102 3.830263973359617 63.530886529716646 0.5822582747749948 
2.3267553412535538 3.8956289220379676 62.60946244131355 0.5621871654748992 
2.326455565775523 3.8907566098002713 62.39275457750433 0.5570203520580975 
2.3192848959018786 3.877262511971458 60.96460630405899 0.5604007656228194 
2.315200577809697 3.8649956372515173 59.88237679713 0.5669614989015503 
2.3139769748393446 3.893501236698089 58.175823412803865 0.5652404265547606 
2.3470742002979277 3.923496112238433 59.30751867804152 0.5500583343644642 
2.4025541754654123 4.0029073671688655 61.085414997544895 0.5185929342457439 
2.2624212155231676 3.772891625837464 61.592952464377085 0.5185929342457439 
epoch: 5, train time every whole data:182.24s
epoch: 5, total time:1450.54s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.69s
test time on whole data:47.71s
1.8818584534014087 3.3396170723419036 57.86495955294267 0.737900958127589 
1.95831065296772 3.4882550440210007 60.28219654761071 0.7035251116457444 
2.060848795033282 3.710594484137356 61.71924293257156 0.6626724951695165 
2.0975079784450075 3.781698765355603 62.28325534845964 0.6452332884209706 
2.1428342181409574 3.877839114001623 62.27232917831862 0.6275085803292794 
2.184786586450324 3.950027083410038 62.16053223970691 0.6217509727423315 
2.2261991321177534 3.985568755534778 63.54545106286375 0.6222314202127665 
2.2658438728455277 4.023035439331102 64.28423167087003 0.624893546708077 
2.2740702582983565 4.02112434833239 64.14168147948998 0.6301883105323726 
2.3656448361774287 4.114453098348189 66.86838144401783 0.6316525776724632 
2.389925939854678 4.1614253644017705 67.07111127000539 0.6190490738909985 
2.4483888868728565 4.253870133339655 68.55021675423997 0.5839158402300699 
2.191351634217108 3.900979391155565 63.420447706345975 0.5839158402300699 
epoch: 6, train time every whole data:182.60s
epoch: 6, total time:1690.00s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.65s
test time on whole data:47.68s
1.8212908465700846 3.2363772537915674 53.950469980004435 0.7480237135499181 
1.8868165129688346 3.397169821606959 55.17375231126101 0.7177465867985544 
1.9724218652321115 3.6128406133566413 55.71542224833435 0.6863630273279798 
2.002111359866957 3.6822652000660088 55.970496268185656 0.6735868349006069 
2.035786699877076 3.738623443958752 56.715679546211206 0.6591395404796204 
2.0616111581388505 3.7766616460292037 57.135174278857825 0.6486456761445163 
2.077257490777987 3.802502727979749 57.48274117785911 0.6422790459074738 
2.0969276435069206 3.8334494975271367 58.09442717446889 0.6362668641149992 
2.10693735841502 3.8485952848199765 58.14152894270762 0.6324074810656289 
2.1354491929480184 3.89435519705196 58.52527842059937 0.6275310968088413 
2.1628724049664148 3.9384891373136584 58.58130788181952 0.6203206177220658 
2.198267667788746 3.996368738635149 59.580389992950835 0.6024884389506197 
2.0464791834214187 3.735882126581638 57.08898148705583 0.6024884389506197 
epoch: 7, train time every whole data:182.66s
epoch: 7, total time:1930.95s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.63s
test time on whole data:47.66s
2.0999408761819027 3.443338083906911 67.45414633135609 0.7507919699381117 
2.190815146640032 3.6093945834278793 69.96701330700378 0.7180469801294654 
2.2996870096622124 3.833834601270522 71.17495388206001 0.683443215626828 
2.3534332144728727 3.9253367348415957 72.41009636768561 0.664279208333085 
2.4307323666870415 4.045594323148051 73.62483932002236 0.6440305061840634 
2.4734751747960138 4.101094343744546 74.08160197880936 0.6337175206612128 
2.506787242949807 4.144072024669439 74.70364284801299 0.6301500090623671 
2.5343167179686326 4.191676893102218 75.26128561225758 0.6240809956855023 
2.5615839937627314 4.2416879043915 75.59316258420276 0.6223705434442273 
2.612965783863373 4.324476013483975 76.7727924001727 0.6175075019010946 
2.6514541654524706 4.40047362218769 77.34250710996936 0.6093866504885509 
2.699969783344439 4.468557529522328 78.61335160043105 0.5931112046076303 
2.4512634563151274 4.071704624397032 73.91679257723378 0.5931112046076303 
epoch: 8, train time every whole data:182.82s
epoch: 8, total time:2171.02s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.65s
test time on whole data:47.70s
1.9410673348727148 3.013759714590329 64.66166888109642 0.7627483488502254 
1.9781538853453737 3.111682672607932 64.64002020670928 0.7421094315359094 
2.0203733694993313 3.222039409952515 64.31799541627191 0.721770037373439 
2.0325223229374916 3.261340217317221 63.455518316676184 0.7141525729039642 
2.058850109152407 3.3066652199335405 63.65059552072706 0.7024232652915979 
2.0795086436441967 3.318899138014246 64.63451818283953 0.6951209871841253 
2.091901115085486 3.3100441201330484 65.89328703173348 0.6945149873293277 
2.0941025652362124 3.3050534818506505 66.8562446744212 0.6950400748916632 
2.1012910772930122 3.313909160367723 67.55047111967369 0.6930297585650698 
2.109284455736035 3.338715909725599 67.53656293055442 0.687539616521918 
2.1229995770394092 3.379820733455928 66.85368537347436 0.6806911726355218 
2.155195042528567 3.467843651144084 66.90016693795893 0.6594040241603583 
2.0654374581975197 3.2811359470073658 65.57930370912958 0.6594040241603583 
epoch: 9, train time every whole data:182.96s
epoch: 9, total time:2412.55s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.66s
test time on whole data:47.71s
1.96783472845455 3.2918972863157165 61.88774172113598 0.758362101049685 
2.0298492331919924 3.419057003082156 62.85432086303986 0.7384855299480406 
2.121646633890413 3.6265559113261463 63.51066598668786 0.7138025464844737 
2.1529585137000042 3.6798589395414223 64.10568140875883 0.7088155232882221 
2.2252629077679345 3.799978644879689 65.12128571952856 0.7021627708308812 
2.2861348366802114 3.8947617998557065 66.20369646512727 0.6943629953013065 
2.331000096030977 3.96394178607389 67.37455558876448 0.6831331426600835 
2.3871633606403178 4.047275976097942 69.33933213878849 0.6712987098077438 
2.4419070704963413 4.13585247775334 71.18120724720993 0.6577997974825188 
2.5157091711810127 4.244016301053954 73.80831446193778 0.6426360047432176 
2.589386109160437 4.347669848411992 76.2953492887654 0.629974108443383 
2.6376356963807983 4.421126233954774 77.6683330539351 0.6145459780056088 
2.307207363131249 3.920965367789103 68.27947443971571 0.6145459780056088 
epoch: 10, train time every whole data:182.73s
epoch: 10, total time:2653.43s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.65s
test time on whole data:47.72s
1.7981272878732888 3.133587859260299 59.00107450837907 0.7630440607557568 
1.8639561226085892 3.267216321223865 60.47299652543675 0.7435300953810495 
1.9480692744261274 3.4547738499222924 60.2666693937705 0.7198221300223057 
1.9853139976692342 3.5162058698212144 60.49957033871137 0.7158173716634838 
2.039038616049946 3.5990537885618745 60.89768761043971 0.7083104326315309 
2.0801657321950686 3.6658427351565273 61.27678070708159 0.7020933847118721 
2.095628302175197 3.6942165024750055 61.12605992483912 0.7002090363652779 
2.097969303319763 3.698304608642966 61.215417994902275 0.7006487436267204 
2.0982753171495916 3.691084960672722 61.216544399662645 0.7009013647792884 
2.1014260779689287 3.686336073732249 61.743739148844476 0.6952200527363044 
2.107628418702455 3.6941840879257946 62.24923555428996 0.6817096399406 
2.1554428484293497 3.8015077423562555 63.45872330432899 0.6504358205231869 
2.0309201082139614 3.580283556482875 61.118759135982614 0.6504358205231869 
epoch: 11, train time every whole data:182.77s
epoch: 11, total time:2894.07s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.72s
test time on whole data:47.82s
1.823068202184868 3.27922381965373 56.81303389120206 0.76347247845747 
1.8933918267110628 3.406535751806986 58.34120054423059 0.7439952236420564 
1.9615617978895703 3.5158415844388626 59.49016163662616 0.7228151946230628 
1.9738003799363617 3.5073772682101936 60.17184464622909 0.7247919408227934 
1.9959798464909906 3.5167289926060143 60.44673259977118 0.7231778981352454 
2.0230785700488125 3.560434016900153 60.96229039446326 0.7139453383584904 
2.044247247044618 3.6051478586283565 61.12975409299107 0.7074156973498362 
2.0504353508452575 3.6276684498754124 61.401458678260134 0.701632349734991 
2.0568811720747147 3.6426511708828127 61.91048758955918 0.6974262841116992 
2.0604630854128017 3.6672212400110205 61.98297343276349 0.6874795946131796 
2.080395173018976 3.7189796068997687 62.54738778868298 0.6753964333136872 
2.127038067872148 3.8223305749236767 63.28145752511253 0.6503009365790322 
2.007528393294182 3.57514818123593 60.70666392433146 0.6503009365790322 
epoch: 12, train time every whole data:218.12s
epoch: 12, total time:3172.81s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.60s
test time on whole data:47.69s
1.8353293374692579 3.157309707887102 52.64520869100509 0.7787018896081969 
1.9006547340749806 3.340227626807488 53.891551925018824 0.7471354430095846 
1.9683679526424303 3.5167107862664815 55.48765293377574 0.7164717734511977 
2.0081720570298356 3.5977329791460293 57.044684347044395 0.6954564989102192 
2.0608539645622175 3.6925199525317187 58.67166385179041 0.6676333218624 
2.113132707084396 3.7787681732336273 60.35081327435597 0.6376536400577651 
2.1555471437733975 3.8403100994742267 61.52623944545075 0.6137028033297052 
2.196010822921903 3.8876517237135477 62.231487212511816 0.5946899863294858 
2.2137195107293803 3.910685415267454 62.45956393450486 0.5877358348276804 
2.2332620160793444 3.922878041060774 61.93075952951117 0.5844581883242422 
2.259040575015758 3.940004442364637 61.080165677259224 0.5839870623555788 
2.2888198260194845 3.9809192045501582 60.67491930397735 0.5768485514643374 
2.1027425539501987 3.722298325486338 58.99975920693902 0.5768485514643374 
epoch: 13, train time every whole data:182.24s
epoch: 13, total time:3417.23s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.78s
test time on whole data:47.85s
1.7669221347969557 3.034101841334914 53.54820826713037 0.7803906949955545 
1.8148228377736217 3.14002435238088 54.82549900116371 0.7593078601123583 
1.868719845767532 3.2737239879484696 55.419885585477616 0.7359175240066496 
1.9015530230488096 3.326791473825206 55.99829161495702 0.7200834082490447 
1.9218014173972464 3.3541754655256173 55.94401086223792 0.7116001935162038 
1.9380614118780053 3.3681773064785245 56.057805617443854 0.7051758718243355 
1.9555788925574826 3.3900592541300645 55.80996386701192 0.6976600704323715 
1.981179771212861 3.4324771900907005 56.248833474253686 0.6838060149037847 
2.0001162674708204 3.4653504145016076 56.17805737839606 0.6737762347201238 
2.025237948458287 3.4915888576833463 56.58416008759664 0.6656454156051632 
2.0444938529929413 3.5180127302585795 56.152685061899064 0.6603670367501743 
2.0703879899596354 3.5787001362963218 56.24477507592799 0.6497107671989225 
1.9407396161095165 3.367778749751246 55.75105510063067 0.6497107671989225 
epoch: 14, train time every whole data:182.18s
epoch: 14, total time:3664.28s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.63s
test time on whole data:47.65s
1.7710830251580725 2.9514482753218987 60.250465714626756 0.793957579488675 
1.841244553077079 3.0421439049329493 63.466532089877845 0.7727121847214453 
1.9137309024583193 3.157285448459342 65.69243558333439 0.7546927855934917 
1.9703006237896958 3.212598815586146 67.78992959420493 0.7453055589170834 
2.028334753381443 3.3183572996065505 68.72062535265633 0.7304901381284045 
2.0925419236620266 3.4552404535896346 69.56339271535758 0.7098833582091714 
2.1551094756149465 3.592138376862039 70.40985330322401 0.6893672137973029 
2.1985629383841796 3.705416505595792 70.5455134499832 0.6721960645703656 
2.26309593558276 3.821977121343425 71.52893929881361 0.6567863579175394 
2.30969842041275 3.906726600772674 71.93066912601162 0.6460435993747874 
2.361650710376157 3.994697749507066 72.20728253738821 0.6381774675211319 
2.420702505303281 4.100603475364878 73.02566847523964 0.6236248015823141 
2.1105046472667257 3.541201081471391 68.76115621622918 0.6236248015823141 
epoch: 15, train time every whole data:182.54s
epoch: 15, total time:3904.84s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.74s
test time on whole data:47.85s
1.76027345314276 2.7573629652415104 59.63637707482121 0.7991845350524075 
1.7897646447849416 2.822526895442528 60.456568175641145 0.7879970622535414 
1.7955111436045597 2.8815171091711735 59.34619855992235 0.778051766480745 
1.8059226235514063 2.933460113227659 59.2501323120587 0.7692218060807502 
1.838606745208924 3.0309193109727586 59.070526299877535 0.7530077973928915 
1.8827295210706514 3.1649776712216484 58.935669500317886 0.7299999970776722 
1.9234968672180992 3.2926327719757134 58.75607462096194 0.7072567641019049 
1.9660375730972737 3.396146445430836 59.21799891964476 0.6880319611070604 
2.0039479853774287 3.4980958164886444 60.05681895106786 0.6695653367819819 
2.0490618616495104 3.5903679079734765 60.687595534833186 0.650764582416359 
2.0828025387063445 3.655721490275774 60.554149183499604 0.6387035616783279 
2.1096134466730234 3.716544359374892 60.691389425836675 0.6271069989944716 
1.9173140336737435 3.2450122253030185 59.721635329478 0.6271069989944716 
epoch: 16, train time every whole data:182.64s
epoch: 16, total time:4146.54s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.73s
test time on whole data:47.90s
1.6912701887251542 2.8560127209816435 56.31138272085089 0.8004167916726491 
1.7459195033420054 2.9851028067998824 57.49186329188208 0.7807028862398188 
1.7841352401821031 3.0724945712324634 57.885840795428855 0.7686469020852834 
1.7993326613497698 3.0805788831213072 58.99472292141507 0.7649256218669598 
1.8383648756239563 3.1541565099445994 59.67699479400132 0.7517732858253432 
1.886610323223862 3.2618912284692088 60.39610852747752 0.7326627578215595 
1.9294504982105323 3.3536492170682233 61.203368976915385 0.7174971776249762 
1.9746363752989897 3.4580381935617224 61.23118955562893 0.7016103245344952 
2.0167430150261416 3.5647288731082005 61.45479374314793 0.6861826092288266 
2.0540285607358175 3.6416470441594377 61.46224554475528 0.6752742580514768 
2.0825190660343282 3.7061263629243952 61.11118245743933 0.6641064776156105 
2.13310527686145 3.8155002814380636 61.54536778453139 0.6400214094986693 
1.911342965384509 3.3423266050568117 59.897194116942586 0.6400214094986693 
epoch: 17, train time every whole data:182.76s
epoch: 17, total time:4390.35s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.69s
test time on whole data:47.80s
1.7154938679830658 2.8766073315279597 57.73111806633718 0.8026937378933271 
1.781265124570401 3.021605302089623 60.265168448080416 0.7772519910078907 
1.8168331114007603 3.137177749951473 60.120501471561624 0.7618767189630428 
1.8593939102080075 3.2412724404527657 60.36127324205711 0.7485471405017484 
1.8952287782129964 3.3338898598496884 59.62269324890419 0.7391894981863681 
1.9244133397928838 3.3906857573501634 59.34042504064521 0.7340040818752224 
1.9203812643496232 3.371321599742903 58.53337210221427 0.7397383098717148 
1.91136193505036 3.3453651571690557 58.22300342117907 0.7426328982954351 
1.916062882583145 3.357891045506159 58.11282628816047 0.740077565564536 
1.9308409763189653 3.388802693839622 58.339494228759456 0.7346941167978374 
1.9663512137086974 3.4663563937764033 58.58170257294052 0.7233285603371336 
2.020827995634327 3.576865964478153 59.37612317414613 0.7082877161880039 
1.8882045333177695 3.2976072287800235 59.050615237698366 0.7082877161880039 
epoch: 18, train time every whole data:202.63s
epoch: 18, total time:4655.28s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.76s
test time on whole data:47.95s
1.8421441101713905 2.8157984220019134 58.66093869506479 0.7998296613943705 
1.8809095068154413 2.904169305327004 59.42276499906822 0.7840033485400776 
1.8962229888883553 2.9486191703258644 58.643059110422456 0.7774871696479331 
1.8998125743967083 2.955312886762792 57.95740168931487 0.7752936433194524 
1.9208004228613738 2.9911224774467255 57.64502495846203 0.7678676930521959 
1.9543415643626025 3.0925727415067534 56.779393476735265 0.7499974233526445 
1.9869510338866994 3.1982397366044775 55.69788183851973 0.7309720635744713 
2.0243441341337527 3.3029709760605446 54.70249989718581 0.7151057369698809 
2.0562572004630986 3.404320103155274 53.64648305398478 0.6987615523509074 
2.088631530021273 3.477439875478462 53.361021087656525 0.6824149263083424 
2.111799141309357 3.5331653730704584 52.86689685661149 0.6693750997901894 
2.1429996085437457 3.600695845158309 52.93499561409192 0.6565821300943318 
1.9837678179878164 3.1959684952405594 56.02639586782241 0.6565821300943318 
epoch: 19, train time every whole data:182.32s
epoch: 19, total time:4911.11s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.73s
test time on whole data:47.84s
1.6721333608625546 2.8888672060695475 56.625893587871126 0.7907970052458432 
1.7177029670939026 2.994542450584443 57.40737182805652 0.7765068761059086 
1.7581431048475207 3.080781147679794 57.96694961462116 0.7649508704074303 
1.7832382633663892 3.1112392398194246 58.69727818537515 0.7603132387065279 
1.8185575513402443 3.173458023441281 58.87374028699306 0.7526448520174991 
1.8572881181253387 3.2592625482775492 59.31697495461036 0.7413687514317336 
1.895560550144918 3.3504197724653526 59.57666975086241 0.7304253315787798 
1.9417618517810922 3.4551756918166436 60.046865536681835 0.7175511455472399 
1.995122316622131 3.574021985721817 60.524395583309 0.7028814723264848 
2.0508489078132524 3.691710495102421 61.225425164269446 0.6833604680049443 
2.1003447624516807 3.7983126176505477 61.75181559407725 0.6623004888306598 
2.1500938604872317 3.896377656475088 62.16900103236684 0.6433507112363367 
1.895066301244688 3.3708596495738 59.51529136139372 0.6433507112363367 
epoch: 20, train time every whole data:182.58s
epoch: 20, total time:5154.48s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.73s
test time on whole data:47.81s
1.6799207269973344 2.769916425534753 62.61488832628839 0.7987108019194405 
1.748170815638222 2.8803035664937005 65.70898563734056 0.781238228773677 
1.8017890974873056 2.990058561120693 66.84953250529728 0.7640183005950091 
1.8689380831682967 3.125494754427455 68.53459422604672 0.7411638533373384 
1.9290908181287703 3.263750909986613 68.28934864672166 0.7160346942462363 
1.9817690258329468 3.3954664755617383 67.69964867149717 0.6927133521781607 
2.0245156185545383 3.500891236402783 67.0367361685882 0.6759915998195386 
2.0512100938101785 3.5573049443425564 66.16057652631801 0.6713756865978957 
2.0759871737528592 3.604450709263563 65.45414826394344 0.6709488312102616 
2.103518789786047 3.6623626894081065 64.83602279786925 0.6678069617532656 
2.134552929813752 3.7274899392559164 64.34308563817297 0.6624837064684079 
2.1876927564714457 3.8390484301430456 64.65613125597503 0.6451451110310548 
1.9655963274534747 3.376476384293165 66.0153021444681 0.6451451110310548 
epoch: 21, train time every whole data:183.86s
epoch: 21, total time:5398.46s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.79s
test time on whole data:47.97s
1.7765691833650428 3.0610556462922367 59.49679698265682 0.7783561177578534 
1.8195946896631447 3.147723153635403 61.41854510395306 0.7620713434320341 
1.8543680854557703 3.2209745081041765 62.73155803195323 0.7485043152495683 
1.8804103311160136 3.245684141632006 64.43778629289774 0.7412788941217844 
1.9102174767026945 3.3089658780678284 64.86553255412905 0.7277742919817279 
1.9420830679093266 3.3710828756364624 65.16868219179571 0.7159535410815309 
1.9614154456338535 3.420193959665069 65.09440786046974 0.7058669139009225 
1.9824778812422106 3.4782466875028324 65.17552550968809 0.6929885222135107 
1.9916683203893757 3.505342367663618 64.62001816774311 0.6866399490638415 
2.013261088882972 3.547262566154907 64.82426895624907 0.6777417536479652 
2.0435513866416044 3.6246595615486163 64.70911082902823 0.662799955434704 
2.086646851132136 3.7232034187581964 64.58361360116784 0.645333063222013 
1.9385219840111787 3.3932241819859303 63.92723936089598 0.645333063222013 
epoch: 22, train time every whole data:182.64s
epoch: 22, total time:5644.24s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.76s
test time on whole data:47.90s
1.7563460945535807 3.0357058088982294 57.837898149631286 0.7788542634488202 
1.8295965701383317 3.1961842693926346 60.45381969803084 0.7516010946341775 
1.9117107882010085 3.3698109238414564 62.74540265437758 0.7209942558215697 
1.9737043299874557 3.5125603427802825 64.21459303488064 0.693213952245187 
2.024131819284299 3.612192228835788 65.05887180976032 0.6704554658493919 
2.05533603558033 3.65479908492717 65.8768888268941 0.6602716609812879 
2.079186564824677 3.678761432438898 66.74065182230419 0.6547297264786397 
2.099194841548091 3.7114226825556402 67.07164673015565 0.6492252592306978 
2.1173142767060726 3.7585706299882955 66.75351317764454 0.6414840827263054 
2.1441174328284602 3.8167110796258243 66.43077200596687 0.6333228908526547 
2.1741434624922418 3.8722805414800465 66.20422000104766 0.6253262620209993 
2.2134725175737624 3.9393858182454484 65.8698746427012 0.6141448488501302 
2.031521227809859 3.6060793158187203 64.60500061803779 0.6141448488501302 
epoch: 23, train time every whole data:182.62s
epoch: 23, total time:5888.02s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.70s
test time on whole data:47.88s
1.7509213474328142 3.0760061725867107 55.1989079168004 0.7764576848216886 
1.8431283929607876 3.3090210987570807 56.364648444396835 0.7546483395267998 
1.936347921633206 3.532210030178774 57.515792482411655 0.730362107055925 
1.990256979697251 3.6295073983917265 59.12581517306238 0.7172717229733329 
2.022089309446425 3.6701468148985614 60.345644611696514 0.7067775981613826 
2.033731587276661 3.6472619179496815 61.442177596838945 0.7044166171687074 
2.0297730849738276 3.5965029505837394 62.01816614603706 0.7063744607982199 
2.0283321214846026 3.5752662545890006 62.788966329297246 0.7014063738400041 
2.043492676285406 3.616462712521972 63.24240866192281 0.6868847889282634 
2.06563312864206 3.6768640886697606 64.07910042172715 0.6731096776498016 
2.107995433535782 3.7543409996303385 65.27048447874861 0.6587923993314063 
2.1547421861047016 3.858035866161989 65.97342435201085 0.6352330001780356 
2.0005370141227936 3.583861683849698 61.113989949314075 0.6352330001780356 
epoch: 24, train time every whole data:182.75s
epoch: 24, total time:6132.65s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.87s
test time on whole data:48.01s
2.218843907137357 3.8532348207893774 51.35710516505152 0.6786020293125016 
2.2314492357862847 3.927824848709668 53.308536172144116 0.6363294794173843 
2.2711363449845288 4.009274004243606 55.15347297436224 0.6007925479775897 
2.3093376627686832 4.049832540966435 57.313445011716304 0.5757866102504693 
2.352207504618203 4.071212247327726 59.17604412909066 0.5550222886847492 
2.3960515290070323 4.087841790177317 60.49410255317098 0.5381270990674475 
2.4450747037781846 4.121424536184645 61.38800164444104 0.5180291167190401 
2.4971090189747156 4.181634048499692 61.506474909454724 0.4915368795843496 
2.5519669681663313 4.259891286803839 60.9724228074967 0.46131660703948346 
2.6011088813483894 4.327333913750226 60.952811463491685 0.43056930430921186 
2.6385161361701432 4.384031533300083 60.55577710091955 0.4082280102686534 
2.6649402957088535 4.423572514777048 59.92526663123399 0.39655975786813 
2.431478515704059 4.144959693406236 58.508813311687945 0.39655975786813 
epoch: 25, train time every whole data:182.78s
epoch: 25, total time:6387.37s
predicting testing set batch 1 / 168, time: 0.29s
predicting testing set batch 101 / 168, time: 29.28s
test time on whole data:48.62s
1.7811267118947136 2.9938615638547383 63.793327439676894 0.765611779259213 
1.8236742017796883 3.1165326540876688 65.6532878562779 0.7419103701599364 
1.858936587642346 3.2226303718133535 66.2467413204872 0.7243423226988132 
1.880951172358933 3.263367590332695 66.92800647831851 0.7184745527718085 
1.8955562313652288 3.2833805060113948 66.57892250099489 0.7167604625929271 
1.911697036783078 3.3271404190839027 65.83934500715573 0.7106032530202173 
1.9326456490175887 3.4013698689326177 64.77542697468529 0.7001119620823113 
1.9582631947137414 3.4818646632814567 63.807692801975215 0.6900145628230859 
1.992948261175837 3.5768074972356305 62.992114745203445 0.6770413184992946 
2.027878922148652 3.6608827918679308 62.47185215277968 0.6652718663888104 
2.0697410197894843 3.7385053148497005 62.626371784166636 0.6507204692350862 
2.1162830954306715 3.8236618549236545 62.879494650519085 0.6287898680964867 
1.9374751736749969 3.416217126720368 64.54931969171194 0.6287898680964867 
epoch: 26, train time every whole data:182.64s
epoch: 26, total time:6637.03s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.84s
test time on whole data:47.97s
1.8832666519324162 3.151543472018174 58.802867286349745 0.7320045613649181 
1.970189640906595 3.376590686165091 58.797137997325066 0.6973981950568633 
2.050041701642885 3.558927027433348 58.313882215721094 0.6677517778462229 
2.104737742365205 3.64829447537776 58.91616875475996 0.6488028031326345 
2.1440993143728093 3.6824768914248693 59.820031796316506 0.6381228250130574 
2.1630035959290606 3.6526227847710437 61.27717252124816 0.6376324303753275 
2.175851241857789 3.60087843392426 63.558732805951756 0.6372740326487225 
2.1920611062914666 3.5835270397606007 65.95830083809683 0.632589154816318 
2.219822998545709 3.6294621235191435 67.44100877593742 0.6157069997697261 
2.2350206978752145 3.684266068338129 67.8610750117642 0.6008712557591307 
2.2402656677576402 3.7325239225293574 66.34838490616079 0.5928009911963231 
2.25860479366664 3.799145271907472 64.43387901459077 0.582776260241947 
2.1364137627619524 3.5955057280532654 62.62759358007594 0.582776260241947 
epoch: 27, train time every whole data:182.99s
epoch: 27, total time:6882.29s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.76s
test time on whole data:47.86s
1.7805001005409729 3.163837490883144 56.81438968620416 0.7623396597878566 
1.836572209940159 3.2779864718535 58.833873148898995 0.7463771314290022 
1.8723657963327354 3.3424013535569324 59.74837956770146 0.7308384880247907 
1.8778070658251111 3.333122838966316 59.835031533787266 0.7362401095267128 
1.895514979647916 3.353205794038821 59.9109703025532 0.7347792612394872 
1.9236313217789645 3.4024910213791437 60.14609799460744 0.7302328671074094 
1.9568245016208716 3.4728160368473557 60.59630546990328 0.7214625695205913 
1.9768815614459592 3.5202983200017983 61.209053335152774 0.7139811968190205 
1.9918970332693842 3.549664464496631 61.57377641277362 0.7061870568174589 
1.999915052914992 3.5621723434606065 62.151888644124995 0.6983160847283291 
2.0144457383139858 3.5967263526474684 62.77875517725261 0.6851067477811891 
2.0431224775896184 3.670025255397485 63.136219402622174 0.6661887327542565 
1.930789819935056 3.440022030303889 60.56131974800751 0.6661887327542565 
epoch: 28, train time every whole data:182.74s
epoch: 28, total time:7126.04s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.75s
test time on whole data:47.89s
1.6976852576269634 2.834981631184019 56.28446015535333 0.8087680630397924 
1.7467357847279026 2.948693355893449 57.83156961926467 0.7889544205170207 
1.7831241583739008 3.041417251525431 58.61904778595264 0.7754638727642887 
1.8148894966361777 3.122714360131018 58.69804045911773 0.7667194851012059 
1.859370170465094 3.2105380237849475 58.92911489410169 0.7571614730085378 
1.9022037501540923 3.301547730165681 58.87783241441205 0.7459587876993702 
1.9271285781878091 3.353164111154612 58.99549803002097 0.7389275995056819 
1.9354745585739257 3.3666583367407608 58.773608149843184 0.7367398664421222 
1.9434287963474082 3.3851879693347695 58.7278207791251 0.7324215762561628 
1.9418476965933862 3.392205591146669 58.54283044908719 0.7286966986710256 
1.9479645513925878 3.399998070606769 58.816456400827555 0.7232639533831503 
1.9743989980925052 3.453365462071793 59.08430566850339 0.7087693240364145 
1.8728543164309794 3.240005442557081 58.51507949375795 0.7087693240364145 
epoch: 29, train time every whole data:183.10s
epoch: 29, total time:7366.79s
fine tune the model ... 
epoch: 30, train time every whole data:368.26s
epoch: 30, total time:7735.06s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.79s
test time on whole data:47.92s
1.5981341762425645 2.6566909720480254 56.1601177758665 0.8264521626701578 
1.6436101375818253 2.754398564471478 57.1067069606351 0.8150235602081083 
1.6807196096046162 2.8466324702496606 57.35779225004006 0.8052040508379766 
1.7030694584348904 2.8950187366120725 57.59757078072598 0.800606005085396 
1.7303022612498276 2.939979940925212 57.85798729801625 0.7978546700812335 
1.759877488036507 2.998115129093721 58.17662212658743 0.790263817692037 
1.78798547531895 3.066789085348574 58.75106753524756 0.7783993325851536 
1.8053184086915461 3.1036248098473274 59.24275749150281 0.7731830409154786 
1.8228550622504027 3.1477974495788303 59.67750697355658 0.7659627547620691 
1.8363376174858284 3.177630353047624 60.09748478161844 0.7590618129020387 
1.8527026698430558 3.2093273700874008 60.67898950935724 0.7522841108912789 
1.893445301320936 3.30444203647817 61.19997719284363 0.7351390206954661 
1.7595298055050792 3.0141901855862763 58.65879772264883 0.7351390206954661 
epoch: 31, train time every whole data:368.82s
epoch: 31, total time:8163.61s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.90s
test time on whole data:48.06s
1.5827976555755097 2.6180237058292253 56.39430489496817 0.8284759158371052 
1.6236728491321917 2.714200807956156 57.036538261468806 0.8173578201206405 
1.6550579820387952 2.786822435444597 57.312346187196596 0.8095430016280861 
1.66953113125513 2.8121590220638093 57.5938881588133 0.8074892072906905 
1.6934523382332354 2.854535850642712 58.15248836648952 0.8030466892841011 
1.7206240172637184 2.9055180027788774 58.703192478990616 0.7954857029877684 
1.7541959311182478 2.9812762401580986 59.455965220225806 0.7838221256514517 
1.784052430165222 3.0509244673682 60.10242363960063 0.774015621737523 
1.8197794016253734 3.1435279760397328 60.570712148867045 0.759390766985298 
1.8447637563260892 3.201149875048625 60.85498734926866 0.7502618021453642 
1.8681113023006668 3.2554157898515546 61.18877836250351 0.7422406562513864 
1.910771014717097 3.359086571999342 61.51808469712196 0.72463221022746 
1.743900817479273 2.9817840428275075 59.07374332879846 0.72463221022746 
epoch: 32, train time every whole data:368.03s
epoch: 32, total time:8592.32s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.88s
test time on whole data:48.11s
1.5876331185290502 2.6322863766008284 55.72301430592781 0.8318443617751927 
1.6323068555126055 2.7352091032317336 56.47000167132467 0.8200065323956518 
1.6658891508342433 2.812274721344701 56.790115313824806 0.8114612105063229 
1.6828293737994418 2.8476269235814335 57.05064062775867 0.8080893172241754 
1.7031560147504012 2.8766461295466925 57.538559815964604 0.8054492185499955 
1.726712613002264 2.919771001390581 58.07014578395072 0.7989560674412285 
1.7493751886341544 2.967693293453212 58.87587299987196 0.7895198745652315 
1.7654709217569657 2.9993472974212914 59.49190518452572 0.7841832593553087 
1.7890421382974655 3.051442505540796 60.1496868101695 0.7747788210437521 
1.8053818809294275 3.0862146156249537 60.509819449060686 0.7676391601288457 
1.8235528634693474 3.126109676901156 61.00069977519378 0.7615586179901346 
1.8673176214082965 3.2281478517770212 61.41468279735065 0.7459443204444565 
1.7332223117436385 2.9447854229139674 58.590535196209125 0.7459443204444565 
epoch: 33, train time every whole data:368.78s
epoch: 33, total time:9020.38s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 29.05s
test time on whole data:48.42s
1.5847089698879668 2.6373731729418854 54.98428728207283 0.8319378406339943 
1.625910084454254 2.7306206506938766 55.556923747376665 0.8221312515365362 
1.6599437761370626 2.8120181334446506 55.68094550162163 0.8148718559616455 
1.6785660140660725 2.8483421190370186 55.858582667378556 0.8126149886308974 
1.6997242573014504 2.8844706446790354 56.31917220528372 0.8088260085378107 
1.7216976156291508 2.92134449467559 56.951374321681115 0.8031846590457109 
1.7459374089270299 2.9721638003914603 57.91962764265893 0.7929407601318248 
1.767901036564527 3.0223107967966802 58.725420512317584 0.7838742762106075 
1.7988391407759239 3.0935769079951756 59.526858967758045 0.7716712160866795 
1.819816141139095 3.1417705296094915 59.78262307522147 0.7638946029195192 
1.840603394736048 3.192926143769328 60.139492073100556 0.7568524415531215 
1.8830081603727524 3.291707492853144 60.43428775002885 0.7426408287376473 
1.7355546666659445 2.968250406259326 57.65674402978039 0.7426408287376473 
epoch: 34, train time every whole data:368.31s
epoch: 34, total time:9449.50s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.70s
test time on whole data:47.80s
1.5835808618924625 2.6278072371154115 55.621919430159764 0.8321573737854505 
1.6221381990883854 2.710061371142678 56.2816108697118 0.8230631595747256 
1.655781271905594 2.788585097361861 56.32258322219344 0.8142974327241221 
1.6720785192425052 2.819734760851607 56.43578554744002 0.8124267155991616 
1.6924782438771355 2.8526435084095465 56.65755639457197 0.8088780136469248 
1.7136677768372355 2.8900322630826643 57.10418985701197 0.8029333892649588 
1.7322317767566335 2.929743222596358 57.91674566228673 0.7940845531410606 
1.7436316240250709 2.949574505070271 58.57517890247098 0.7899532969250734 
1.7624960454468217 2.9957949515751956 59.247388415337795 0.781462904620824 
1.7732956080128928 3.028298510708089 59.488539773857994 0.7755032966452085 
1.7868692249225542 3.05900765452637 59.74355576540687 0.7721592411107127 
1.8239203624735099 3.1491659869981206 59.64721276073235 0.7628146409974859 
1.7135141262067335 2.9035942462543436 57.753606557884716 0.7628146409974859 
epoch: 35, train time every whole data:368.33s
epoch: 35, total time:9876.53s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.88s
test time on whole data:48.02s
1.5766206335736705 2.564313355828961 56.19377111517281 0.8328959944183135 
1.6168416080365755 2.66324417245197 56.35900558978556 0.822517248898103 
1.6506550496002392 2.7474057146364452 56.196065560962396 0.8146239367365891 
1.6673980251612763 2.7842933207163716 56.04021436897659 0.8124581572888928 
1.6892621112982964 2.818804836234676 56.20962400762577 0.8091776027048528 
1.7117189535916384 2.858812443959243 56.557779584394055 0.803048287975446 
1.7334897801680933 2.906284992724787 57.38415299487536 0.793519070125295 
1.7507276434397236 2.9374394739952407 58.02290313735388 0.7880809369492072 
1.7771223821223137 2.999058080861146 58.771262567743285 0.7771921208592636 
1.793948230567876 3.0404067031331237 59.074798749741575 0.770034001998776 
1.8080289501231164 3.079461924291794 59.29521726737176 0.7647910266849419 
1.8468321696779735 3.175436726553596 59.41495827541474 0.7519171092448911 
1.718553794780066 2.886282482461475 57.460049318858374 0.7519171092448911 
epoch: 36, train time every whole data:369.86s
epoch: 36, total time:10304.36s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.86s
test time on whole data:48.05s
1.5766937367864662 2.6190642119621357 54.962986327865856 0.8334217115757284 
1.6172564760345434 2.7133878046341335 55.81511490897773 0.8232393623371809 
1.651373151209915 2.787026554516263 56.242063262129406 0.8157432737646837 
1.6713045045922377 2.82700596606556 56.65849513084268 0.8121041968761374 
1.699400450338654 2.872525020359239 57.28221608875166 0.8074774924602384 
1.7330092628741016 2.939282436797566 58.000378321784915 0.7984470397535425 
1.7662645635303287 3.014287826931867 58.909346869446196 0.7858435373093052 
1.7928197666301082 3.070718744166797 59.57460254541556 0.7773125645437914 
1.827860247108021 3.151850250056099 60.44218951104655 0.7628245696313997 
1.8519426436702766 3.2048216158703116 60.94515943368547 0.753121887574881 
1.8731885454177502 3.2449289689548633 61.43692191983929 0.747062755826239 
1.9139815001976455 3.3447826445783773 61.89932744795333 0.7308266736415633 
1.7479245706991706 2.9903964753726515 58.514198388253824 0.7308266736415633 
epoch: 37, train time every whole data:368.89s
epoch: 37, total time:10733.08s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.90s
test time on whole data:47.99s
1.5590991309712685 2.5480070784515076 57.10340560366479 0.835054577544255 
1.594044106793989 2.634153137958126 57.26107966408854 0.8259585061034685 
1.627828714386693 2.7210870039452333 57.11592919503312 0.816656761871682 
1.6459663970823444 2.7599597982636657 56.96856240439742 0.8142762515424165 
1.6749678177685432 2.8129555492149683 57.18149065351958 0.8089607093221737 
1.7028227254211725 2.874241479916669 57.438554816434554 0.8010307941958507 
1.7302696598459568 2.9369358521507287 57.999715899186455 0.7910277297412139 
1.7466174831092358 2.9697080782326712 58.36734503487766 0.7864493018782857 
1.7682873671500987 3.015816004199595 58.943226915450076 0.778134762099339 
1.7828149692329267 3.0475325154212145 59.31346311037464 0.7713721845930661 
1.7968106657231138 3.08106799292158 59.80682161135066 0.7651736445624087 
1.8310006650864545 3.1584253777228937 60.00154834567526 0.7541939624586763 
1.7050441418809832 2.8856363844741453 58.125150522372635 0.7541939624586763 
epoch: 38, train time every whole data:367.68s
epoch: 38, total time:11160.57s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.79s
test time on whole data:48.04s
1.5681252612162027 2.59658823990993 55.298905464618784 0.8347379121713052 
1.6064084038339732 2.6895262701700227 55.650334666409364 0.8253822520293096 
1.6412452971092648 2.7599234690864525 55.54389559801125 0.8197026738687505 
1.6577364007781836 2.794154119331825 55.78236929347287 0.8165284077618948 
1.6757419946436727 2.824688098683999 56.3493214997412 0.8112293272594125 
1.691327728541834 2.8504945067941767 57.0400911612188 0.8051374909594822 
1.7106387299780867 2.890447066414376 58.14304831817747 0.7955638089255646 
1.727491603774329 2.917031497930276 59.1151801893373 0.7898459049060544 
1.7556685652798485 2.975236059679491 60.16452863860422 0.7794045230470619 
1.773616319249606 3.0196438758274153 60.53900049327201 0.7719045695902329 
1.7870118434535605 3.050586412503819 60.64739229267514 0.7684892121778784 
1.8237225368828291 3.138429646616613 60.61448145876943 0.7573886241593363 
1.7015612237284492 2.8794434139501703 57.907501338211034 0.7573886241593363 
epoch: 39, train time every whole data:367.52s
epoch: 39, total time:11588.32s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.77s
test time on whole data:47.84s
1.5804275567170587 2.601049331039552 56.199270081976714 0.835829555930193 
1.616650212183772 2.684802051058353 56.93392729182726 0.826155732479176 
1.6459242172461181 2.735453136575046 57.358454095561775 0.8196518803874898 
1.6579811941196343 2.7519854410915348 57.755671296383596 0.8174348340699727 
1.681240557911939 2.7894082248442422 58.185402407704586 0.8128027781436333 
1.708512901372498 2.8410603778180126 58.619521156102074 0.8072523568241676 
1.7388557037907164 2.9072879106450116 59.2721424481103 0.7979553876039333 
1.7585620615329771 2.946112037840537 59.78660812096557 0.7928003047497745 
1.7816746284544822 3.0010672032352725 60.53374397138587 0.7828104865999276 
1.7924557666411358 3.0258590733232364 61.054662215124885 0.7767836173676618 
1.803740185879437 3.046842163318143 61.63897747863658 0.772309674276349 
1.835185502522226 3.1190595419349862 61.91185609466381 0.7617246097050607 
1.7167675406976661 2.8750083469535355 59.104290613913825 0.7617246097050607 
epoch: 40, train time every whole data:367.55s
epoch: 40, total time:12015.45s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.94s
test time on whole data:48.11s
1.562397141686951 2.5538949083997666 56.44960582463021 0.8358275034173256 
1.5963134481649668 2.6391846157046537 57.21127481035202 0.8251033472670334 
1.6286199173914002 2.7092398419910086 57.47557625164228 0.8167785904675208 
1.6434157226184116 2.7446588802482563 57.80218719968594 0.8127535658500405 
1.6624849050958597 2.7690271729339475 58.33105374907066 0.809935771340105 
1.6863803497477479 2.815616154914639 58.9817709603422 0.8028906367850426 
1.7114111398267782 2.8658954209261083 59.820900962305736 0.794171134273345 
1.7322393909302496 2.9052513894590537 60.42279752963312 0.7879447624171614 
1.7617418827634481 2.9749548407479214 61.05352312996334 0.7759896664231353 
1.7831902210348773 3.0243527346193395 61.241289694005886 0.7674177630197958 
1.80105886866445 3.0671118338261536 61.32986019504567 0.761006475315584 
1.832720448244363 3.1507276802633792 61.15238997828716 0.7486464414963551 
1.7001644530141253 2.856880309437852 59.27278953065523 0.7486464414963551 
epoch: 41, train time every whole data:366.81s
epoch: 41, total time:12440.61s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.88s
test time on whole data:47.91s
1.558533888311968 2.5560453289989695 56.59709272275945 0.8363129482752862 
1.5874316340497738 2.6227367392640755 57.11905158271724 0.8283897876446737 
1.615559153004239 2.6858480903681516 57.10914613534624 0.8210750034097823 
1.6319907316374813 2.719389351215501 57.25107185101385 0.8184035661698605 
1.655004700987378 2.7589025400504403 57.64680554184432 0.8142213068843778 
1.683059048502839 2.8178514830456827 58.26318746077656 0.8061251343021825 
1.7115337171237028 2.8773597618093993 59.16380290372781 0.7964913835837106 
1.7315371137763418 2.9193175319776152 59.94320990039339 0.7898588871510148 
1.7586113208491532 2.9805865453700933 60.783015575728996 0.7794941005200315 
1.774384887437824 3.0150263261842944 61.10576115222707 0.773614884603127 
1.784594874088342 3.0402185843198763 61.209637559419384 0.770360532426208 
1.8137720193232276 3.1115512661326603 61.16538462161265 0.7609998955529599 
1.6921677574243559 2.8471239672263144 58.94653188648144 0.7609998955529599 
epoch: 42, train time every whole data:367.14s
epoch: 42, total time:12868.04s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.84s
test time on whole data:47.93s
1.5717455197374026 2.616398005960133 55.16528718284358 0.8348126083896016 
1.6052478797100485 2.6878892281760915 55.90498432122962 0.8265924666633158 
1.6325145555491603 2.73891872896648 56.24623431405031 0.820417488298462 
1.6454996037678349 2.7650156084086785 56.642823246182715 0.8175453808347161 
1.6668415184297731 2.7974868267987127 57.19187792246356 0.8140289698896855 
1.6932911794608725 2.8520674567302873 57.82914574060663 0.8067757806303333 
1.7254122109924932 2.9181722011915965 58.75763459020283 0.7966194859754431 
1.752630200976772 2.9743456759501945 59.55488305443131 0.7885744175775814 
1.7870347162090419 3.0510514880063155 60.41542515875725 0.7757174685175219 
1.8069597232921848 3.0974670407091467 60.833016948398125 0.767337161374513 
1.8238127528567398 3.132846826524499 61.22154797793474 0.7615238657859811 
1.8551550945793944 3.2088375302213303 61.41481456765684 0.7503353390975175 
1.7138454129634766 2.90910268885204 58.431598018990485 0.7503353390975175 
epoch: 43, train time every whole data:367.03s
epoch: 43, total time:13295.93s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.74s
test time on whole data:47.87s
1.5517288881461357 2.547452651810415 56.10062571000589 0.8372960080306716 
1.5796077045055905 2.6092655579811024 56.834870058890196 0.8292861893346786 
1.6052622220604016 2.6640190926730964 56.929498686924376 0.823016189012842 
1.6173389170895376 2.6902384056598776 57.04914326463528 0.8211007199504462 
1.640048197586178 2.7306387218838437 57.393281187307785 0.816819347113558 
1.6661756109258248 2.7888255895255893 57.92010536161784 0.808833854698268 
1.69568208971593 2.8492841543954857 58.721117402538425 0.7999198017363569 
1.7198203343999172 2.902586120682704 59.40904709705034 0.7922296638627172 
1.7544550712107725 2.9784634519621576 60.2228771636661 0.7801069385720791 
1.77425669644365 3.0222038861495273 60.499762458199235 0.7733518423014588 
1.7898406756752658 3.0583320258377507 60.6491868948847 0.7685175474604609 
1.8223110953304207 3.1346890109708134 60.67591726425871 0.7587376737335574 
1.6847106252574686 2.8371663590824143 58.5338809592732 0.7587376737335574 
epoch: 44, train time every whole data:367.05s
epoch: 44, total time:13723.87s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.76s
test time on whole data:47.76s
1.5619057319455203 2.5841824314857003 55.038708526838995 0.8379458540929371 
1.594558034267099 2.6669316898068702 55.54281541915137 0.8294704048844991 
1.6242210973337114 2.727975324916641 55.70687917714638 0.8231097759329982 
1.6401149006683735 2.7627180836993896 56.00090952493756 0.8201168506864779 
1.6639659465443166 2.8018101127402906 56.55758355301986 0.8153743936951899 
1.6888466335128816 2.85169775449028 57.20787242708515 0.8073589009139094 
1.7145044631325594 2.8963321574106855 58.150912265220946 0.7990465973230768 
1.7334415301276105 2.929445890920691 58.85986094779593 0.793111919497249 
1.7603060575121392 2.992022964224286 59.57802890504363 0.7819244054228224 
1.778098973274231 3.035156550477569 59.821059872936566 0.7749429156980813 
1.7912627611566512 3.072422853620088 59.744465092590715 0.7708954472106615 
1.8244162857340915 3.148678676780077 59.65257320482691 0.7630371841835125 
1.697970201267432 2.8771518548551915 57.65524593229024 0.7630371841835125 
epoch: 45, train time every whole data:366.98s
epoch: 45, total time:14152.20s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.80s
test time on whole data:47.97s
1.5640405811498916 2.5827344555882386 55.70350817580363 0.8365545714769022 
1.5971484347627987 2.6539618223361936 56.45695699815473 0.8280392171343351 
1.624996536342161 2.7050932722630168 56.82222753174327 0.8218841433642009 
1.63742175019426 2.7283376605394816 57.11969883603022 0.8196642272104935 
1.6609234163710582 2.7688013294164437 57.538327238086076 0.8152259856019561 
1.6870429997545269 2.830630450085954 57.93457996369147 0.8070338295097762 
1.7141077686683053 2.89203128768765 58.74529641354501 0.7964880102976996 
1.7306101212931708 2.922905082406608 59.52444983758301 0.790531845472517 
1.7568108635670727 2.9760343998827765 60.66596178946171 0.7800165820274451 
1.770384604485262 3.0064556964822398 61.368787739980746 0.7734520040045105 
1.779739744709095 3.0268358159102378 61.682876385356444 0.7701992933157894 
1.8078762121974004 3.081915176825065 61.596414526755986 0.7637224784624038 
1.6942585861245836 2.8521458015316297 58.76337426835296 0.7637224784624038 
epoch: 46, train time every whole data:367.50s
epoch: 46, total time:14579.85s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.77s
test time on whole data:47.86s
1.55041621136417 2.543360452386347 56.1316493754595 0.8384802279843844 
1.5759975950868712 2.604623368036639 56.89249300227577 0.8306405833548215 
1.600025929282641 2.6544654323388936 57.1471873454066 0.8245627928288727 
1.613925151791601 2.6852987684155933 57.40588616641198 0.8212150368817203 
1.6393448677202243 2.7291135918026868 57.866540990822244 0.8165035589520493 
1.6730913725333023 2.8066316079674554 58.36837784462008 0.8069749884911216 
1.71093496368311 2.8894692799074706 59.18542281755062 0.7954106878439449 
1.7441701620078336 2.9630212092631822 59.90604773916813 0.7848646773533423 
1.7825118518493006 3.049924852220754 60.77583281263633 0.77058538171912 
1.8075762024953783 3.106794590616574 61.2754279230377 0.7600578494539866 
1.826119700362906 3.147222382165161 61.681235507275964 0.7529622336933613 
1.8546253468218659 3.209017975865506 61.83134215020038 0.7435775381856622 
1.698228279583267 2.8740295492247743 59.03906472346736 0.7435775381856622 
epoch: 47, train time every whole data:367.89s
epoch: 47, total time:15009.98s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.88s
test time on whole data:47.95s
1.5685338819452694 2.6027834115463704 54.52050829942344 0.8381665622008528 
1.60324267694017 2.6818593216099123 54.921268262882975 0.8290112326919992 
1.6290582235706526 2.731781652639682 55.064927650278285 0.8228640087348242 
1.6436804206149565 2.7565192030485077 55.31291880155111 0.8206614161784404 
1.6661329706317434 2.7998015090011097 55.89468358358701 0.8144675805884235 
1.6903965480302772 2.8444179744759435 56.60384055720258 0.8077929763891389 
1.7165452657421785 2.8919821154820204 57.582556973404685 0.8000889534103847 
1.739365572730168 2.9360378068844213 58.41119373977715 0.7931050190204985 
1.7675134127539183 3.0001865052291286 59.29618400383983 0.7821700106564449 
1.7820752846486867 3.033273440300869 59.7471943308878 0.7760808662509336 
1.793785238931665 3.0592720187654905 60.1229936385883 0.7717828838352837 
1.822436380276545 3.1209451888358206 60.294846483849796 0.7642174469369982 
1.7018971564013525 2.875799478292411 57.314548081896476 0.7642174469369982 
epoch: 48, train time every whole data:367.33s
epoch: 48, total time:15436.27s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.74s
test time on whole data:47.84s
1.5509228340539016 2.5352109274320203 55.811888434744596 0.8386753898111594 
1.5804094987739588 2.607464559910252 56.53558326397382 0.829647422306769 
1.6098190762688893 2.672726114776515 56.687092600445446 0.8226631860105723 
1.6305333999315543 2.716670966829763 56.88065220424971 0.8194302685581236 
1.6639469731581353 2.7875418804011525 57.368377786530985 0.8126901227487172 
1.706198965462723 2.8847583368483387 58.00145545352397 0.8008354581720193 
1.7469109803975693 2.981599962849821 58.783533464535495 0.7868780043123503 
1.7801817371221702 3.059757122193302 59.47327616548114 0.774916706004865 
1.8152138384309198 3.1408183252180297 60.259838364103594 0.7601679282719247 
1.8376273792723992 3.1874453436862247 60.74548450394857 0.7501975356303815 
1.8521512354212326 3.2135951341887483 61.253172003430436 0.7442763250894089 
1.8769795367957225 3.2619821643180633 61.63997738285241 0.7352139623993926 
1.7209079545907646 2.9309622024244724 58.620138043988156 0.7352139623993926 
epoch: 49, train time every whole data:368.08s
epoch: 49, total time:15864.38s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.82s
test time on whole data:48.00s
1.553979907605531 2.557113404513116 55.14745913230681 0.840116187874496 
1.5849314160056058 2.633808446561984 55.60103549776729 0.8317591678151697 
1.6148544359195622 2.697668369252596 55.75792749451398 0.8252994489524247 
1.6325065621161567 2.736583728646866 56.11652990774086 0.8214579197476193 
1.6570403331174914 2.784378813514693 56.74876774895489 0.8158669602676417 
1.6829140683717672 2.8352739232783355 57.51305757536939 0.8083227637622911 
1.7079080015944228 2.88248262664381 58.53745961350516 0.7998790285647197 
1.7315417574498626 2.9322945456487064 59.41244546590482 0.790802728474039 
1.7609051238403965 2.995051422338138 60.36945296036831 0.7791912344858019 
1.7800381035961743 3.0347921152263555 60.848718107479385 0.7716969317468058 
1.7932319749334737 3.064430687693956 61.04214959386738 0.7673639678490483 
1.8203452056125693 3.1253328174539194 61.05737019054751 0.7597310968176338 
1.6933497408469178 2.8618091435346664 58.179494177544264 0.7597310968176338 
epoch: 50, train time every whole data:368.39s
epoch: 50, total time:16292.39s
predicting testing set batch 1 / 168, time: 0.30s
predicting testing set batch 101 / 168, time: 28.89s
test time on whole data:48.12s
1.5634883677278246 2.593416289502916 54.62507936105779 0.8376647126368952 
1.5910655689882558 2.6482428525980275 55.24491001097055 0.8313523933955875 
1.617386490098511 2.703200679129982 55.41454228634121 0.8252488090471859 
1.6333145656657537 2.7338618976314324 55.69554371656798 0.8219239918351827 
1.6584334116013986 2.7845600698948156 56.11976271464276 0.8156719548561908 
1.6826861057831417 2.834386250473015 56.66857272506002 0.8082253373334456 
1.7084375099969051 2.883994622167324 57.642345089015066 0.7998071855528365 
1.7307749530529337 2.933459678692737 58.47274613479576 0.7921899177693792 
1.7620763116422153 3.0066784638814634 59.387917441658075 0.780273871292844 
1.785605802674822 3.063175709065098 59.93194532023989 0.7712439219082472 
1.8023871166220025 3.095647771933472 60.15388566523734 0.7682293849098197 
1.8375480150380836 3.1759284678752757 60.281761879150906 0.7586692509256443 
1.6977670182409874 2.8769651469561763 57.470033792648124 0.7586692509256443 
epoch: 51, train time every whole data:367.46s
epoch: 51, total time:16721.46s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.77s
test time on whole data:47.91s
1.5661649447263528 2.577155863052118 55.44159444952727 0.8388875947893377 
1.6085828965634463 2.678736839780659 55.7286178749545 0.8295305438053604 
1.643745542211192 2.756920285817838 55.81002547942744 0.8232456970784194 
1.663289833297953 2.795541779001961 56.027763568639955 0.8209689386560582 
1.6893597245384895 2.847130848319338 56.546008176646524 0.8154755101651149 
1.71429594527318 2.896175790644508 57.11173362214594 0.8083397720391312 
1.7401099502830988 2.943761220505013 58.006411242911604 0.7993990335166681 
1.7602736785156385 2.983764442053852 58.75740214545216 0.7916234842387581 
1.7860784305028972 3.040815872732827 59.662494424835124 0.7798227345623637 
1.8018677994776517 3.075483447499831 60.19921233899147 0.7723131504257293 
1.8145921691923093 3.0962747322523363 60.52333147234301 0.7694043901328268 
1.8454961459435346 3.1653305350602627 60.797293350106926 0.7604320656000664 
1.7194880883771453 2.909853798847703 57.884434802878296 0.7604320656000664 
epoch: 52, train time every whole data:367.00s
epoch: 52, total time:17148.45s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.84s
test time on whole data:47.92s
1.55305684211726 2.555667292590405 55.74363477744348 0.8390986164290118 
1.580261150329152 2.6159274416668845 56.432659982861864 0.8314836870804768 
1.604771743306358 2.6669986454220656 56.66618312017967 0.8256467760590224 
1.617761411368226 2.693938000984531 56.96450061749218 0.8227680513510958 
1.6386330905064408 2.734852054275192 57.46562526997187 0.818106297929353 
1.661741330279126 2.781648774230747 58.031927505278546 0.8118290341190636 
1.6878646493135463 2.8343956547986418 58.7997472863102 0.8038888749424772 
1.7106942278290433 2.882473271526514 59.42419050559684 0.796963017391877 
1.7400738510380367 2.950602761459244 60.15087198086424 0.7859643242194871 
1.7610946697073855 2.9975464792951567 60.52852323801042 0.7782172906411052 
1.7781060364182506 3.03824533896748 60.764347023005605 0.7720078762986984 
1.8085795133208766 3.104851492953323 60.89092966245939 0.7630480253578458 
1.6785532096278084 2.8264599627713127 58.48869942216343 0.7630480253578458 
epoch: 53, train time every whole data:367.10s
epoch: 53, total time:17575.50s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.79s
test time on whole data:47.89s
1.5514539147357322 2.5430508640913563 56.50277841396931 0.8392996940009291 
1.5857820829761524 2.6287609365027627 56.93477017111128 0.8300916786738634 
1.6165204490654703 2.694951053875811 56.965755267943386 0.8228566896578892 
1.6291549046389937 2.721450150508529 57.2380122367531 0.8197319997553313 
1.6480140120937001 2.7557281995969483 57.71899096810982 0.8148769125626392 
1.6677077703216068 2.789788976014147 58.339683813219914 0.809309284800525 
1.688891842078861 2.825264919065281 59.29137502430736 0.8024648949005724 
1.706821649183741 2.8633241712655715 60.119780840188106 0.7951930078541923 
1.7309931143540003 2.9142968356305965 60.98966683988173 0.7858400564162866 
1.7465541833206302 2.9476897608141166 61.40260023345965 0.7798460441164949 
1.7576071048441033 2.979425178000909 61.31198609212478 0.7756527131023787 
1.7836818965474766 3.040606426845593 60.905188782700336 0.7690196957601291 
1.6760985770133723 2.8123020564859575 58.97682353822411 0.7690196957601291 
epoch: 54, train time every whole data:367.92s
epoch: 54, total time:18003.52s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.81s
test time on whole data:47.99s
1.5561359241791957 2.577881325223093 54.749879173747196 0.8395893440484672 
1.5936365989189418 2.6680724461786953 55.1658600907342 0.8307685961563265 
1.6290598051993266 2.7455773018258034 55.38523620232814 0.8233441349285769 
1.650255368122565 2.7897642850295528 55.60407907512297 0.820113818988187 
1.6855219307727225 2.8627408040783724 56.15737728918209 0.8128947569959386 
1.7198932552045831 2.9338403906183075 56.677444077186635 0.804409885071976 
1.7553895296216722 3.0065669728214575 57.61161866084829 0.7929100738336662 
1.7843327518603098 3.0644901308382817 58.47150102897248 0.7830407175166831 
1.8157884011931185 3.1279462976631396 59.55898982339293 0.7702304231983624 
1.8380268221675817 3.1753066783216415 60.314857864087315 0.7591652488501319 
1.8545085694184082 3.2100996844107774 61.002256710099736 0.7514838913716465 
1.886801051449208 3.2772494271069013 61.5537581424982 0.740191793420397 
1.7307791673423027 2.9612783781970653 57.68786623171582 0.740191793420397 
epoch: 55, train time every whole data:367.48s
epoch: 55, total time:18429.88s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.74s
test time on whole data:47.81s
1.5618268936418351 2.563936642695131 54.31631487066926 0.8400963762900283 
1.589545388330395 2.6258504247856966 54.68935768988318 0.8331393650027494 
1.6158619664416072 2.6719886456441317 54.869449962112924 0.8276307784639958 
1.6287014295312443 2.6946298326561324 55.1007927443892 0.8252880211866003 
1.651958903764419 2.740955829738398 55.624807757088824 0.8189143785770934 
1.676083604549457 2.790467337262737 56.08420758719349 0.8117400384099663 
1.7013816494617966 2.8330740102485477 56.87390148758206 0.804814037261794 
1.7196835606374912 2.876149426184455 57.408690562102095 0.7975196562850058 
1.7410866791801083 2.9274091481153004 58.01299610488876 0.7880842805150235 
1.7513043192650768 2.9544814091124705 58.27387400363888 0.7829687369108792 
1.755754355658644 2.973318834639185 58.3331305703644 0.7803724855287063 
1.7750259659893457 3.0232046868724063 58.225345806574616 0.7754624876395689 
1.680684559704285 2.809901862123709 56.48449486452372 0.7754624876395689 
epoch: 56, train time every whole data:366.61s
epoch: 56, total time:18858.21s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.73s
test time on whole data:47.81s
1.5496908308440553 2.554105585747804 55.5176945265973 0.8391817034468272 
1.5799299530169617 2.626111510317017 55.98336373588438 0.8316402448413417 
1.6028374748718703 2.670380863909927 56.32954155499108 0.8259111297833034 
1.6111751769648066 2.6812697235248666 56.89824440917568 0.8231352914323959 
1.6270111584780471 2.7083275030227734 57.61065719683257 0.8183913339557622 
1.6495052305412967 2.7543346352952964 58.355856994438824 0.8114142598246938 
1.6759489072755511 2.804912037216739 59.285996011295225 0.8040643052013215 
1.702062616873089 2.862175518618452 60.01185545596979 0.7959008749727189 
1.7315026143174619 2.9293310758603797 60.716088117709724 0.7852922242734728 
1.7504382640073697 2.9728145077789545 61.0699607260006 0.7779814477132309 
1.7646385181199404 3.010441566460221 61.247285605920666 0.7719037665729214 
1.792829623212417 3.072794172079449 61.33771076916018 0.7627279553185612 
1.6697975307102388 2.8083872118147175 58.69714770283282 0.7627279553185612 
epoch: 57, train time every whole data:367.59s
epoch: 57, total time:19284.63s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.93s
test time on whole data:48.17s
1.5396715546763013 2.5169387346908145 56.34813525987169 0.8406157181833632 
1.567631228472594 2.582716280510477 56.62279531445552 0.8338354753014203 
1.5965135108899502 2.6479320269709445 56.73012479484456 0.8272941390111582 
1.6151762191572536 2.684220955197304 56.98214668934687 0.8246188536175915 
1.6446170433127276 2.741492398313918 57.57716955643534 0.8182888955316876 
1.6738152648797702 2.810025167235896 58.27106122868808 0.8089058377550005 
1.7086978255811902 2.8834176707135724 59.33099797640008 0.7977549108592731 
1.7363498725515922 2.946484760073652 60.12337871412049 0.7881790111786802 
1.7732915584958557 3.0329667268114107 61.02888121450749 0.7735865577324427 
1.8010704055401896 3.095782878938215 61.51521653398333 0.7633101462140722 
1.8211888974300985 3.1402558952468795 61.71157085297302 0.757043536177769 
1.8536759505004932 3.2135915584893544 61.97688200250967 0.7463280347658467 
1.6943082776240015 2.866487668487927 59.01831792615735 0.7463280347658467 
epoch: 58, train time every whole data:367.38s
epoch: 58, total time:19712.47s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.97s
test time on whole data:48.14s
1.5445814745247008 2.5235265871530257 56.64577081490846 0.8405448124802428 
1.5734904919561177 2.594705181580072 56.99723270161989 0.8329249728549959 
1.6019131661771486 2.6529284729775524 57.170755307387324 0.8269633974707025 
1.6194329560099259 2.6857363088270403 57.5600115268466 0.8239556094290287 
1.6462710752370988 2.736450116575569 58.1603029563604 0.8177082931759069 
1.6716669852918102 2.7926853915972676 58.71871324326147 0.8097176122312099 
1.6964626961173934 2.8359504410514025 59.57811798767286 0.8022545275745043 
1.7119794379938393 2.8693711360360905 60.28044610247204 0.7955170930996242 
1.7306030915791968 2.914774484004993 61.22732862787792 0.7858832238464328 
1.7420630792339467 2.935976326931445 61.98566418846529 0.7805757850147121 
1.7498235747893118 2.953964289813819 62.492041954280666 0.7772246998792837 
1.7690070453657813 2.994671313448852 62.57589710563979 0.77095524076182 
1.6714412561896892 2.7947104137703125 59.449477798748994 0.77095524076182 
epoch: 59, train time every whole data:368.44s
epoch: 59, total time:20141.24s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.80s
test time on whole data:47.96s
1.5591183276085094 2.567620328918206 55.86111501391298 0.8387080164440396 
1.587849477583986 2.6311316317921327 56.76209189338943 0.8305742389971177 
1.6105478851077635 2.671652672183722 57.240150090609085 0.8252743348527752 
1.621867579850324 2.6863013210017317 57.635446332041255 0.8235043356772304 
1.6424584336357102 2.730661128294955 58.02881857160613 0.8182035948083388 
1.6658437223991467 2.7838967390336538 58.28148299049255 0.8117310640878085 
1.6932416142761886 2.836615746230993 58.79323719704822 0.8046438210734091 
1.716115441391067 2.8872018958819443 59.17252024127102 0.7973126945319512 
1.7404455204814495 2.9427855625888872 59.74679251120942 0.7876156027519134 
1.754941694780386 2.978095835221524 60.08197537969628 0.7803345718210719 
1.7637285030805283 2.9981414216079543 60.23611087514439 0.7767248885340082 
1.784232073772876 3.0462356678271973 60.23023618658512 0.7704529151005065 
1.6783658561639947 2.8174431513767315 58.50591415625348 0.7704529151005065 
epoch: 60, train time every whole data:367.44s
epoch: 60, total time:20569.93s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.79s
test time on whole data:48.11s
1.5395791075461145 2.5198555909110074 56.44694129562051 0.8410119649895651 
1.5677089524260235 2.5883620635617257 57.25065557407195 0.8323349370639741 
1.5960420864514475 2.645176918038794 57.706219080821555 0.8254396067430713 
1.6128182319296258 2.6731410413674497 58.107572740500466 0.822682397101251 
1.6441571338757694 2.737422289599393 58.64945606165273 0.8151045890992754 
1.6808066257578986 2.825079960179552 59.180425351922224 0.8033745360601262 
1.7193363991101227 2.9129196371916026 59.946159338085536 0.7903218152884719 
1.744665464748052 2.9717438743178795 60.54237890880101 0.7808062282808373 
1.7694741373429341 3.030301654004079 61.33001766956051 0.7696061592670526 
1.7848445911168875 3.0600734448473306 61.92298487474554 0.7631209965346515 
1.7943381325442875 3.074575139310383 62.28649765219188 0.7603210533235728 
1.815545035446595 3.1189188373427466 62.4514671489879 0.7534509964444778 
1.6891096581913132 2.853510980200559 59.65184702254979 0.7534509964444778 
epoch: 61, train time every whole data:366.56s
epoch: 61, total time:20995.66s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.88s
test time on whole data:47.98s
1.5379724731484339 2.511785526992325 55.910133035847565 0.8414945766659561 
1.5617956543312008 2.5693815809841856 55.93972416079872 0.835565932737856 
1.589331953148668 2.625445102541747 55.924484761860825 0.8291829073436918 
1.6027634320886184 2.655687753148348 56.14443677072957 0.8261023312088795 
1.626718926492014 2.706015338713683 56.766713371073685 0.8196147302455045 
1.6489502405747771 2.7533814341682272 57.50638173322544 0.8125792183959529 
1.6721270090444458 2.7912307206562343 58.41444752729777 0.8067565547117387 
1.692030249752814 2.8327853103894887 59.05002395145259 0.8008922505061377 
1.7177803007906214 2.8940259218948756 59.67493742624993 0.791672263813719 
1.7362064990424328 2.943321739515152 59.88997237342707 0.7846423874216896 
1.7529766547507828 2.9867460187749524 59.94573443801917 0.7797375229832817 
1.7848321744956608 3.0613938227646287 59.991953069832306 0.7708038934423186 
1.6602904639717058 2.7824900475217706 57.93001145733828 0.7708038934423186 
epoch: 62, train time every whole data:367.42s
epoch: 62, total time:21422.12s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.87s
test time on whole data:48.03s
1.5368134621157354 2.5042841536291305 56.259456722046885 0.8410293394090979 
1.561751475116238 2.5646373487853773 56.823617351849485 0.833766246796026 
1.5854163810675521 2.6139115266158957 57.10228190029816 0.8278201520926158 
1.5978641546985932 2.636888935252596 57.254366497526874 0.825804057576041 
1.6230303341023447 2.6915896246364728 57.57955280202089 0.8195996618188583 
1.6507842203136533 2.763057366117573 57.840022599063914 0.8107276117941645 
1.683404584362837 2.836576689966529 58.33224287759784 0.8012589780044387 
1.7116604503487192 2.903668603992137 58.63769972513114 0.7924553202075966 
1.7450598015906733 2.9809455133275233 59.2059080012677 0.7804555411688975 
1.768663823761312 3.0370480802556585 59.49135904978571 0.771036678123083 
1.7859895610774734 3.075808666546933 59.824308501839916 0.7642620355210218 
1.81485500263609 3.1356479047072088 60.28307415719093 0.7544066770289888 
1.6721077709326018 2.8194813929933145 58.21956096720865 0.7544066770289888 
epoch: 63, train time every whole data:367.17s
epoch: 63, total time:21849.62s
predicting testing set batch 1 / 168, time: 0.29s
predicting testing set batch 101 / 168, time: 28.81s
test time on whole data:48.03s
1.5339698839271885 2.505725900274397 56.210833688775075 0.8421561899554527 
1.5590606399761247 2.5618246920647296 56.41771772728777 0.8356462043834464 
1.5851741311171403 2.6155164448155284 56.57045366659683 0.8294411309601769 
1.600559050409212 2.652120779004213 56.784256275371256 0.8257620240664345 
1.628431041705023 2.7137228496576227 57.27180440672824 0.818793640209785 
1.6594038024367321 2.7847323532935593 57.8721372341801 0.8099781733897732 
1.6908142517868074 2.849785206008182 58.732895249660864 0.8008112587689038 
1.7140835324876187 2.8997943760994622 59.49700379391747 0.7925858767960247 
1.7388005470082697 2.955372995846316 60.362785333380785 0.7822202697117615 
1.7549770161019904 2.987438351040008 60.949141662070005 0.776002934898838 
1.767602707879263 3.0150602180755905 61.32098429585031 0.7715970324979301 
1.7926321769270692 3.0644532328805645 61.48517695282947 0.7649659181087979 
1.6687923984802033 2.806330154741779 58.6230431217046 0.7649659181087979 
epoch: 64, train time every whole data:367.11s
epoch: 64, total time:22277.42s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.68s
test time on whole data:47.70s
1.5355291174021328 2.4916057966283667 56.97275713661563 0.8423000461192941 
1.559411890855059 2.550649003817805 57.330893355971746 0.8353998673817079 
1.5873867067379788 2.6129517168045435 57.298985490218826 0.8289544123786435 
1.6067515148416693 2.6601584884889626 57.41719496527566 0.8246077119924182 
1.6356090794344034 2.715978099393273 57.842070156764116 0.8188899243582605 
1.66564428288188 2.781076878688434 58.412191508639 0.8105241674340964 
1.6989219171015457 2.8474172636533823 59.36635344018644 0.8007683315124087 
1.7256070413066162 2.9092214486715657 60.163620492967304 0.7909077471580661 
1.7563746803149227 2.9815938102086905 61.08285082702739 0.7787250183359087 
1.779586220225408 3.030072230685766 61.69273694605933 0.7711220797436678 
1.7978816648313687 3.0705556515305434 61.95343493936609 0.7659195198596271 
1.830801443889826 3.1473040940200514 62.199176116890186 0.7554038345773492 
1.6816254633185674 2.8240813290733913 59.311130429592 0.7554038345773492 
epoch: 65, train time every whole data:365.67s
epoch: 65, total time:22704.47s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.74s
test time on whole data:47.82s
1.538658222263058 2.5122564225783344 56.27857488829415 0.8425706062451018 
1.5657520506342961 2.575006651246497 56.910573083097034 0.8353187789313332 
1.5940576975913274 2.6296709936233063 57.36439129002989 0.828909415557893 
1.6114529973041443 2.6646032013061878 57.883055526350866 0.8251457484474298 
1.639211417199423 2.7198109240242707 58.49500805426858 0.8189091650595455 
1.6712428016944656 2.790699072598563 59.11543008026408 0.8096082542495652 
1.7065357848835134 2.8683714160921836 59.93172686686331 0.7982392529961648 
1.7344691961280825 2.9321025875096383 60.6091429500232 0.787896059422034 
1.7623574945492049 2.99828467897556 61.46559471977893 0.7758679961050654 
1.7825124001534922 3.037078978279973 62.130551735240246 0.76850032760349 
1.7945265269301653 3.060533729034148 62.713538333430506 0.7640182055530549 
1.8180515129928079 3.103771684148717 63.30455504478749 0.7566390598334155 
1.6849023418603317 2.8310973931324033 59.683642320459526 0.7566390598334155 
epoch: 66, train time every whole data:366.23s
epoch: 66, total time:23131.43s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.64s
test time on whole data:47.67s
1.5330780866804223 2.498632981556733 56.130608970267026 0.842403840789419 
1.5605879393252766 2.5686717321118513 56.263468276645234 0.8345437665248662 
1.589192168847702 2.6318000751413457 56.193108096120326 0.8285459521210652 
1.6088584340459533 2.680366862662238 56.326828821482856 0.8254466861291588 
1.6448843654412777 2.759137545484817 56.85564338122443 0.8183459254431907 
1.6815066002586945 2.8333684792800344 57.496626702938805 0.8100149499187979 
1.7197242002371875 2.9082924458307886 58.49935408124421 0.7994571837684585 
1.7525783651282212 2.980543655822726 59.34820729656399 0.78771620596067 
1.7879394409878269 3.064939124989696 60.26528998042251 0.7719998393737361 
1.8109467703699178 3.113277467617149 60.849719455341 0.7629858520478067 
1.8257297426569497 3.145029091410112 61.31820821218952 0.7572345432769141 
1.8529299419665974 3.194581815928225 61.97385328175227 0.7491099979478258 
1.6973296713288355 2.873985490012215 58.46019295285202 0.7491099979478258 
epoch: 67, train time every whole data:365.84s
epoch: 67, total time:23557.64s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.69s
test time on whole data:47.73s
1.5439299831594384 2.5402928137437533 55.967772967947205 0.8408633284388003 
1.573581365310454 2.6087579225523494 56.56311534886027 0.8337723121781008 
1.6071732947097292 2.6772887807337495 57.20279698317088 0.8258533534465019 
1.6314716936849234 2.728726697254509 57.662532476621905 0.820584789736837 
1.6738849009744645 2.8137722792483726 58.394357664908014 0.8110159317375534 
1.7223338866388158 2.9259892743769735 59.1333663311466 0.7962117306620881 
1.774164673345252 3.0500198219211025 60.05665490919405 0.7774486094672859 
1.8112021641209721 3.140884354370191 60.72733872159674 0.7618465084561024 
1.8402146679245468 3.2085885634626115 61.461295631617574 0.7481731138944471 
1.853605944805468 3.2290795642687034 61.98618092714179 0.7425183296599629 
1.856622191672435 3.221259580337707 62.35230491950817 0.7424547065049262 
1.865372011322262 3.233431197735184 62.750030654220105 0.739182599820681 
1.72946306480573 2.959062621746825 59.52161328385319 0.739182599820681 
epoch: 68, train time every whole data:365.44s
epoch: 68, total time:23984.94s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.89s
test time on whole data:48.05s
1.5369257210761842 2.4958206122862543 57.57607187465741 0.8408042545228027 
1.5582868010470023 2.54526520915887 58.1777610377623 0.8345529561884406 
1.5817280306377937 2.592627915859971 58.20676911962863 0.8291586992741996 
1.5955306160468608 2.6235572906500058 58.17558259791484 0.826535722394728 
1.6216105308162847 2.678666886328962 58.35802456311531 0.8209303137944132 
1.6517935313220535 2.750951894434822 58.598577907170636 0.8121837654226727 
1.6859114034010896 2.8269167907641126 59.20514444450621 0.8018877728935877 
1.7137185951882885 2.891747493760937 59.70882137721753 0.7921831017801174 
1.743704402076524 2.968246073311752 60.351492747837945 0.7793560146606846 
1.7639959719725662 3.011091278409581 60.702799857241665 0.7722527373909364 
1.7774459889215373 3.0413987072384123 60.9007751559856 0.7678340647269992 
1.8043908818864396 3.095023146333057 61.198817926539014 0.760043325540277 
1.6695868728660521 2.8005260281990934 59.26345304022297 0.760043325540277 
epoch: 69, train time every whole data:366.25s
epoch: 69, total time:24414.65s
predicting testing set batch 1 / 168, time: 0.28s
predicting testing set batch 101 / 168, time: 28.63s
test time on whole data:47.64s
1.5391119077843392 2.4813330839839707 58.01282158323961 0.8416241640213238 
1.5585227395904562 2.535055883852158 58.35233304845059 0.8349377931987796 
1.5803184653473574 2.5808070737989257 58.38777353343606 0.8294888725919694 
1.5929903207742386 2.613275994172287 58.40557911948866 0.8259897936709492 
1.6158065863349254 2.6557702010335102 58.70054307808553 0.8213989961975477 
1.6443452411085544 2.7250008705654993 59.084993341165024 0.8125078582769962 
1.6750665416627057 2.7945061430042926 59.819336560309424 0.8027394062451535 
1.7011794409552323 2.855577558980483 60.46626369844629 0.7933987902878106 
1.728086835807012 2.9204780543636244 61.1553227121921 0.7827747701400438 
1.7471230611872106 2.961518286851175 61.47442467544091 0.7761499650987205 
1.7616774879769377 2.9990911835277076 61.700539735229185 0.7705269915669556 
1.7903482732094291 3.0608980420921315 61.99845472277147 0.7613134254450654 
1.6612147418115333 2.7715678331693274 59.796612465476926 0.7613134254450654 
