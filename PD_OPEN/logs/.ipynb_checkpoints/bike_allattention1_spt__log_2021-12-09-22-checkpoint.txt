total training epoch, fine tune epoch: 20 , 50
batch_size: 4
folder_dir: MAE_ASTGNN_h1d0w0_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1_spt_
load file: data/bike.npz
ori length: 3001 , percent: 1.0 , scale: 3001
train: torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12])
val: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
test: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
TemporalPositionalEncoding max_len: 12
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): ModuleList(
          (0): TCN(
            (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
          )
          (1): TCN(
            (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): ModuleList(
          (0): TCN(
            (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
          )
          (1): TCN(
            (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): ModuleList(
          (0): TCN(
            (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
          )
          (1): TCN(
            (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): ModuleList(
          (0): TCN(
            (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
          )
          (1): TCN(
            (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): ModuleList(
          (0): TCN(
            (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
          )
          (1): TCN(
            (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttention(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): Linear(in_features=64, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): ModuleList(
          (0): TCN(
            (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
          )
          (1): TCN(
            (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
          )
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (src_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
  (prediction_generator2): Linear(in_features=64, out_features=1, bias=True)
)
delete the old one and create params directory experiments/bike/MAE_ASTGNN_h1d0w0_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1_spt_
Net's state_dict:
encoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.2.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.3.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.2.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.3.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.2.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.3.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.2.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.3.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
encoder.layers.0.tcn.0.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn.0.filter_convs.bias 	 torch.Size([64])
encoder.layers.0.tcn.0.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn.0.gate_convs.bias 	 torch.Size([64])
encoder.layers.0.tcn.1.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn.1.filter_convs.bias 	 torch.Size([64])
encoder.layers.0.tcn.1.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn.1.gate_convs.bias 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.3.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.3.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.3.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.3.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
encoder.layers.1.tcn.0.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn.0.filter_convs.bias 	 torch.Size([64])
encoder.layers.1.tcn.0.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn.0.gate_convs.bias 	 torch.Size([64])
encoder.layers.1.tcn.1.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn.1.filter_convs.bias 	 torch.Size([64])
encoder.layers.1.tcn.1.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn.1.gate_convs.bias 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.3.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.3.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.3.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.3.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
encoder.layers.2.tcn.0.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn.0.filter_convs.bias 	 torch.Size([64])
encoder.layers.2.tcn.0.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn.0.gate_convs.bias 	 torch.Size([64])
encoder.layers.2.tcn.1.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn.1.filter_convs.bias 	 torch.Size([64])
encoder.layers.2.tcn.1.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn.1.gate_convs.bias 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
encoder.norm2.weight 	 torch.Size([64])
encoder.norm2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.3.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.3.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.2.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.3.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.2.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.3.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.2.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.3.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.2.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.3.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
decoder.layers.0.tcn.0.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.tcn.0.filter_convs.bias 	 torch.Size([64])
decoder.layers.0.tcn.0.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.tcn.0.gate_convs.bias 	 torch.Size([64])
decoder.layers.0.tcn.1.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.tcn.1.filter_convs.bias 	 torch.Size([64])
decoder.layers.0.tcn.1.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.tcn.1.gate_convs.bias 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.3.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.3.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.2.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.3.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.2.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.3.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.2.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.3.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.2.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.3.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
decoder.layers.1.tcn.0.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.tcn.0.filter_convs.bias 	 torch.Size([64])
decoder.layers.1.tcn.0.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.tcn.0.gate_convs.bias 	 torch.Size([64])
decoder.layers.1.tcn.1.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.tcn.1.filter_convs.bias 	 torch.Size([64])
decoder.layers.1.tcn.1.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.tcn.1.gate_convs.bias 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.3.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.3.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.2.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.3.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.2.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.3.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.2.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.3.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.2.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.3.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
decoder.layers.2.tcn.0.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.tcn.0.filter_convs.bias 	 torch.Size([64])
decoder.layers.2.tcn.0.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.tcn.0.gate_convs.bias 	 torch.Size([64])
decoder.layers.2.tcn.1.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.tcn.1.filter_convs.bias 	 torch.Size([64])
decoder.layers.2.tcn.1.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.tcn.1.gate_convs.bias 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
decoder.norm2.weight 	 torch.Size([64])
decoder.norm2.bias 	 torch.Size([64])
src_embed.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed.1.embedding.weight 	 torch.Size([250, 64])
src_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed2.1.embedding.weight 	 torch.Size([250, 64])
trg_embed.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.1.embedding.weight 	 torch.Size([250, 64])
trg_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed2.1.embedding.weight 	 torch.Size([250, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
prediction_generator2.weight 	 torch.Size([1, 64])
prediction_generator2.bias 	 torch.Size([1])
Net's total params: 945922
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459]}]
predicting testing set batch 1 / 168, time: 0.66s
predicting testing set batch 101 / 168, time: 33.82s
test time on whole data:56.20s
80.7378811806208 89.54850939236472 3315.125233870376 -0.018832325744420852 
86.00336507681739 91.6320507296821 3529.3497356375938 -0.018259218196304283 
88.20565549579041 93.97477402563678 3617.809143329152 -0.01656100119665289 
90.6067752341849 97.06346126511767 3714.626510069693 -0.013645821826903503 
91.63817370129048 99.08444245775253 3755.8610509001082 -0.010529227811771654 
91.49259114753819 99.5873906340395 3750.0967873441814 -0.008389604125275634 
91.41441348276891 99.63853116989446 3748.113752247011 -0.007297212225535414 
91.65869489023994 99.39845698367772 3759.4075389803643 -0.007088621494596507 
90.3971420749033 97.76553267507553 3709.667463855254 -0.007112677148755724 
87.35216810539 94.70075360271252 3586.76707578882 -0.006645036156075392 
84.0154604311879 91.0872241029796 3451.285332660266 -0.005970774593018666 
81.28658758719914 85.31946665850616 3339.9951065023515 -0.007439503140478171 
87.90074236732762 95.0060669439886 3606.5110832206406 -0.007439503140478171 
epoch: 0, train time every whole data:221.53s
epoch: 0, total time:288.50s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.76s
test time on whole data:57.81s
2.863007403604154 4.751409300063147 59.6429574183047 0.30702337764223187 
2.8730684073752766 4.785128270512873 59.590243228268314 0.30000265772789203 
2.878804227721922 4.799960933992873 59.58911715482086 0.29219170715213805 
2.8832765144361625 4.803432266532414 59.62449934047075 0.28049974705867603 
2.892628487737051 4.818858749825969 59.854440982069555 0.2700720574973937 
2.9002509323557217 4.833518102806948 59.954610180218424 0.26621183116389 
2.9029213602851542 4.829296927365141 60.07108405970251 0.2592715191985094 
2.9060390199715536 4.83196390021889 59.97260405035972 0.25782934326758433 
2.907399650296906 4.8317759819851425 59.75299078074545 0.25742482126170385 
2.911177206334259 4.829597657132208 59.8744549255562 0.2501305618949766 
2.9161470865116765 4.836706305163247 59.92716056258374 0.24768760574165047 
2.9184910203289416 4.830218775513474 60.19958173961947 0.242397099581717 
2.8961009430798983 4.815219138551037 59.837822430748425 0.242397099581717 
epoch: 1, train time every whole data:223.18s
epoch: 1, total time:580.25s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.78s
test time on whole data:57.84s
2.5343743602778055 4.153303189145451 59.180293693252764 0.5713744960368043 
2.561427987656689 4.20639669856134 59.86788681870042 0.5433376221953362 
2.576160165124972 4.247464711331672 60.43454567700059 0.5111580312733599 
2.5998019407219477 4.288658479418519 61.38435784786689 0.47808221962677944 
2.6265119526850147 4.333809475227299 62.14210433276585 0.443099652817185 
2.6571620377614384 4.378079933772214 63.007965859268324 0.405025019522267 
2.692357100667698 4.412734254608545 64.26967567823466 0.36797339351674657 
2.7259027062232297 4.446685962662425 65.10954564776812 0.33386027416008746 
2.7610915048708695 4.476003602436162 65.88801091232432 0.30293318635708927 
2.7956204157354576 4.496070191785045 66.63667638935618 0.2793317464605808 
2.828091776236271 4.520090579659561 66.90932198349897 0.25645556975046807 
2.8583605193524133 4.538637264158425 67.07851962081588 0.23911518969117382 
2.6847385389428173 4.376563332137705 63.492572832420066 0.23911518969117382 
epoch: 2, train time every whole data:223.09s
epoch: 2, total time:871.86s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.59s
test time on whole data:57.56s
2.388160859718919 4.237631610881655 60.94312419095321 0.615809771565509 
2.4070763598400213 4.29961048050002 59.74043881848203 0.5923070377030483 
2.4277473767240134 4.354590896602266 60.43247924903229 0.5616361729292589 
2.4555675199352027 4.408097725965957 61.11836619782585 0.5255815485255472 
2.494917305550582 4.468625889720086 62.110286023080654 0.48546317854639115 
2.5417773225721683 4.530022465730502 63.34827844437336 0.4437467516254813 
2.5897753778316437 4.582984969911903 64.51969249239168 0.4004867025610092 
2.643753469645445 4.634913373793352 65.84192784125761 0.3603835945681876 
2.6996532192915854 4.680625843892047 67.06019852049639 0.32408112336380945 
2.755253599404579 4.7145870264864005 68.15513251818277 0.2930276169538521 
2.8153623998498634 4.751958917890204 69.39670636166012 0.2656582685906727 
2.8524378834695865 4.7332948533739465 68.85849834460885 0.24578739453355042 
2.589290224486134 4.536285524692623 64.2939506541199 0.24578739453355042 
epoch: 3, train time every whole data:223.24s
epoch: 3, total time:1163.30s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.72s
test time on whole data:57.74s
2.336971815800383 3.703050519571089 68.4677541501033 0.6193107606214491 
2.3639297466298475 3.794508999844672 67.52700355365377 0.5966288080203017 
2.3963794028623298 3.8622421495614625 68.56525357179531 0.5670226726658375 
2.447631413173374 3.94291712450877 70.36173554017637 0.5306353315380469 
2.512471696799 4.031460546151205 72.43412150163331 0.4891928142368239 
2.5826299891609343 4.119500289206243 74.6265980740753 0.446141282536154 
2.6622302586244686 4.20668078139768 77.36610060550046 0.40092940483836786 
2.7403269106350128 4.284771556436968 79.73677665665309 0.35943954733238614 
2.817025884648962 4.355290873770232 81.76982883130873 0.3212946335647923 
2.8895496967325784 4.418972393567755 83.45233569647435 0.28694140912115446 
2.9474699095296897 4.4743325207695195 83.89135987800856 0.2568824782118758 
3.019160762584724 4.504783086551271 85.35086986089266 0.23341831010568723 
2.6429814572651087 4.149913418794842 76.12951597673047 0.23341831010568723 
epoch: 4, train time every whole data:223.22s
epoch: 4, total time:1454.89s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.73s
test time on whole data:57.71s
2.4779701989362284 4.364607807933541 67.22968656188971 0.6126067043270643 
2.485104324732508 4.42889442915067 64.23783141152227 0.5971536408723185 
2.4952350305628386 4.462837681091696 64.40383479793209 0.5753586800136073 
2.514690072963369 4.50337315997579 64.65564164285476 0.5477679821075766 
2.550393365167259 4.5589211348405465 65.42496615336985 0.5150741559173734 
2.5997772173926768 4.625499557282779 66.78973822782366 0.4791185496035645 
2.65390063152276 4.693696289510877 68.29054703228397 0.4385990073717326 
2.7171135801813078 4.763457017848419 70.14616323280258 0.39951653205626103 
2.775641633149414 4.821039649718579 71.653498194041 0.3628235272779285 
2.8269677717611192 4.863226029503728 72.77078085370312 0.3293551118762575 
2.8831255185943805 4.902272981314322 74.10870518982131 0.2982430786900131 
2.9406577678329886 4.958547900055701 73.79323068525962 0.2741902192465751 
2.6600480927330707 4.666149208512731 68.62557011686144 0.2741902192465751 
epoch: 5, train time every whole data:223.15s
epoch: 5, total time:1746.53s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.75s
test time on whole data:57.81s
2.371072049543794 4.169617852280156 55.16546985653202 0.6009833480619208 
2.403965330260673 4.226114005311345 53.3740691387742 0.5881759388190263 
2.407050354513384 4.252716520478181 53.523113562378654 0.5724817296244267 
2.423706748214506 4.285825583867822 53.98865918476257 0.5517035227936113 
2.4535097286796996 4.328458096494527 54.84608522450941 0.5264712082208169 
2.491468870224076 4.379271432905536 55.90127470533815 0.4972796773108617 
2.5344583097440854 4.432139677409336 57.13134322568189 0.4625233210089394 
2.580896663925921 4.485007268392769 58.354141319895255 0.4271028325882089 
2.6279845108758835 4.531606848964862 59.492424460194336 0.392425089691719 
2.676421542477927 4.572992617674374 60.65415590908633 0.35945523223186626 
2.7275265826176676 4.614161224783402 61.81456704596972 0.32682068286262345 
2.7852686958439827 4.669983097135447 61.00094115279872 0.2978964230851052 
2.540277448910133 4.415119095253967 57.104013055408565 0.2978964230851052 
epoch: 6, train time every whole data:223.22s
epoch: 6, total time:2038.38s
