total training epoch, fine tune epoch: 30 , 40
batch_size: 4
folder_dir: MAE_ASTGNN_h1d0w0_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1_spt_trend
load file: data/bike.npz
ori length: 3001 , percent: 1.0 , scale: 3001
train: torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12])
val: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
test: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
TemporalPositionalEncoding max_len: 12
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (src_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
  (prediction_generator2): Linear(in_features=64, out_features=1, bias=True)
)
delete the old one and create params directory experiments/bike/MAE_ASTGNN_h1d0w0_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1_spt_trend
Net's state_dict:
encoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
encoder.norm2.weight 	 torch.Size([64])
encoder.norm2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
decoder.norm2.weight 	 torch.Size([64])
decoder.norm2.bias 	 torch.Size([64])
src_embed.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed.1.embedding.weight 	 torch.Size([250, 64])
src_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed2.1.embedding.weight 	 torch.Size([250, 64])
trg_embed.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.1.embedding.weight 	 torch.Size([250, 64])
trg_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed2.1.embedding.weight 	 torch.Size([250, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
prediction_generator2.weight 	 torch.Size([1, 64])
prediction_generator2.bias 	 torch.Size([1])
Net's total params: 1140994
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411]}]
predicting testing set batch 1 / 168, time: 0.63s
predicting testing set batch 101 / 168, time: 34.35s
test time on whole data:57.11s
67.01194679049829 90.32395027192939 2725.949110924922 0.014721081448519874 
65.37310008488329 87.62026716318287 2665.3153673575266 0.01422713375194906 
63.44465209479577 84.72653611430384 2589.1120293081945 0.014088733061435895 
61.697881408599514 82.69061133687627 2514.630147459955 0.013149448868065954 
61.770214547201284 83.12509876792132 2513.1954472127754 0.011676950284844848 
62.54272056868912 84.3163703133976 2541.7859521691244 0.010478281984664526 
63.20959798847272 85.2685955897135 2566.892004124322 0.009676961989177425 
64.21686651693702 86.67737330781966 2606.0939041518177 0.008904446007297588 
65.78166836643503 88.87580308586283 2668.8750058092764 0.008049450494498352 
67.19630895260917 90.87720016690267 2725.7485459389814 0.0073337510480748274 
68.2302136585127 92.35313644963468 2767.086584890806 0.006812416128605342 
68.67203660771118 92.98916961592562 2785.5538197088085 0.006498990486640914 
64.92893396544542 87.5540045168946 2639.1880359070365 0.006498990486640914 
epoch: 0, train time every whole data:209.37s
epoch: 0, total time:278.40s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.44s
test time on whole data:58.96s
2.466634959120037 4.315887973802829 55.780809291126964 0.6245651153587888 
2.4817643340402946 4.3646300923430745 56.38982190791367 0.5874488866454944 
2.5156538186054678 4.4411637820679735 57.77794126802161 0.5421163391591515 
2.554832788914797 4.521789214546128 59.1694539898115 0.49708874893871396 
2.60176054622091 4.598466918337241 60.765325726472675 0.44702307307122496 
2.654913448530471 4.676587387820751 62.58201771706594 0.39755047649828995 
2.7070560378708124 4.746373610523629 64.27100150441439 0.35217358337681864 
2.7594894756094686 4.805758555405781 65.85546298452743 0.31345515706644406 
2.8135145714826706 4.85538789324415 67.42284001528338 0.27932348078606806 
2.8630412264642793 4.883578822380022 68.60912422869508 0.25052869158923224 
2.9099215300503585 4.899825033336154 69.58332658048998 0.2253732704012537 
2.9498339104784566 4.918287301820202 70.25829365447632 0.20742953406882997 
2.689868053949002 4.673588135047238 63.20574629905343 0.20742953406882997 
epoch: 1, train time every whole data:211.31s
epoch: 1, total time:560.83s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.51s
test time on whole data:59.05s
2.9196551379555213 4.011318507966895 99.17317492877812 0.5862603793515423 
2.9267532733495214 4.035295568476321 99.73681284479864 0.5695153926599611 
2.9565408529542565 4.091637592976594 100.29398438089024 0.5361807292827819 
2.9950448274169825 4.146086296511675 101.78067314831742 0.5039266789042477 
3.050252813186408 4.218014680915172 103.73486556775187 0.4579973600060037 
3.095519745651899 4.286977693937972 104.80325097723788 0.4101826878007796 
3.129325577741489 4.347028364868528 105.09123078535474 0.3666363689967208 
3.158778608329062 4.398022298821953 104.81195356977922 0.3287483149641761 
3.188998571531492 4.444787176121214 104.51876039034458 0.2936102089500655 
3.222590754790764 4.489639799459964 104.26258334549856 0.26092143743111396 
3.2579389302216115 4.531265401660012 103.9641905514257 0.2317657307314977 
3.279006473286787 4.554405211308082 102.94256766198625 0.21185521358062073 
3.0983671305346494 4.3002215945743805 102.92628753604593 0.21185521358062073 
epoch: 2, train time every whole data:211.03s
epoch: 2, total time:843.36s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 35.60s
test time on whole data:59.15s
1.9543228930319172 3.363085578970738 62.79506986034632 0.6900969732517146 
2.045794873837648 3.529765632353813 66.32345484190073 0.6480430529524003 
2.218363479833695 3.96383348618037 62.864344445526356 0.5710627784534078 
2.249769401637039 4.014645302893052 63.71141446626966 0.5498664890168689 
2.2919108687317267 4.0764791453758615 64.29440130801486 0.5299200216754293 
2.33816177851414 4.148516957315486 64.23878987831765 0.5121784326702645 
2.3860263147613123 4.210691950156829 65.18943341684901 0.4889912457854962 
2.436689826709085 4.2840126856396665 65.40126054822957 0.4659729300576016 
2.4906958488894717 4.368372301348258 64.99632800884963 0.4402408252254207 
2.5490396420525476 4.437881047998341 65.95794875206892 0.4079054657328449 
2.615733821593225 4.503106946926498 67.48061908665288 0.37498298337844876 
2.685624943601588 4.5608978601047205 69.15367898433267 0.3447437901941883 
2.3551778077661165 4.136894322514412 65.20062337472827 0.3447437901941883 
epoch: 3, train time every whole data:211.09s
epoch: 3, total time:1125.72s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.51s
test time on whole data:59.04s
2.114380835811031 3.562468037262082 66.46476895661768 0.7139112985265786 
2.2656650154642053 3.8428119444364777 69.30280214395859 0.6669464293479438 
2.5720899758860467 4.29747459271622 75.73249470673474 0.6054650623343916 
2.6180636526303633 4.3823382032746405 76.62397193780471 0.5726407362584447 
2.7093179660012856 4.514658710939167 77.82033666946431 0.5473925534552652 
2.8110827783920933 4.650535819811139 79.50160642640535 0.5253978395625195 
2.911123963903636 4.7820730042702175 81.92931636349967 0.5136522311013474 
2.9913684099088647 4.87848632690174 84.16328618305636 0.5024274525015455 
3.13419773773645 5.0304976673300414 90.3931552269894 0.4908977078658862 
3.1542297365783405 5.063494238002026 91.32806683708797 0.4746741764781111 
3.1769758873329987 5.0951943998134 92.2115027877642 0.45687905069456514 
3.202095850270153 5.129015307754135 93.19662928890182 0.43398623447545676 
2.805049317492956 4.628010016785843 81.55615495586208 0.43398623447545676 
epoch: 4, train time every whole data:210.65s
epoch: 4, total time:1406.18s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.70s
test time on whole data:59.26s
2.5454105108419345 3.5520842774793677 84.60614699859607 0.7332558066072042 
2.6172421233288587 3.7013076883094547 85.34937845588505 0.6902624726116574 
2.70705044798757 3.897120561895238 86.09501033704976 0.6223055558810437 
2.73473338063283 3.9417316392635438 86.95217042595912 0.6021030609542107 
2.7617154502047314 3.9971269242991316 86.16905416843771 0.5780469245838168 
2.7763519059832964 4.013508782832583 85.7211133991159 0.5676126241540567 
2.76645946187322 3.999574577625111 84.28671299342281 0.5716638857441981 
2.749848858440472 3.9751664438854695 82.81628446339646 0.586320758605833 
2.746551575945513 3.9578170750969504 82.61397932328728 0.596019628847787 
2.742555623537373 3.940271455417634 82.8694009578366 0.6099288584519394 
2.735202056421588 3.943713749556662 82.44593428801349 0.6095266794589399 
2.746263950739588 3.970287228122753 82.87392755781934 0.5939216591844019 
2.7191154454947477 3.90972839763974 84.39985410883378 0.5939216591844019 
epoch: 5, train time every whole data:211.24s
epoch: 5, total time:1688.66s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 35.52s
test time on whole data:59.06s
1.9072056149010148 3.0441685836927626 62.078332711179044 0.7579755421838849 
1.9657649294912283 3.2084990732294085 63.76182744052568 0.7232604454410217 
2.0142377158324454 3.390917978803188 64.83796803511149 0.6861946662171979 
2.0674853345472366 3.475933042727956 66.60398218301044 0.6625847548526759 
2.1115947845626977 3.573033388603561 66.26745982613436 0.6401368231160122 
2.145351073122805 3.649461879240816 65.43070720913005 0.6231589325410593 
2.1547221774429617 3.6879516511344472 63.64208689119396 0.6177333227441372 
2.1911135225153218 3.7695600803516816 62.50129068333092 0.6011977296099567 
2.2336121870475334 3.8215139763245536 62.20418347393394 0.5884807812666739 
2.2477070781544204 3.8858265244552417 58.59992324214549 0.5868165127906957 
2.2840688984561712 3.9416292948213165 57.76741181107947 0.5767979095617779 
2.3339469022856405 3.9912725744124717 58.92544112066701 0.555968394402048 
2.138067518196623 3.630976558498681 62.7182912441903 0.555968394402048 
epoch: 6, train time every whole data:211.01s
epoch: 6, total time:1970.64s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.60s
test time on whole data:59.23s
2.2828986190207123 3.6427204212561013 72.61595803930939 0.7218244643628575 
2.362777698677033 3.8123557451203824 73.9614163419714 0.6925776327952946 
2.438393885354556 4.000280075606311 73.77855631499742 0.6657794989603862 
2.4911105665011064 4.080784069765996 75.34783174233168 0.6415454671150803 
2.526469999406132 4.169821623892886 75.17219174510087 0.6223482733198578 
2.5757592345924074 4.273840539959493 75.12610095375548 0.610853227915772 
2.6202023587263232 4.34462399175978 76.22705153487776 0.5937623225659923 
2.639981515755788 4.3787451052533735 75.90603683697006 0.5898679871374174 
2.6135785586733724 4.341409694801225 74.6093024387321 0.6094123404615215 
2.572398646403725 4.24463905209488 74.18929643041719 0.6391691200225087 
2.622843046060098 4.301827853878456 76.1693107915542 0.6318805341327871 
2.672078712222654 4.392089972348597 77.28013610817183 0.5978564502008092 
2.5348744034494923 4.171548227727437 75.03198056436952 0.5978564502008092 
epoch: 7, train time every whole data:210.84s
epoch: 7, total time:2251.17s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 35.41s
test time on whole data:59.29s
1.9316518241461544 3.184040676462167 64.04867936197203 0.7602125013737934 
2.01146493154844 3.3430068589376627 66.18859772388058 0.7373201127837683 
2.129367792475525 3.592948287875762 67.27795672697138 0.7108856002706854 
2.1694003801713033 3.638546345735722 68.5160452269673 0.7060307689260366 
2.2537582783728305 3.7796914637557952 69.76626354480854 0.6924026194725578 
2.3116330761953834 3.895508904057547 70.66205750341757 0.6759624994061788 
2.396959212056317 4.020158776164904 72.47620273137757 0.6634290103663548 
2.472174236304171 4.116786876946223 73.94429084688299 0.6570756952380201 
2.5055696876470352 4.154885445371662 74.75169161662822 0.6529364966678548 
2.5459707029772303 4.203227335904828 76.45452577217895 0.6415337213809904 
2.6071884484959855 4.289020625978828 78.3071189980025 0.6280441805488775 
2.6379606181918867 4.355536480388679 78.71551757876976 0.6037887753930883 
2.3310915990485217 3.897962756575176 71.75934104293347 0.6037887753930883 
epoch: 8, train time every whole data:210.93s
epoch: 8, total time:2532.46s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 35.59s
test time on whole data:59.12s
1.8998206360593792 3.066212814446469 61.28499798922643 0.7577013704966575 
1.9518626310989438 3.138500549184415 64.1360478246061 0.7429549321832676 
1.9720018495214837 3.1751439478807817 65.72832799518274 0.733943462179777 
2.0181623658517465 3.2507811460889 67.30157500601094 0.7199135895406067 
2.0616696245351007 3.354725450256449 67.659769683341 0.6991694528358815 
2.1151284917900783 3.4625062019838113 68.95116385891333 0.6759655628699018 
2.1115679713042366 3.502900043173774 66.67757793846096 0.670653939245962 
2.101144857641487 3.485450921928878 65.31042034290573 0.6791205236072417 
2.0873965620517376 3.4436375115156856 65.20540180335462 0.6893445959449245 
2.069771634200588 3.429069420722486 64.41705621544092 0.6940513851718452 
2.0904759761851635 3.484451552703146 64.2197349248058 0.6873917430154687 
2.143381893677814 3.5957519638661775 65.21628505943966 0.6599002927282005 
2.0518653744931465 3.369603883357915 65.50905616027562 0.6599002927282005 
epoch: 9, train time every whole data:210.54s
epoch: 9, total time:2813.87s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.55s
test time on whole data:59.17s
1.9066269975988461 3.0766325070332172 63.74987749401401 0.7853362802664238 
1.9619845989804183 3.1763677088953504 65.06921620380268 0.7684526293351718 
2.022121093074481 3.3555564434843324 64.01778320696862 0.7438577904456543 
2.0506529423163404 3.3935456407190205 64.34456130145739 0.7431323878046208 
2.1007897365362873 3.4616860018552384 65.25932772822529 0.7343644851167337 
2.1340180768171058 3.5113082783800666 65.84848078444193 0.7292716366417377 
2.188527203867744 3.596997727540493 66.90051656810337 0.7223376568091362 
2.249898625800386 3.658233088763356 68.83432325805087 0.7198705383974783 
2.28310981835895 3.7050488102240666 69.8528256661487 0.7148547486806792 
2.3387326187026645 3.7706303273594903 72.0522388188867 0.7110954846413462 
2.3820124569074914 3.8455626368083244 73.15675393705715 0.7042208435464602 
2.421007427256111 3.94942618490817 73.57836310289156 0.6799504023547913 
2.1699567996847353 3.5507730048870436 67.72220180739977 0.6799504023547913 
epoch: 10, train time every whole data:210.56s
epoch: 10, total time:3095.79s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.58s
test time on whole data:58.99s
1.8828702164679056 3.013869722955623 57.410123389548836 0.777829277067261 
1.9305926540002583 3.1348552279022384 58.05090124541303 0.7564157487017136 
1.9942763111954112 3.3130126866790413 57.759866380281345 0.7314326950036891 
2.015979175485848 3.359140724015874 57.8076473252956 0.7239495049285831 
2.0436680936466547 3.424097330592488 57.089871867978125 0.7105218133294864 
2.067033988788724 3.4587851448045375 56.80626826825452 0.700942983460557 
2.0654445629386853 3.4279136279379596 56.886695902005094 0.7066279567590443 
2.057170870175408 3.3967973691686444 57.2284940446786 0.7088790999406022 
2.0504196560184162 3.380041817804852 57.62725403339829 0.7063343954338871 
2.058247287240589 3.38651278742237 58.91451771117637 0.6981359900035905 
2.0665321976559325 3.426384368401636 58.87387898278713 0.6923396905967958 
2.090933550050926 3.4846042531808803 58.80767171153241 0.683398437154672 
2.02693071363873 3.353142768093038 57.77194267459818 0.683398437154672 
epoch: 11, train time every whole data:210.17s
epoch: 11, total time:3377.20s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 35.39s
test time on whole data:58.80s
1.7489426710139073 2.8680052730806302 57.18900715209486 0.792497750573086 
1.8026577422489367 2.95143933586096 59.39997683823401 0.7721382248986162 
1.856519823820818 3.064357226378113 59.9987937457183 0.7536818658652992 
1.9260160692232173 3.1549703373602855 62.21454118905383 0.7321493442752816 
1.9586247409123572 3.221713703424796 62.00832166819825 0.7197663764296862 
1.9987982477528885 3.3095812605562522 61.902453620137365 0.7014241912318465 
2.007764784115677 3.35745722728727 60.45658802607837 0.6951940249750012 
2.012332481675027 3.368572481009414 59.739260248060745 0.6968975068607639 
2.0005280645595245 3.3637551136599804 58.91019990900141 0.700750877498765 
1.981183736893659 3.3211467944149136 58.346600828911896 0.7081129023357989 
1.9631750862435215 3.3111207739532116 57.41671768489171 0.7115372131798237 
2.010089668118262 3.4147249428188267 57.96742568080223 0.6900546688369493 
1.9388860930481497 3.230091201623186 59.62914017259857 0.6900546688369493 
epoch: 12, train time every whole data:210.23s
epoch: 12, total time:3658.36s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.37s
test time on whole data:58.90s
1.691041951158838 2.835324123586746 54.7426834551214 0.7999535200880088 
1.7423096765330328 2.9114537806116596 56.48866760496992 0.78387100301212 
1.7949754116750722 3.0078627963343747 57.74472241904982 0.7652337189448253 
1.8427774934856487 3.0735563452296453 59.600577441665 0.749838133195356 
1.8700594818350815 3.1348869909894304 60.627513780543865 0.7381433283294248 
1.8927936105148955 3.1563497260622673 61.29130227843687 0.7335995521482809 
1.8931545935049654 3.1276540890914033 61.067180637200366 0.7400732054822688 
1.8987812113752145 3.122268677357173 60.73201683466092 0.7415175398172582 
1.9216217211400646 3.18756200486677 60.181924003271114 0.7303025010537234 
1.9453123832436368 3.2777113263160897 58.78762033779835 0.7161627055693871 
1.9743384429020363 3.3769014119217697 56.94235178636443 0.7041785897097557 
2.0186668883931067 3.45787720254703 56.609025741678074 0.694686984667278 
1.8738194054801327 3.143707683779797 58.7346949446804 0.694686984667278 
epoch: 13, train time every whole data:210.92s
epoch: 13, total time:3940.54s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 35.65s
test time on whole data:59.16s
1.7152218042547327 2.950813152748453 54.568424438638786 0.8037612721620254 
1.7548023745775931 3.0098796558192813 55.91632535818639 0.7936465906398603 
1.7941352752585495 3.1010401455555727 55.93772899201057 0.7822560560182523 
1.8306434173295718 3.1729615996719565 56.596078560776476 0.7690257623181648 
1.889765425576252 3.3071383132259884 57.80499915846161 0.7480213045207866 
1.9449116851205805 3.4232121825134674 58.69981282018618 0.7298844978154462 
1.9739631153398327 3.453674860645252 59.23228614134425 0.7332891248074425 
1.986590182379509 3.439462140882609 59.87114700969263 0.7348378777306235 
1.9958296280943213 3.45382085005627 60.29530798934935 0.7297621740029877 
2.0065984302968496 3.470453872276685 61.1035146966947 0.7202335347467661 
2.008042092647581 3.4657288374745128 61.30981641288923 0.7205919169873539 
2.0390565588110614 3.541423565406994 61.36942597066902 0.7086413399122256 
1.911629999140536 3.32152086427322 58.558874970830885 0.7086413399122256 
epoch: 14, train time every whole data:210.90s
epoch: 14, total time:4223.84s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.58s
test time on whole data:59.09s
1.7140219376857968 2.941431103226087 53.593047679574866 0.8040075896762849 
1.7699546173536884 3.069014017418022 54.64239749246968 0.7856811892567354 
1.8502455310171382 3.2658544482327856 55.36952394400294 0.7594045256318334 
1.8732826016643749 3.2851286222821283 56.47902464949897 0.7564080479816673 
1.9030925794378633 3.329165636010292 57.565016654416176 0.7478653364553556 
1.9175290166161778 3.336881651522231 58.07202958423725 0.7471118368724411 
1.9372656863956224 3.33489541753888 59.094045686209164 0.7501832068476257 
1.9357799110468477 3.303428428349259 59.735596363773766 0.753276879663082 
1.9480724280334654 3.3293973974915128 60.252441547208235 0.7446014667302867 
1.9593601491235728 3.371301475021918 60.55026065235837 0.7326677185295214 
1.963859482776551 3.3934704546512555 60.869431798409444 0.7281229380846654 
2.000994171313498 3.487900040791129 60.91357474033408 0.7130124674353537 
1.8977881760387163 3.2902973297582174 58.09484861464998 0.7130124674353537 
epoch: 15, train time every whole data:212.01s
epoch: 15, total time:4506.74s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.18s
test time on whole data:59.83s
1.619528530286359 2.6933005338149103 56.73194007178163 0.815193616345032 
1.6549587849818526 2.760869559102804 57.261144329739544 0.8055847493236318 
1.6930329622367308 2.851612151536749 56.896947803388265 0.7968422775458743 
1.7281939440627716 2.912028645289197 56.60248799728256 0.7899706040888339 
1.7686832487956576 2.999201683844761 56.40311359377741 0.7807977998483266 
1.808537262560445 3.0688653499123926 56.761146682758415 0.77127330840273 
1.8285793199587081 3.0944648198604936 57.2034172659691 0.7673280453554311 
1.8306669553786161 3.0775451955135074 57.727271889105644 0.768806150674131 
1.8416288933463927 3.085101591711361 58.74808723479651 0.765839594799261 
1.8436682836383227 3.084413439819815 60.01371041745271 0.7604490419601331 
1.8524256468544757 3.1142186544355033 61.11106960918756 0.7533022634621276 
1.8823937278658684 3.1963261960460323 61.130982800124144 0.7414380320306524 
1.7793581299971835 2.998532008718919 58.04934522018381 0.7414380320306524 
epoch: 16, train time every whole data:211.49s
epoch: 16, total time:4788.92s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 35.61s
test time on whole data:59.66s
1.8250342475517343 2.8597039979472454 61.53027457161387 0.8219448074476515 
1.8788014723530837 2.9341491222227187 63.005608120116044 0.8155430127247761 
1.9613012255092284 3.084219180957322 64.09976492635427 0.8057120399279432 
2.008208662965823 3.1425390342760897 65.34061053001112 0.8009484038395741 
2.051074712846162 3.2289553474969592 66.49372963666124 0.7876912511234765 
2.0924260300769517 3.305017765466355 67.8628175715625 0.7754811123374307 
2.132273048975904 3.3624936483084733 69.07572673619697 0.7729533540692537 
2.1492280210030397 3.3858553211684526 69.12268874613106 0.7764136389663725 
2.186311667605348 3.441362598029407 69.64426242633 0.7748885182522136 
2.188469689895177 3.455758379424645 69.85487573335132 0.7681445825850107 
2.202463810932867 3.469191981911167 70.93468316616753 0.7636336643914452 
2.2424363899923683 3.560417276764174 71.70588035148666 0.7500878863945567 
2.0765024149756406 3.2760733498420485 67.38942985952175 0.7500878863945567 
epoch: 17, train time every whole data:211.59s
epoch: 17, total time:5071.86s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.53s
test time on whole data:59.08s
1.652837986169649 2.759868307373672 52.0952519026211 0.8269410097909414 
1.6969178614438112 2.8546205799270337 52.59806961429163 0.8172044108631554 
1.7520985370768856 2.976743177424757 52.16516892946386 0.8070128840409968 
1.787258893761429 3.0395622592942044 52.37098954072253 0.8017727308929 
1.8190371961471226 3.108638168231228 52.569648007932024 0.7934538576900231 
1.829013122694567 3.1229865952708233 52.62641869594792 0.7895913269109568 
1.825937973067608 3.1087855792058696 53.01255834802882 0.7902130535147796 
1.8177724662911856 3.086066156679471 53.59946394730952 0.7879921615162057 
1.8350877120283742 3.130370497678147 54.947221251410696 0.7770669507445739 
1.8584391171115318 3.174383298697324 56.49951689737877 0.7641445557767915 
1.8801512773205482 3.2165353557647056 57.82667572632463 0.7559620151728299 
1.9321023129138741 3.3312666640537993 58.73432084285675 0.737140070777559 
1.8072212046688823 3.0793676988908496 54.08720490363852 0.737140070777559 
epoch: 18, train time every whole data:211.49s
epoch: 18, total time:5353.61s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.65s
test time on whole data:59.27s
1.705131861657525 2.7086377762145997 57.43312955862391 0.818782528001273 
1.750036715861676 2.845798451536603 57.337652711633766 0.8021026154698668 
1.79132044574982 2.9348840910813756 57.57945916153522 0.7905523856002664 
1.804786162864062 2.9333739442921476 57.71082930802355 0.7901149965729419 
1.8154426634214995 2.9538378202223585 58.011635294531416 0.7842556864365317 
1.8260797519511764 2.954789269599479 58.78125361125416 0.780218274950377 
1.8300521810915145 2.9427719103937124 59.08669058227005 0.7787789770883535 
1.8336313422477493 2.9261828191024306 59.5368775073341 0.7794312061291941 
1.850177551215071 2.957161794095204 59.50168807769786 0.7751450335298962 
1.864155653479376 2.9887444885833743 59.82106088272915 0.7701870924639056 
1.8822865850461559 3.0331334060019692 60.32914021967849 0.7633237932989596 
1.926614331024832 3.131039817418138 60.88431007124154 0.7484769710260254 
1.8233096038008716 2.9440987974438664 58.834543587385234 0.7484769710260254 
epoch: 19, train time every whole data:211.60s
epoch: 19, total time:5635.47s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.53s
test time on whole data:59.53s
1.6004521183350255 2.573581313929917 58.40541485067834 0.8281537766503209 
1.6252254748044623 2.6403849211215826 58.32571431909298 0.8195327991703267 
1.6532650339079222 2.7313912413499195 57.55278607377869 0.8106505348766115 
1.678323785633913 2.7981546901852536 56.87903575715204 0.8050845037561614 
1.7162604031947752 2.8836825636265644 56.776345425798795 0.7948765741335553 
1.7441051694317942 2.9292681896381287 57.11296254954941 0.7868138167862065 
1.7633437108887093 2.9606764113970114 57.38545017010112 0.7802330790495509 
1.7821781742265892 2.9887752693154956 58.06989575015945 0.7743447434090359 
1.8093880497557777 3.038568487177535 58.916144831720594 0.7664289291470233 
1.840652950110446 3.0874340405680196 60.51644994424248 0.7574668092677458 
1.8589412061974762 3.117029946923676 61.458177282614514 0.7533966561039522 
1.8862137881890826 3.185674903105479 61.53248160245878 0.7436641599761513 
1.7465291553896645 2.917037430583614 58.57762192825546 0.7436641599761513 
epoch: 20, train time every whole data:212.03s
epoch: 20, total time:5918.25s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.45s
test time on whole data:59.20s
1.6418776735204847 2.5443660218649957 59.746756152636685 0.8337958499478653 
1.6694681888937595 2.6091448284059724 60.2071365070175 0.8249180136962919 
1.6803080863543742 2.6582319791984004 59.804863046642765 0.8168473180454 
1.702914203915745 2.6997562470311802 59.224810552490496 0.809548644488333 
1.7315852466501473 2.7749193671918735 58.51280311069339 0.7969523100331966 
1.7659967460015877 2.8635904225281763 58.122170806682306 0.7832458897976132 
1.7814077505925228 2.9281246108211647 57.65590613793105 0.7733365074535046 
1.7848346607119199 2.950749287043026 57.70198530832348 0.7695288244198037 
1.789611107947571 2.9714135350434443 58.22095667668032 0.7660507085727247 
1.808604873706365 2.999040176035448 59.62052478731121 0.7612894683646337 
1.8332281424829826 3.051330263891027 60.81825462154462 0.7529265973981633 
1.872519235828182 3.140665908675469 61.27715179570469 0.7370433930643059 
1.7551963263838035 2.8550455050999415 59.2427614234216 0.7370433930643059 
epoch: 21, train time every whole data:211.03s
epoch: 21, total time:6199.95s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 35.66s
test time on whole data:59.18s
1.608850230172099 2.659490441719475 53.72318278813778 0.8284199198814624 
1.6414880760119608 2.748608364343968 53.809879378174365 0.8172130162572178 
1.6703364453728178 2.8075643711506486 53.410747319786424 0.8146167853931623 
1.680710327570371 2.8133887919314065 53.43833441545167 0.8139596025984688 
1.698338418801182 2.84159672953236 53.94710625934734 0.8077968388376923 
1.7193978722837 2.8681122812416104 54.87651888780665 0.8002675461141597 
1.7365082447777191 2.886209082670091 55.8147893780548 0.7931456978446749 
1.7501144557013397 2.9034130349249607 56.72549882385436 0.7865518241081704 
1.7704961027401012 2.940028831093306 57.309297177514026 0.7803389860684704 
1.78926933581045 2.98769069114301 57.87135236705312 0.7736282577437368 
1.8067792832319225 3.0463278950170785 58.22662040738443 0.7679569431777812 
1.8420111396207164 3.1384274710658673 58.30995797398847 0.7593192752275049 
1.726191661007865 2.889434309152899 55.62204620946638 0.7593192752275049 
epoch: 22, train time every whole data:211.55s
epoch: 22, total time:6481.59s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.42s
test time on whole data:58.92s
1.6457626806172942 2.635307322877631 56.50058277783092 0.8343725789158766 
1.6854126415282842 2.7270833446063736 57.449741048331674 0.8229752398499268 
1.7346099031318334 2.830677747087443 57.7285630310689 0.8140316007490555 
1.768314977617491 2.8911940532466405 57.90858235987567 0.8113211600167786 
1.8190243608934715 2.9974739714572745 58.611873699184954 0.8000707516472648 
1.859777685453317 3.063439324907905 59.60219679896438 0.7907746235098361 
1.8873687088520577 3.1080159647762597 60.61672772973359 0.7798042099627319 
1.8947815112742994 3.1194573789781406 61.502254421248736 0.7737629616467521 
1.9124174733829817 3.1653814199701666 62.349910093369466 0.7641348575168859 
1.9318847599652196 3.193149256348678 63.511699370917974 0.7570404431838469 
1.9461088827423574 3.2193859421042674 64.32584389537578 0.7535207001063672 
1.9775278840875696 3.294697849274502 64.89440512990477 0.7440312136609522 
1.8385826224621813 3.026950852601688 60.4170193548879 0.7440312136609522 
epoch: 23, train time every whole data:211.58s
epoch: 23, total time:6763.21s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.80s
test time on whole data:59.27s
1.5901254752165683 2.600898075767791 55.75000574940254 0.8330907132055958 
1.6339947521628901 2.7151253759203247 56.25292247644903 0.8199385697691465 
1.6912061576897368 2.858691224370713 55.80794920703187 0.8088329693718336 
1.7477270943029295 2.9981327845315593 55.70071387993376 0.7993166638981063 
1.79969499800576 3.1152265838750646 55.76501300770404 0.7905795386848964 
1.8050706757891568 3.111333498654043 55.856953376264116 0.7891316093077558 
1.785052041547639 3.0538500369809007 56.136736581707645 0.7849271756780226 
1.769518063006479 3.0122380380654055 56.79766391545835 0.7794802320238446 
1.784167332325042 3.045531493959054 57.673627112825834 0.768006625809774 
1.807267079456931 3.1062800871719474 58.91193076343737 0.7537544890874057 
1.8262425935534494 3.141050049061584 59.08061170377796 0.7522338106634571 
1.8579567685399558 3.196423046969919 58.637372782354035 0.7542356475861851 
1.7581685859663783 3.0012449769374734 56.8643489981436 0.7542356475861851 
epoch: 24, train time every whole data:212.09s
epoch: 24, total time:7045.60s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.58s
test time on whole data:59.18s
1.616475348893819 2.5690363317731837 56.96779771623844 0.8281451851204658 
1.6636925653616588 2.6616629860690555 58.602193108508615 0.8143054737143008 
1.6875908241884872 2.7225566747299554 58.47586651272155 0.805221086160844 
1.710438211947857 2.7568344031592273 57.963901338202575 0.8016410217782133 
1.7197415006372723 2.770695502100315 57.09695553347534 0.8023644026253319 
1.7356962192617358 2.805106960067572 56.40178568479469 0.8002295331808847 
1.7652470166683196 2.876428220885463 56.15305796962679 0.7913834782595858 
1.792083643725408 2.9270323743076325 56.1957018301724 0.7837237586024962 
1.816011474556866 2.988178503795719 56.19507771610274 0.7742695269205347 
1.8358176121398628 3.0547549480842138 56.38110280358279 0.7635969105318743 
1.8491207645250751 3.1115233711171886 56.49574511937416 0.7574850686249067 
1.8799825412709088 3.1947538604774417 56.67747835993705 0.7495007886820353 
1.7559914769314393 2.875649973387249 56.96717864920523 0.7495007886820353 
epoch: 25, train time every whole data:211.56s
epoch: 25, total time:7328.29s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.40s
test time on whole data:59.05s
1.6042698575531442 2.6490556485133467 55.55311963850218 0.8339738716808359 
1.642400285593633 2.7484877159340906 55.975322976640705 0.8246355013158485 
1.6888079540678078 2.856467188229365 56.01583351608627 0.8175814161698124 
1.7192200974628684 2.911853802231077 56.45524475284991 0.8137802419241967 
1.747947246216149 2.950547065255122 57.13842936448022 0.8086052556945985 
1.7687449761866814 2.9658575588412717 58.17942954460097 0.8042636685543868 
1.7885738276730159 3.004455999158468 59.28233155190298 0.793735659281289 
1.8051116555809443 3.0344278034267016 60.12203328892575 0.7849914016148375 
1.828273249712817 3.088272740012633 60.53851417117113 0.7757329812751642 
1.858713113932205 3.136382094110138 61.26706171288746 0.7686802754548249 
1.8837032174626809 3.1893971516544286 61.50476068435313 0.7641859805484205 
1.9321603025762097 3.3027928047862756 61.96289482620939 0.7506669006831729 
1.7723271486681798 2.9916765166675163 58.666382512249136 0.7506669006831729 
epoch: 26, train time every whole data:211.56s
epoch: 26, total time:7610.00s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.48s
test time on whole data:59.58s
1.6059245947767935 2.6154021483228505 55.41147792228712 0.834220790478187 
1.6336233627503471 2.680679948820557 56.55570079439869 0.8227421171191891 
1.6679761995566742 2.753099578694371 56.80332576320809 0.8138530210571014 
1.6908114379946853 2.8080608409273218 56.751757278205275 0.8072551331173489 
1.7245748576626536 2.883199875675479 56.519800154610756 0.7988758273087508 
1.750072858368091 2.9430259642458227 56.521089679864986 0.7902751354305935 
1.7646923098394736 2.978489148323296 56.40518338793138 0.7836482555308909 
1.761043556505016 2.9683918880413267 55.991637447446294 0.7825655546099918 
1.7610839058570564 2.956756643620172 55.72472783393716 0.781732227052052 
1.7608862783527446 2.920890543221686 56.54726181965574 0.7830708299727975 
1.782050618569145 2.933534465344001 58.14180338874048 0.7777427282701969 
1.8159751533266335 2.9957173933645085 59.12336057033026 0.7670857504415871 
1.7265595944632761 2.872311587878713 56.7081102529862 0.7670857504415871 
epoch: 27, train time every whole data:211.37s
epoch: 27, total time:7892.31s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.53s
test time on whole data:59.11s
1.5491816681955186 2.498002327212147 56.53059998452136 0.8405115457862203 
1.5794429554567628 2.576980097619736 56.96654000167464 0.8307445626790829 
1.6047785513144697 2.6248172732326385 57.102514113896376 0.8244689509486716 
1.6137967541073228 2.644214688233703 57.0173856195394 0.8217449495230107 
1.630617673809862 2.664682190070602 57.114553051731534 0.8187033961655231 
1.6423687510726352 2.6862935653091324 57.76870516148984 0.814962831624394 
1.6609593683457269 2.721497039043871 59.13430487529037 0.8083128202000676 
1.6786226256967833 2.7513812186502222 60.641592615499604 0.8026678593831174 
1.700603684809325 2.7895893420026736 61.888012909523404 0.7962533467481268 
1.7188622152481583 2.8265994213934684 63.015776724656966 0.790362525256165 
1.7290287817866496 2.8452956060115193 63.16920184750349 0.7875563663870943 
1.7430080527510672 2.885760950139065 62.76125990721504 0.7818071840737782 
1.65427259021619 2.7118647405975707 59.42600931535183 0.7818071840737782 
epoch: 28, train time every whole data:211.54s
epoch: 28, total time:8174.87s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.64s
test time on whole data:59.25s
1.5582485459720095 2.539765988518157 56.50771426268847 0.8403079547151205 
1.5826350449211009 2.599063531664349 56.6787638628935 0.8333163886770805 
1.6035867731717548 2.6386343200394333 56.699165365849524 0.8288402013242652 
1.617369714801244 2.665271041802837 56.59014753805267 0.8261283952692053 
1.6442127905438344 2.7212800792797984 56.60845164594785 0.8206199918426562 
1.6698070113799934 2.769239533897731 57.062513341412455 0.8158953696485035 
1.69884802413856 2.83737065002064 57.813169609868 0.8061725470067557 
1.7232811165696809 2.8881817814244966 58.583393949977555 0.7964000570171961 
1.7523572432084806 2.954391237405045 59.204947516341576 0.7841308469106454 
1.776761503145631 3.017173216425684 59.9284806084505 0.7711085077701665 
1.791042812891748 3.04635918811833 60.16390791236322 0.7663873956790888 
1.8159077428322108 3.099399894890502 60.530097021934615 0.7588386882132306 
1.6861715269646873 2.8204058249126502 58.030974168808704 0.7588386882132306 
epoch: 29, train time every whole data:211.74s
epoch: 29, total time:8457.27s
fine tune the model ... 
epoch: 30, train time every whole data:434.71s
epoch: 30, total time:8891.98s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.74s
test time on whole data:60.33s
1.533166976756904 2.495354026444665 56.0775087202441 0.8436515828331069 
1.555661521809619 2.543986971010789 56.6442166055261 0.836633935657482 
1.5761744643176596 2.5797499580495793 56.96633900070711 0.8323325232397861 
1.586424750916039 2.602044257089615 56.917987784166776 0.8304267017905866 
1.6039372925306892 2.641704537039722 56.85585958934902 0.8261342537609574 
1.621631624080418 2.679482666653952 57.28793291468741 0.8215059013972469 
1.6457704187286575 2.7352181731551486 57.84174032922692 0.8140152467413918 
1.6671547027249776 2.7856388470893902 58.30587332734627 0.8065194416901514 
1.6935775381835798 2.8503024954207614 58.71420248349328 0.7971798797812688 
1.714695270256982 2.8960247226152873 59.45491123395483 0.7901755628899241 
1.735194817515711 2.9311114226616706 59.95865804982344 0.7853869652232952 
1.7644036013468922 2.98740838950307 60.28331515504791 0.7795024346649787 
1.6414827482640109 2.7317736091242817 57.9424486670492 0.7795024346649787 
epoch: 31, train time every whole data:434.59s
epoch: 31, total time:9399.31s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.44s
test time on whole data:58.97s
1.5292401090203118 2.478919365710468 56.21628666322594 0.8454086035752307 
1.5535006097536534 2.5398004814286783 56.79657670087084 0.8375381283458226 
1.574995212834772 2.5776366201200855 57.10670279183996 0.8330031032589338 
1.5876418860792405 2.6068481807852666 56.98023752176259 0.8305978543559777 
1.6064196082455593 2.647040412600582 56.76267208246759 0.8265628026762825 
1.6267717144978897 2.6896912689222026 57.09727140001311 0.8216413697009883 
1.6522271467371001 2.749953271572486 57.5559542170957 0.813696974425492 
1.672153414813092 2.7968891635902646 57.9697884264875 0.8062136844011281 
1.69501367784531 2.853095301967566 58.343322800301344 0.7974192540717081 
1.7123616793442162 2.8866214348374575 59.12135531752687 0.7917227856276118 
1.7329511801764546 2.9219357459032063 59.74127180787755 0.7863985849725459 
1.7589399466103919 2.968043703317505 60.16760428974016 0.7814056585172575 
1.6418513488298327 2.7307098861173333 57.82164552415403 0.7814056585172575 
epoch: 32, train time every whole data:436.77s
epoch: 32, total time:9905.69s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.58s
test time on whole data:59.18s
1.5311176690079626 2.4895514869530677 56.09789974750047 0.8457814308959595 
1.5530583376636995 2.537101662851974 56.73391791401258 0.8394324168784698 
1.5744809855510968 2.5760332396357186 57.21886317816202 0.8344587677749375 
1.5877833937924533 2.604781329101664 57.2761916320742 0.8320167609479178 
1.6070000284687571 2.647283628089193 57.31949720651033 0.8275787471693166 
1.6269396988800948 2.6879655792165598 57.803476948858965 0.8231536389915209 
1.6595802101625927 2.7574773891566293 58.576353735650514 0.8141874969178058 
1.687975894820832 2.8199778323173352 59.22497369617571 0.8046974292398269 
1.7174997430614063 2.8924881680492263 59.7652130862621 0.7934293487900551 
1.7393805815369956 2.938420058237627 60.57749017836238 0.785424879795975 
1.7545204075273304 2.964185599630567 61.00243864596601 0.7812096912274947 
1.7777522621098907 3.0060797664294983 61.37892199382055 0.7759571060902323 
1.6514241010485926 2.748765217317385 58.58136302343131 0.7759571060902323 
epoch: 33, train time every whole data:437.30s
epoch: 33, total time:10413.35s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.38s
test time on whole data:58.94s
1.5261880612435441 2.4909643298481656 55.41994496624317 0.845851568197926 
1.5505511553279523 2.548617723761487 55.62683202788533 0.8393317915500161 
1.5746966716041344 2.6006197218730955 55.753581794679285 0.833876729549559 
1.588452209482147 2.631592671141347 55.714493217076644 0.8317563617752075 
1.608541085921228 2.6737472114728393 55.78004664255428 0.8274301606025158 
1.6263831606539232 2.7076304312071477 56.39759741085213 0.8225024842085316 
1.6504513559429241 2.761285247574871 57.268165621433894 0.8139550258638536 
1.6705049225785549 2.8076178227027 57.950667470945895 0.8054068516914406 
1.6936547869159175 2.8589486200374603 58.45983437001014 0.7969304752495631 
1.7079400841631882 2.8892134146356048 59.05718217362291 0.7914835718485185 
1.7216379664933221 2.905634429596672 59.33310607272898 0.7892085935894197 
1.7428839404155456 2.9445604600864077 59.643072568112274 0.7849816877029927 
1.6384904500618651 2.738776314153578 57.200461743595135 0.7849816877029927 
epoch: 34, train time every whole data:435.87s
epoch: 34, total time:10919.21s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.57s
test time on whole data:59.25s
1.5238441950324035 2.4616259935860687 56.03879364023865 0.846883127840584 
1.5465320490070043 2.519126969154849 56.249722350619 0.8401392325653206 
1.5691774293544392 2.568947908701851 56.29281204926667 0.8353394880278407 
1.5856986806361626 2.6093347025325704 56.21014797440418 0.8323184006221992 
1.6089223389310674 2.6586166216227722 56.23019531806178 0.8280167807386494 
1.6300558241850565 2.70373921335919 56.719139113568254 0.8226339783473611 
1.6582820287725577 2.76460465207409 57.50488023828355 0.8137709593293133 
1.6810537797302185 2.81444280067281 58.15711324391366 0.80492282565253 
1.708157043855992 2.8776605957744477 58.72667926695351 0.7938950021390337 
1.726675659587873 2.9157835690996072 59.47072626538493 0.7863363946640635 
1.7415051677770734 2.9364525396025494 59.871334206131635 0.782976582772084 
1.7612722278527384 2.9728232261891927 60.258120289874896 0.7782814262194215 
1.645098035393549 2.738604498332627 57.64421774773491 0.7782814262194215 
epoch: 35, train time every whole data:437.03s
epoch: 35, total time:11427.87s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.05s
test time on whole data:60.32s
1.5204749774330606 2.4675543036489764 56.28725343934645 0.8469830034349946 
1.5425463747189692 2.515387919071106 56.705048983785346 0.8409024274062955 
1.5630088317919346 2.5569409645135885 57.14448864686691 0.8352963642303102 
1.5761055106549924 2.5863459564039295 57.301036099312 0.8321458192665878 
1.594435938820243 2.6237772852028787 57.494874077585514 0.8279982096712917 
1.6132790364201757 2.6622092885115642 58.08567724873146 0.8235275104138772 
1.6437361869448353 2.7347782112533445 58.83776437537212 0.8139221041965177 
1.6711343758482309 2.797750473716116 59.46111611571643 0.8044328601468956 
1.6982635726003596 2.8615765009779275 59.9126355211497 0.7944550430774605 
1.717280341611022 2.8999269296379655 60.53956854870457 0.7875240817529808 
1.7315499690269076 2.921020306370078 60.822545621922444 0.7840583827983522 
1.7513376884692837 2.959397382281892 61.083476904403014 0.7792570596308706 
1.6352627336950012 2.720442779644834 58.639716786524396 0.7792570596308706 
epoch: 36, train time every whole data:438.05s
epoch: 36, total time:11937.67s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.25s
test time on whole data:59.80s
1.5181489205514747 2.457296029889865 55.87848271229293 0.8475036570688086 
1.5407321491129697 2.5092600242279204 56.12427422780456 0.8412782781328835 
1.5621285818981094 2.5546695634855925 56.28928317244174 0.8359884773413374 
1.5756671558861577 2.5851206598345273 56.29966631988053 0.8331106494780716 
1.5950296573502323 2.623880674517889 56.416810280167965 0.8288836980362928 
1.612560903334103 2.659935329574373 56.96287907176482 0.8241332510069265 
1.637900807511505 2.7185664457869074 57.72714228039045 0.8156689558168643 
1.6614242819916634 2.7727637804042353 58.41611850901431 0.8070134212113573 
1.6867028960943045 2.8333299340332685 58.973352840640146 0.797757251971687 
1.70566966503149 2.874252345170131 59.77233410654148 0.7912478554581209 
1.722393306368075 2.899823949729937 60.19566852387278 0.787712566920615 
1.7435820582467354 2.9344231737405955 60.5174053446643 0.7840461251398385 
1.630161698614735 2.7063445005321642 57.79787312994391 0.7840461251398385 
epoch: 37, train time every whole data:435.80s
epoch: 37, total time:12444.12s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.74s
test time on whole data:59.81s
1.5187433342531855 2.4568439551283188 56.801768348214146 0.847154160069928 
1.5404000167490117 2.505372346754097 57.116589373521705 0.8410180621156896 
1.5611800143143961 2.549890472836191 57.156350510548826 0.8357337985500625 
1.574022163273678 2.5789415509086577 57.076384893640174 0.8334549359562827 
1.5923522967182633 2.614213027790262 57.180990345244055 0.8299911801566838 
1.6092412761723003 2.649007313815997 57.73296180014954 0.8256512695253369 
1.63641212525609 2.7058133036156407 58.576550971593235 0.8176112561164282 
1.6601957201427293 2.7610770939934164 59.30115061607781 0.8086830878578863 
1.6861693656064924 2.821299591890612 59.81352518830592 0.7995291348828757 
1.7064095174067078 2.862281174815071 60.37514343914796 0.7932175387323959 
1.7222916078276578 2.892007502410055 60.469088920704 0.7892758443702397 
1.7437881257699892 2.9334673003134983 60.51902262413022 0.7849572027455896 
1.629267130290875 2.698510365699004 58.51004053150862 0.7849572027455896 
epoch: 38, train time every whole data:437.69s
epoch: 38, total time:12952.02s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.73s
test time on whole data:59.18s
1.51681969845836 2.460293800669198 56.06414041307545 0.8478724083796383 
1.5407206114621035 2.516793933223374 56.22525718146096 0.841319745504654 
1.5638759784569876 2.5672367485702003 56.208708038478704 0.8362132066915091 
1.580517699770452 2.611049823734429 56.139814603325945 0.8328463869470633 
1.603063384124211 2.6576502747578816 56.23273149540022 0.8286813340233136 
1.6223492721496593 2.696805120371247 56.75265760083952 0.823498053283349 
1.6487639220193737 2.7560658897567234 57.5639658927249 0.8141428563919233 
1.6699715331865563 2.8015079328041477 58.23622025199413 0.8058334271868777 
1.6964360563268974 2.8664055105506456 58.8047687122371 0.7950763124041567 
1.7148087245664958 2.9041635859068964 59.39320410714794 0.7887386113462194 
1.7295015074770366 2.927149746747008 59.591325034207735 0.7856267223013953 
1.7489153515189177 2.9619297113030907 59.774119668274786 0.782313627172404 
1.6363119782930875 2.732024152815152 57.58232026273302 0.782313627172404 
epoch: 39, train time every whole data:437.23s
epoch: 39, total time:13461.26s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.51s
test time on whole data:59.04s
1.51538428933679 2.4551885483064884 56.147917905381185 0.8478942018533389 
1.538366117379229 2.5121851046646824 56.19579993492134 0.8416450273660299 
1.5605351342146419 2.5637617043474457 56.35054508542936 0.8359718415173195 
1.5774186480633383 2.602655921366317 56.4439144926722 0.8327847929496796 
1.5998962807478827 2.6496756421857093 56.55894404403118 0.8282635339078344 
1.620009073790695 2.68889874843419 57.10889882812623 0.8238168977846453 
1.6490345439244771 2.7547527439838735 57.84502992079518 0.8147005512175676 
1.6740540994270927 2.8140092118265683 58.49676495819397 0.8047647470273244 
1.699682049936748 2.871129636907983 59.075661497246344 0.7954528343071611 
1.720513749923557 2.914854659674376 59.79591448894096 0.7878408821368453 
1.7362361617110493 2.9402387246486037 60.155337114833316 0.7839090082561918 
1.7556849207828442 2.9679602050480383 60.48720604274812 0.7812194005411119 
1.6372345891031954 2.7330925609100087 57.88858021336968 0.7812194005411119 
epoch: 40, train time every whole data:437.05s
epoch: 40, total time:13969.19s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 35.66s
test time on whole data:59.26s
1.515317194534377 2.4510046409893405 56.204376835845885 0.8483718713321573 
1.5388404640349604 2.507308109222352 56.37265016340866 0.841789665454484 
1.5601834739177887 2.556508318296173 56.60454610546739 0.8358707549039013 
1.575111159790396 2.5916423104919137 56.58227855222226 0.8328920238497808 
1.5963551405647858 2.6329644249637076 56.653859508100304 0.8290155517434503 
1.6164208152413901 2.673948809172977 57.1433023660459 0.8241776232695159 
1.646447527367533 2.7478727149126394 57.86405296027473 0.8140253596780712 
1.6712087992521092 2.8090356970588855 58.437327849892164 0.8044275176090949 
1.6972101354990154 2.871433040415271 58.848392041896645 0.7946198408692808 
1.71590278261421 2.9079309170192635 59.38762098856526 0.7884923183187492 
1.726738492687632 2.921447786830349 59.59074746062111 0.7867992160199591 
1.744775484248641 2.95040412534763 59.90303095616852 0.7839639937414515 
1.6337092891460698 2.723528407713613 57.79942191363672 0.7839639937414515 
epoch: 41, train time every whole data:436.26s
epoch: 41, total time:14476.68s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.97s
test time on whole data:59.65s
1.510952372229791 2.441779946763574 56.57830593395313 0.8488626082126081 
1.5329960822016 2.4969251336582436 56.72613084854626 0.8423877596039111 
1.554078586593714 2.5444313844449264 56.87477131908906 0.8369154806944671 
1.5685973001711426 2.577866350948579 56.893223987456786 0.834108241000559 
1.5897546946329197 2.621911753291588 56.95117260534827 0.8300210117943685 
1.6118592355417176 2.667140233047864 57.40678771968859 0.8249459409992167 
1.643331107732618 2.7370143307966286 58.05904439033927 0.8158166478330083 
1.6712039833670216 2.801401184170692 58.63220193231354 0.8060794305104118 
1.6995022266232187 2.8700864431659774 59.0466482913474 0.7953263397922531 
1.718961187856538 2.9109421099644783 59.649291090080915 0.7879221678422201 
1.7319858357190554 2.92993080073838 59.992259346899715 0.7845194114009961 
1.7488052171707331 2.9518429310901166 60.38621494914126 0.7820191390768728 
1.6318356524866724 2.7180332362024537 58.09974274951658 0.7820191390768728 
epoch: 42, train time every whole data:434.31s
epoch: 42, total time:14982.23s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.78s
test time on whole data:59.34s
1.514042579434812 2.4559821885939535 55.811483791474316 0.8482228819937782 
1.5370539094565348 2.5099653299374585 56.09516247533218 0.8416060698836958 
1.5584708277920172 2.5541937233271113 56.31126751880699 0.8366018959532906 
1.5747500147094862 2.5953907433172834 56.32704090176017 0.8332206319332354 
1.5960137386543765 2.6392687219994113 56.45890151249898 0.829314353377877 
1.6151795350872866 2.6757943254824634 57.06266901674128 0.8243277216249022 
1.6425803277336415 2.7376586835841645 57.89610265659305 0.8149351613509278 
1.667420128209605 2.7956177986200217 58.652276721394635 0.8049568297838819 
1.694337464600358 2.858142561098798 59.13177037669614 0.7949209853080176 
1.7127969986672202 2.8955487853020223 59.594750342847405 0.7886861991390967 
1.725148272827445 2.9126624792500886 59.70074222892183 0.7864605470390844 
1.7407482054229115 2.9381059530398286 59.86389337383729 0.7841077821874004 
1.6315451668829746 2.718713738990072 57.74225763015613 0.7841077821874004 
epoch: 43, train time every whole data:436.53s
epoch: 43, total time:15492.05s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.61s
test time on whole data:59.20s
1.513114396161799 2.4514547140503726 55.871638334427374 0.8487012860125818 
1.53549136216087 2.5032717694457194 56.080960098709774 0.8423999061430419 
1.5549460309838787 2.5428018104851415 56.23715571111707 0.8377416256838295 
1.569685294209935 2.580427887885788 56.19400647916858 0.8343437883163468 
1.5872383063446198 2.6150401131641003 56.18680434750405 0.8313735718137977 
1.6053160692998873 2.650626195474219 56.67618801662911 0.8267901577115592 
1.6302305658694711 2.7081271745650537 57.34235583223444 0.818498482590573 
1.654392029852828 2.765540653710198 58.021062296394646 0.808897734815512 
1.6794336105592194 2.823671838172324 58.48387496196212 0.7997256859186985 
1.6964637785786318 2.8588163020661863 59.05844518745146 0.794017627774902 
1.708816178063995 2.875880396406957 59.30768851170928 0.791812328406074 
1.7275769804406556 2.9076979006657844 59.59625290954668 0.7886012059638687 
1.6218920502104825 2.6944111795820804 57.42144188828756 0.7886012059638687 
epoch: 44, train time every whole data:436.09s
epoch: 44, total time:15997.42s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 35.73s
test time on whole data:59.36s
1.5161699654820953 2.476415189238963 55.09613339948205 0.8480691458533488 
1.5424812615239727 2.5382375600715976 55.365096291794856 0.8408904673100659 
1.5654505036748236 2.5883904195148326 55.5426272678058 0.8352374664276475 
1.5824385722802863 2.626678055953913 55.6804800270554 0.8320917485639556 
1.6025232276520913 2.6633633251832642 55.964870942621744 0.8286688552405967 
1.6200196076989883 2.6938305558858247 56.608637123937854 0.8252361530110663 
1.6483410453447807 2.757406817956451 57.481347661348906 0.8165723991106977 
1.6750910203143776 2.8189620976185994 58.27029420421821 0.8064015230389916 
1.7031976362975936 2.8821402527692177 58.89389882021809 0.7960393668829393 
1.7232513070760207 2.927046840148706 59.51824580470082 0.7877972972479795 
1.73533836165789 2.9423101347956595 59.781498517593754 0.7854525191650491 
1.7529325606701451 2.969512415827215 60.17395642736626 0.7822935679398548 
1.6389362558060887 2.7450573789650616 57.36485934353025 0.7822935679398548 
epoch: 45, train time every whole data:437.15s
epoch: 45, total time:16506.18s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.55s
test time on whole data:59.17s
1.5130057941916444 2.461489220341359 55.67352260546821 0.8482312181963709 
1.537928532732562 2.522998336749944 55.75812914262648 0.8416110955051094 
1.559465980533599 2.5678683593627714 55.938504903268935 0.8364551622671333 
1.575919481986158 2.6088604505611324 56.02397513559231 0.8326474011486834 
1.597302691882831 2.6491813643959636 56.220460898752535 0.82879768541849 
1.6173702621614294 2.686864521281075 56.76780825702543 0.8241799312023596 
1.645285347731252 2.7470569445993074 57.48222542671455 0.8160264311242104 
1.6701346075620858 2.801384777489256 58.122498498264896 0.8072776315739437 
1.6963386345516358 2.8604826854394982 58.64004457961213 0.7977398781025843 
1.7160454523259152 2.899877714408821 59.29823507173758 0.7902327268336762 
1.7289288429585064 2.91771508183424 59.77995017076964 0.7866537126648249 
1.7450015062202833 2.9412003235356394 60.36658682157303 0.783464257007587 
1.6335605945698253 2.726588291411892 57.50608264004279 0.783464257007587 
epoch: 46, train time every whole data:436.53s
epoch: 46, total time:17013.45s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.93s
test time on whole data:59.48s
1.5103412602544717 2.454093401398171 55.43463462040581 0.849280542363515 
1.534140455151775 2.507277759846609 55.71540790119669 0.8431390400352431 
1.5539685851825136 2.5490615601455975 56.05702711139388 0.8376829146977194 
1.5686668957119718 2.5840106990101916 56.250943437806214 0.8338282257193642 
1.5866773600453246 2.6159025286674424 56.48123891868442 0.8305939649794783 
1.6056177702235679 2.653087325563153 57.04384804898037 0.8260919694966167 
1.6352312084423113 2.7217975755808146 57.78338252341509 0.8169094703234263 
1.6644868969746112 2.790406091382103 58.50836341884087 0.8064221554675719 
1.694096191788447 2.8575307756910613 59.02951261781075 0.7962797714720453 
1.7165098777561494 2.9063546959348625 59.604574072329186 0.7882613231760395 
1.7318979588323937 2.932801079159653 59.926941069877614 0.7842743448639184 
1.7504235397600347 2.964071804873034 60.28328071215131 0.7806188441551255 
1.6293381666769644 2.7166099048047374 57.676691077887874 0.7806188441551255 
epoch: 47, train time every whole data:436.25s
epoch: 47, total time:17520.98s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.83s
test time on whole data:59.94s
1.5144637851687592 2.45884717859284 55.87676627330155 0.8484864237967831 
1.5390623342035605 2.5166366712594606 56.1881808830644 0.8416082178842565 
1.5598659975870202 2.5582516645491373 56.480320977371846 0.8365966022178392 
1.5743436171614698 2.5943601019640257 56.57038056383031 0.8331317330294105 
1.5933210626160283 2.6289353392113872 56.80888865328478 0.8300258937784176 
1.6124138658530123 2.6654216805547852 57.41164436424383 0.8255410149279951 
1.6394766835015089 2.724499719602757 58.217844317943054 0.8170106395542079 
1.6640565883621927 2.781267464011652 58.946901222710366 0.8073262024816674 
1.6883878205750316 2.8356835198772163 59.45161452542843 0.7983579905985051 
1.7050519219467506 2.8712281953599526 59.98198854103576 0.7920068491354612 
1.7157740215765578 2.886889322849802 60.18373997536636 0.789519897260342 
1.7315665408303695 2.9115674394587483 60.42092551878656 0.7868688900457808 
1.6281486866151884 2.7068627901083535 58.045025914117375 0.7868688900457808 
epoch: 48, train time every whole data:439.25s
epoch: 48, total time:18030.80s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.58s
test time on whole data:59.19s
1.5080304108029674 2.445264575857738 55.73724193675419 0.849455158324208 
1.5314425045026554 2.4997468243024317 55.871447744535075 0.8431695986833095 
1.550920115152019 2.541110691859324 55.96182031911049 0.8385392046137412 
1.5670572317506941 2.5803730718016564 56.06275853217001 0.834587554207527 
1.5865519474871634 2.6165521047037554 56.28169865351118 0.8312132407570346 
1.6050162373499146 2.6535571626112153 56.799865872293765 0.826974935746889 
1.6336635877152106 2.7171587176165706 57.498699899345304 0.8188464637507424 
1.6627676001842178 2.7830493598168484 58.21068644501109 0.8087593705951144 
1.6920468810125298 2.853012540483569 58.73155839084065 0.7979008311681958 
1.7127609204722658 2.898807874952407 59.30932975032038 0.7903023066287367 
1.7277734947009455 2.9247448905933693 59.67063219144018 0.7865182674841077 
1.7473090200495152 2.9570202223450646 60.05777358796567 0.7832812988944519 
1.6271116625983415 2.711096279793772 57.51621127287526 0.7832812988944519 
epoch: 49, train time every whole data:437.74s
epoch: 49, total time:18538.81s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 35.61s
test time on whole data:59.61s
1.5081478417757013 2.444326597209908 55.888576961477256 0.8494153981775091 
1.532455949009263 2.5021178427666877 56.0646384084445 0.8427604670006762 
1.5531560106572828 2.5461714941597116 56.39684437782767 0.8374605776833467 
1.5692038265831236 2.581763030017088 56.6238676954067 0.833567851154001 
1.5868695611179406 2.6141887167559674 56.94671731377525 0.8301184164344735 
1.6074655175076886 2.6543199181948087 57.54530728443046 0.8253227333757934 
1.6399596755972043 2.7303105788524404 58.3229097554447 0.81516092573635 
1.6706360291791635 2.800518227685141 58.983030274805216 0.8046756205631268 
1.7000631053840654 2.8697286417106405 59.44348027641284 0.7941931050975323 
1.724034759079594 2.9207182470726236 60.04230200566559 0.78585293602954 
1.7415973656979344 2.9509642265464437 60.420442226471685 0.7811346104246949 
1.7605864059876295 2.9837634792919365 60.804296149376135 0.7770634668220503 
1.632848003964716 2.7224021689161826 58.12363164890921 0.7770634668220503 
epoch: 50, train time every whole data:437.95s
epoch: 50, total time:19048.08s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.23s
test time on whole data:59.82s
1.5117848001325591 2.44892552198586 55.53716464462888 0.8494524195995068 
1.5370990135861295 2.507781731065401 55.69326886830726 0.8428954484319358 
1.560017419182385 2.5592699814792876 55.82134634754195 0.8372493712186561 
1.577508886143654 2.6024105737668664 55.91572252097994 0.8331327757462683 
1.5962838983910956 2.6370750421819777 56.15127505077334 0.8299970477977192 
1.6156918978284867 2.673236822538357 56.76531526022263 0.8252790145353837 
1.643228146313468 2.7383793260803824 57.59881038113901 0.8156509109758808 
1.6704200585187368 2.80114906761158 58.33757372436772 0.8054048377066944 
1.697885568628531 2.86674465154303 58.79017123355873 0.7951831486108852 
1.7176203472035094 2.911252565979689 59.29488641899846 0.787843338238555 
1.7311077452654995 2.931031504443608 59.55953464106748 0.7848956167455436 
1.7485623769035474 2.9601158061352697 59.9375801959091 0.781489536240772 
1.6339341798414668 2.724900212871108 57.450310806831816 0.781489536240772 
epoch: 51, train time every whole data:436.68s
epoch: 51, total time:19556.40s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.83s
test time on whole data:59.35s
1.5129030353630937 2.448029492749922 55.66984517708886 0.8489344388075558 
1.5395789219188016 2.5131065168211606 55.774276204331386 0.8416386023121987 
1.5599898630615678 2.5564532051924163 55.88332850188335 0.8369473185139781 
1.5765654902224917 2.595799734976965 55.923356549252155 0.8334664558204637 
1.5979083469736257 2.640164967750947 56.1820017803438 0.8292475767091237 
1.6174415429545832 2.6750681487071835 56.789817445001425 0.8252019296859792 
1.6462025361538288 2.739098539503865 57.65322390897792 0.8162154198841884 
1.6736723612058197 2.804712857087762 58.46286180196129 0.8055803140662166 
1.7009796718499135 2.8700849140129243 59.07950195229342 0.795072237375303 
1.7214079064694012 2.9124005356556957 59.75649154493921 0.7876863600727133 
1.7353232313142646 2.933848088758272 60.10703493288283 0.7841326117687692 
1.752175117030385 2.9600434699547824 60.50964598383794 0.7809329243009392 
1.6361790020431481 2.7259020516054977 57.64937961116735 0.7809329243009392 
epoch: 52, train time every whole data:435.98s
epoch: 52, total time:20064.01s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.56s
test time on whole data:59.13s
1.51070676686393 2.4306125573701043 56.81334785199906 0.8499641538105609 
1.5330198799406312 2.48356004522745 56.92033617669959 0.8435671810885582 
1.5516232813831774 2.525596876230612 56.958122599331475 0.8385761549939313 
1.5670803341382138 2.5612945879938382 56.979830531905165 0.8349985998216058 
1.5853088657581025 2.5983675376794446 57.128967467853066 0.8315159266368755 
1.6063062668589965 2.6413225032263936 57.64756038192363 0.8265758160745377 
1.6365562983739765 2.7113263416201203 58.41608135456108 0.817145232864632 
1.6655098701230295 2.7772259320078354 59.14861985409397 0.8071859408013544 
1.693983507451557 2.8440929085256306 59.61042681109403 0.7973513335838712 
1.7156088246868124 2.89239106672855 60.14023030221976 0.7899025972211324 
1.7307848156407653 2.9184753590510124 60.38917122234617 0.7863951534049561 
1.749630472276271 2.9519547827416024 60.776000398274476 0.782643386519973 
1.6288432652912885 2.700208576836552 58.41080471503366 0.782643386519973 
epoch: 53, train time every whole data:437.11s
epoch: 53, total time:20570.97s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.57s
test time on whole data:59.09s
1.5082566886465287 2.4463101109122336 55.8707161942635 0.849445241364653 
1.5321340148454266 2.5033587400414006 55.99955658207312 0.8431092821207465 
1.5525691568156084 2.5429118586439214 56.14342721532996 0.8389110319277174 
1.5686793056547288 2.5824000761464165 56.21291071220506 0.8352049979436741 
1.5873826849593649 2.617672562379202 56.43949306923931 0.8319164083635014 
1.6067107505016915 2.6579595530940705 57.0061783322881 0.8266782826119369 
1.6351967517342418 2.722816372356914 57.80629137341926 0.8174768264077235 
1.6630329572076776 2.78645911569608 58.50838628348697 0.8075871844449313 
1.6920470216218382 2.8528599099739917 59.06728244722896 0.7976578254256279 
1.7139699980220091 2.899601475690476 59.69620321009982 0.7903061427681368 
1.7296972682942593 2.9260300297548976 60.01806306371024 0.7868128101021131 
1.7499871015249795 2.9583451090460664 60.41678570367871 0.7835542063933798 
1.6283053083190295 2.7132610598679814 57.7655316564269 0.7835542063933798 
epoch: 54, train time every whole data:437.53s
epoch: 54, total time:21080.26s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.97s
test time on whole data:59.52s
1.5058677736718562 2.438998444295639 55.83153638342827 0.8499892537814905 
1.5291944298009787 2.4963125896890737 55.80249108833817 0.843994852403809 
1.551032451849697 2.5459263649311454 55.78730373360272 0.8390889128034211 
1.568212261636165 2.588494082835315 55.93733629038687 0.8347292737153006 
1.5866258240152329 2.623898496835812 56.229273665174226 0.8312084279491211 
1.6066942761988334 2.661924256923401 56.877795048139504 0.8264287435755455 
1.6350684972393903 2.7246508818530324 57.6800052168269 0.8178523904407407 
1.6643877765073307 2.793154121134382 58.403857069295675 0.8071409902509796 
1.6914267895357418 2.8563335721348566 58.90032997172183 0.7975273009214748 
1.7117230112475477 2.901471100164253 59.48149403289109 0.7899247449094134 
1.7256876962102417 2.9241441003561532 59.75903004542133 0.7865146289990419 
1.743795574907657 2.9515310169660864 60.120059529657546 0.7836944787475595 
1.626643030235056 2.7141263044345436 57.56763336200519 0.7836944787475595 
epoch: 55, train time every whole data:436.50s
epoch: 55, total time:21588.87s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.61s
test time on whole data:59.90s
1.5212571500948675 2.4637839488136097 55.258163133623704 0.8495934432745943 
1.5486310682120246 2.528620725986544 55.485612994906965 0.8425803081601804 
1.573295746183555 2.5811337732663127 55.684940514144024 0.8370398697215715 
1.5922302683023293 2.6214076621033753 55.930468658953316 0.8329783429541029 
1.6125284508932383 2.65750727991834 56.26884169284019 0.8298148807775174 
1.6343568967084976 2.7001294034786265 56.903418394767534 0.8245158092242945 
1.664337545631897 2.767514641838572 57.68556288291686 0.8150212415743306 
1.6911874415109023 2.828612195981948 58.36273303746823 0.805324823853726 
1.717676161774301 2.89114305506134 58.8786403300376 0.7954327921154493 
1.7378285133696738 2.934480602641945 59.51306014527847 0.787697135837113 
1.749927000566164 2.9536987033799296 59.78486849888276 0.7846887777709016 
1.765564823420807 2.977268788600825 60.11292842253163 0.7825024841756502 
1.6507350888890213 2.747267761449306 57.48920159826254 0.7825024841756502 
epoch: 56, train time every whole data:437.03s
epoch: 56, total time:22097.55s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.44s
test time on whole data:58.98s
1.5091266559247105 2.4613122642679284 54.72925911054982 0.8497144187147083 
1.5343902417698077 2.517962678364347 55.04172705944706 0.843355494771126 
1.5529492534352909 2.5502805349659328 55.57315718242526 0.8390959240387238 
1.5693541744970494 2.5886620032674714 55.88966743109083 0.8348520342811581 
1.5877364400077079 2.625631845963213 56.285862058885904 0.8311727756835146 
1.6088312111877436 2.666753165036268 56.95016104673726 0.8261786661034822 
1.6389576871707325 2.735974756304533 57.735516680801524 0.8167138263643275 
1.6677024311439268 2.800889811902718 58.3730335797678 0.806776796759889 
1.6958332594488525 2.8681624145559805 58.79708983053078 0.7967480157474118 
1.7175180414177122 2.917183934652274 59.303915509587114 0.7887130761090395 
1.7331543990403768 2.9415332079864736 59.55245048221296 0.7850389681755962 
1.7516888217094044 2.9707581861157353 59.87421629159944 0.7816496394094478 
1.6306035513961097 2.725706209472868 57.3422750879317 0.7816496394094478 
epoch: 57, train time every whole data:438.02s
epoch: 57, total time:22605.85s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.76s
test time on whole data:59.35s
1.5054762864033913 2.4450264601971545 55.75914082532958 0.849829095984083 
1.532263846042433 2.512554116198522 55.816595004107825 0.8429767992489238 
1.5547216360294038 2.562308617673475 55.918221399056414 0.8380326305784263 
1.5733801895609747 2.6056314761548873 55.98104572648903 0.8342222401374899 
1.5935186441632962 2.6431404618577643 56.21418529083982 0.8311568702516933 
1.6144937990141943 2.6848519282212493 56.797522016759615 0.8258363563421035 
1.6409713957588232 2.7423290427638096 57.57541374018131 0.817260169299533 
1.6657142935253324 2.7967723418864074 58.2666497340405 0.8079981690493233 
1.692035477282302 2.8589998685149522 58.773667076283544 0.7978685625138167 
1.713055429702536 2.9043372430078382 59.374943685965974 0.7898941159407231 
1.7271729617989844 2.9276429384685336 59.606540734904634 0.7862182818356724 
1.744177154227825 2.9530925532626418 59.83175059259612 0.7838508210819063 
1.6297484261257913 2.7246353392231484 57.493059019503804 0.7838508210819063 
epoch: 58, train time every whole data:437.28s
epoch: 58, total time:23114.60s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.16s
test time on whole data:59.76s
1.505850328125237 2.4336174215068467 55.93400852814517 0.850057602033862 
1.5307392308490262 2.4921487140155487 55.96037635044766 0.8438038305947033 
1.5512243987215417 2.5367569636369924 56.091020605334606 0.8388486998117618 
1.5683556928242601 2.577462253332166 56.17649757335911 0.8345638087012539 
1.5867900684811176 2.615568049350323 56.489987971386235 0.8302964174698073 
1.6049818211199627 2.6460238922946653 57.00432600239584 0.8265903509873108 
1.6326920938386271 2.7092270646453067 57.732174174977004 0.817874896884699 
1.6619866703181039 2.77354067476993 58.40713428599569 0.8079689882055697 
1.691874456747568 2.846190664803514 58.91537449793165 0.796962083059521 
1.7150940785524214 2.897928949149561 59.54795928533995 0.7885387865589458 
1.732557095389281 2.9281679584514837 59.90451552683824 0.7838105582301751 
1.7505917065548044 2.957480686621094 60.25275194838458 0.7803461247636089 
1.627728136793496 2.70657344022694 57.70143114082036 0.7803461247636089 
epoch: 59, train time every whole data:437.54s
epoch: 59, total time:23623.36s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 35.58s
test time on whole data:59.12s
1.5043929708787196 2.4360059731482964 56.341420904149366 0.8497165184747776 
1.5282227046190806 2.493361391022876 56.43666805792129 0.8433271030796337 
1.550343400366693 2.5398741454353795 56.52399336177886 0.838510452194231 
1.5679749639051124 2.581113314672222 56.67232577223863 0.834410692692637 
1.586677271091867 2.617707850792727 57.04144989686757 0.8306214422221656 
1.6072541222652155 2.6569691893408804 57.7316106464921 0.8254766833988845 
1.6349068637049446 2.717636060405002 58.6255914574005 0.8165161007561277 
1.6611094048174897 2.7750214019322117 59.35191001171744 0.807277866613359 
1.687746080705009 2.8387526782321495 59.82324216569515 0.7976573883760737 
1.709568293664427 2.887828580701496 60.3781797265282 0.7898228400668476 
1.7236512626680944 2.910892900770835 60.59806848159529 0.7866126858179158 
1.7414820305189738 2.9399481454029748 60.89475628084304 0.7832678837315832 
1.6252774474338023 2.7045781434078617 58.36836558954652 0.7832678837315832 
epoch: 60, train time every whole data:438.10s
epoch: 60, total time:24132.93s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.53s
test time on whole data:59.15s
1.5051511622127145 2.4395586252255295 55.65239692039421 0.8500194636232289 
1.5292693648400406 2.4957367692033996 55.95763880297293 0.8435290742396379 
1.5489566994611883 2.536892965367129 56.103220230591035 0.8390912627184017 
1.5655293275956952 2.5765233582763534 56.15745403204128 0.8352514944053797 
1.5837318214447726 2.611849608233938 56.292376764748994 0.8321168306691379 
1.6047699106118332 2.6538616381043916 56.77497500193527 0.8271242898346074 
1.6325413255314565 2.716289759233422 57.51641222315107 0.8182116248443057 
1.657501391242746 2.770508483371517 58.20980343943457 0.8091622215817323 
1.6831592613706987 2.830829512884008 58.8118953190986 0.7994700454732818 
1.7037174852516148 2.8776573173643856 59.53366245742716 0.791477148249317 
1.7200114687187154 2.902017624050425 59.93737926968623 0.7877011007907601 
1.7371297038279119 2.9265293378619015 60.26926229456496 0.7851489135926802 
1.6226224101757822 2.699597527814476 57.60146047723643 0.7851489135926802 
epoch: 61, train time every whole data:437.20s
epoch: 61, total time:24641.76s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.17s
test time on whole data:59.89s
1.512058486742544 2.429472719486632 55.925429783693836 0.8502954566653353 
1.5361728497238032 2.485984314719618 56.15686969890751 0.8435849596187782 
1.557545838179865 2.5302770647548214 56.34080550439342 0.8383928567008733 
1.573334324433158 2.5688788237368234 56.350543879337366 0.8344118959220538 
1.5922481175267271 2.6072740094868214 56.58569479865976 0.8308817189520158 
1.6146532897873827 2.6534565805637342 57.02878627152975 0.8258397655477295 
1.6442721387490276 2.7214053096491955 57.68094402991858 0.8168472359436029 
1.6717288021053232 2.7881434470642272 58.257683307810936 0.8068351464932454 
1.6971898452368166 2.8480227028371896 58.634165974953575 0.7983170582915523 
1.7169694887408544 2.892029187698358 59.20346843520707 0.7911810248259986 
1.7311995910903706 2.9151181786821385 59.55777922247062 0.7876665466216701 
1.748275863396802 2.9385082908554554 59.939126566888824 0.7850959754199615 
1.6329707196427228 2.703550251967935 57.6385176647752 0.7850959754199615 
epoch: 62, train time every whole data:437.98s
epoch: 62, total time:25151.80s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.99s
test time on whole data:59.68s
1.5041142115019084 2.443732944405927 55.778480110457686 0.850003755550741 
1.528920763050073 2.501666791978374 55.986320844038026 0.8435446267557692 
1.5511882564496426 2.549293770762198 56.123291796796885 0.8385621800942038 
1.5702468207637292 2.5986835103938906 56.19786331683495 0.8337635843237798 
1.5909716885376366 2.6405125260098568 56.48957042967992 0.8296605323070311 
1.6138243783414363 2.683033571996092 57.101790280306275 0.8246410379919386 
1.6443795503432908 2.753490774510074 57.9494926349785 0.8148457581492398 
1.6712575919769173 2.815888148525009 58.68362951317826 0.8048155303084666 
1.6995904749551167 2.876848436849555 59.30257492198211 0.7953169545430979 
1.7201138867319872 2.921418248374681 60.010834729271856 0.787609269460282 
1.7353349999771233 2.9428317281829033 60.41377551062818 0.7842851836648428 
1.7538877395916552 2.97022239764453 60.747208624895634 0.7811503505501981 
1.6319858635183764 2.7303472978069503 57.89883491952269 0.7811503505501981 
epoch: 63, train time every whole data:436.60s
epoch: 63, total time:25659.42s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.59s
test time on whole data:59.27s
1.5052442418299616 2.4375551078705096 56.31535451158358 0.8498109883551321 
1.5284832523982261 2.493372512300801 56.362805048897805 0.8435203878786309 
1.5481757972256414 2.5367825008936262 56.381715666224586 0.8387583183994063 
1.5646430393315498 2.577176673004309 56.401484983959584 0.8347106086633275 
1.583687519944938 2.6151957659512215 56.601870184318024 0.8309725129641674 
1.604329102056633 2.6560987037960064 57.1363327485656 0.8261234355320815 
1.632629925560561 2.71801935007832 57.9263136150987 0.8175959884891536 
1.6595780597473717 2.777163383294877 58.639871986385586 0.8084121580053963 
1.6874921364458721 2.8434487429273894 59.196061178666945 0.7983244824547009 
1.708699390910211 2.890477193894919 59.813842522263094 0.7905943843475935 
1.722460908598577 2.9103978884037636 60.12053734173447 0.787804707946695 
1.7396748028423283 2.933927253830737 60.444512486194945 0.7853578130673107 
1.6237581814076558 2.7041398120115216 57.94514302760211 0.7853578130673107 
epoch: 64, train time every whole data:436.66s
epoch: 64, total time:26166.78s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.86s
test time on whole data:60.56s
1.5084621192724221 2.436167281254429 56.49816350309871 0.8499641665799655 
1.5328056713449103 2.4943456749986614 56.523520546797265 0.8438289541597328 
1.5557679475410175 2.5425129730245803 56.66938122326719 0.8389263258691038 
1.5738864543389175 2.5841406164062066 56.83318394966012 0.8346010494369479 
1.5917052233970415 2.6164787313301003 57.13217457590149 0.8313646371156108 
1.611169234135676 2.652047889355728 57.678343138978065 0.8271763860060138 
1.63918614769931 2.71461702846971 58.45233998452355 0.8185427912147921 
1.6665676445355195 2.7728477841876393 59.12976810403142 0.8094822777962581 
1.6935965996475093 2.837400975986062 59.64276867684445 0.7997339893795514 
1.7148329848393442 2.888111986078243 60.21021437733809 0.7914385440973971 
1.7287378282778498 2.908404098394836 60.444720682904716 0.7883663123126521 
1.7444164969466094 2.930313430912105 60.71650226757027 0.7861094081253439 
1.630094529331344 2.7029854183548587 58.32767898295702 0.7861094081253439 
epoch: 65, train time every whole data:434.32s
epoch: 65, total time:26674.74s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.86s
test time on whole data:59.47s
1.5044142908951534 2.4467955925658047 55.07580483045824 0.8502868384118941 
1.5305933895262755 2.5073831920130285 55.34760397762465 0.8438891498584089 
1.551278429754167 2.5484275232258864 55.72443892451806 0.8390143867296579 
1.5696144363817182 2.59073971954454 55.944057663744694 0.8343253474793795 
1.5897815345087576 2.6320970067553344 56.26800481422588 0.8300220346426733 
1.611677730783288 2.6728190118522943 56.833408292726105 0.825782292440966 
1.6421980156810687 2.7439985284553896 57.610386176447236 0.8163794838850382 
1.6701598718101602 2.806448828520608 58.25512641990812 0.8067181936527814 
1.6959988707881422 2.86545901596944 58.768271039049125 0.7973533930096227 
1.7147315158799645 2.9063341501445783 59.39846629596837 0.789791044860655 
1.7279162075437073 2.924923027186151 59.86603733716199 0.7864271798568545 
1.7445517111621087 2.950116377241679 60.34653007811058 0.7830293466167774 
1.6294096670595426 2.7214237736896694 57.453277832736674 0.7830293466167774 
epoch: 66, train time every whole data:434.56s
epoch: 66, total time:27181.89s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.83s
test time on whole data:59.57s
1.507881051345418 2.431328958293961 55.1967320042015 0.8505482699325982 
1.5318590722660579 2.4881503354807677 55.380810819992135 0.8442865533894517 
1.55264216830883 2.5333038725635384 55.557664342862466 0.8394516044174608 
1.5698727407503341 2.5718919141577854 55.67114229207416 0.8353900601493051 
1.5887715326527223 2.6109130554877558 55.90239385288205 0.831288167077484 
1.6092436239360166 2.648778770694555 56.364090477718996 0.8269660693428527 
1.6385212208651716 2.7185566001583186 57.00262904793669 0.8176541283384181 
1.6676721453314558 2.7868381016065413 57.59548710476904 0.8072946374129698 
1.6964510787281075 2.8548483668925555 58.088650552118196 0.7972548022660049 
1.7184953651354604 2.9036972549848903 58.706510710005475 0.7890567055280283 
1.7335243855343156 2.9292635686769954 59.07752112781508 0.7849958708693655 
1.7502730073712411 2.952211004362289 59.473407676631915 0.7825354099698764 
1.6304339493520943 2.7080462436365504 57.00150153775936 0.7825354099698764 
epoch: 67, train time every whole data:435.60s
epoch: 67, total time:27689.14s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.58s
test time on whole data:59.28s
1.5034690522964866 2.4374058356237724 55.634371365433545 0.8500197810542891 
1.5278238579950163 2.4965432319202696 55.73793151772344 0.843644982867822 
1.5510771695496959 2.5456544500041334 55.73728912956453 0.8389889683584307 
1.5697371108794496 2.590645427060768 55.812301484421035 0.8348446316998566 
1.5898390119372912 2.6319481704599172 56.06686367989607 0.8311628913044066 
1.6122804899378902 2.6768389895006464 56.65056752206864 0.8259767913711709 
1.640511001513118 2.740014231386162 57.47265651226656 0.8172283402392001 
1.6659533701578952 2.797469266456014 58.21793196713705 0.807667502984138 
1.6905628279468843 2.852664047656204 58.85667339697469 0.7989547340927305 
1.7112903563511515 2.897608091613675 59.58797609961278 0.7912424442143148 
1.7252981202738094 2.9199590994887883 59.9912426570347 0.7878119587972009 
1.7425591869698394 2.94469977845153 60.371611597285515 0.7849820410028867 
1.6275334629840439 2.7160456673549995 57.5115458863261 0.7849820410028867 
epoch: 68, train time every whole data:437.98s
epoch: 68, total time:28197.31s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.70s
test time on whole data:59.29s
1.5020463710622418 2.435595718273112 56.06463821863693 0.8501375503402041 
1.5268522164135994 2.492355918402105 56.34623231557681 0.8438296849127799 
1.5460784258648221 2.531265386807641 56.698082916733064 0.8392849757131443 
1.5620178709938413 2.569769587661245 56.86898867482897 0.8351986206863187 
1.5815383098953892 2.6095539540909263 57.1541902815319 0.8310421124590697 
1.6038223580937123 2.652703775670671 57.75647068351068 0.8257221456676439 
1.6326861249541953 2.718170514327472 58.513364493014606 0.816610155057416 
1.6610846764445304 2.7818787591728262 59.20564912683559 0.8068033623298542 
1.6874002279805995 2.842798497079318 59.66683424377397 0.7976692666938986 
1.708510420851498 2.8926343088391198 60.24356410313694 0.7893480820546921 
1.7220490065097276 2.9116414541058213 60.428710433159296 0.7866158613919897 
1.7389427262587207 2.9392506054257663 60.638824136679595 0.7836025568018615 
1.6227523946102398 2.703309405060787 58.298889772634 0.7836025568018615 
epoch: 69, train time every whole data:437.33s
epoch: 69, total time:28705.35s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.61s
test time on whole data:59.27s
1.5083650045429489 2.4538384094511194 55.29269388503049 0.8504513838261609 
1.5361376209358375 2.5220730364964616 55.51318145073816 0.8431596623839313 
1.556455325530842 2.5621925084508974 55.86851538573937 0.8385223914059053 
1.5746951015719346 2.6044784748921814 56.04981266395521 0.8342158924256958 
1.5927396925688677 2.638691062123829 56.33523436454867 0.8309435360670292 
1.6141423151381313 2.678627016516174 56.931112857942914 0.8262667528683717 
1.6427689891778643 2.7422836973476503 57.75437390573893 0.8173211218533892 
1.6700215386357158 2.801253283867033 58.461676908144966 0.8079399757901001 
1.6969438197985292 2.865214951760089 59.04361494653043 0.7978919440029553 
1.7174218824337282 2.9074193445826193 59.73210109431686 0.7903939958646004 
1.73081815277652 2.928653848474582 60.087413057504456 0.7869095211053861 
1.7474768617290648 2.9528680658174533 60.5054743373047 0.7839570085400948 
1.6323321920699987 2.7263249701302423 57.63136886386221 0.7839570085400948 
