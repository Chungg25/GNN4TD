total training epoch, fine tune epoch: 30 , 40
batch_size: 4
folder_dir: MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_ed_gts_tcn
load file: data/bike.npz
ori length: 3001 , percent: 1.0 , scale: 3001
train: torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12])
val: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
test: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
TemporalPositionalEncoding max_len: 12
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder2): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (src_embed2): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed2): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
  (prediction_generator2): Linear(in_features=64, out_features=1, bias=True)
)
create params directory experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_ed_gts_tcn
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.0.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.0.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.1.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.1.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.2.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.2.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.3.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.3.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.3.sublayer2.norm2.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
encoder.norm2.weight 	 torch.Size([64])
encoder.norm2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.0.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.0.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.tcn2.filter_convs.bias 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.1.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.1.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.tcn2.filter_convs.bias 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.2.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.2.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.tcn2.filter_convs.bias 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.3.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.3.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.tcn2.filter_convs.bias 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.3.sublayer2.norm2.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
decoder.norm2.weight 	 torch.Size([64])
decoder.norm2.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.0.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.0.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.0.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.0.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.0.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.0.tcn2.filter_convs.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.1.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.1.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.1.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.1.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.1.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.1.tcn2.filter_convs.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.2.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.2.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.2.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.2.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.2.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.2.tcn2.filter_convs.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.3.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.3.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.3.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.3.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.3.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.3.tcn2.filter_convs.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.norm.weight 	 torch.Size([64])
decoder2.norm.bias 	 torch.Size([64])
decoder2.norm2.weight 	 torch.Size([64])
decoder2.norm2.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed.2.embedding.weight 	 torch.Size([250, 64])
src_embed2.0.weight 	 torch.Size([64, 1])
src_embed2.0.bias 	 torch.Size([64])
src_embed2.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed2.2.embedding.weight 	 torch.Size([250, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([250, 64])
trg_embed2.0.weight 	 torch.Size([64, 1])
trg_embed2.0.bias 	 torch.Size([64])
trg_embed2.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed2.2.embedding.weight 	 torch.Size([250, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
prediction_generator2.weight 	 torch.Size([1, 64])
prediction_generator2.bias 	 torch.Size([1])
Net's total params: 761986
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483]}]
predicting testing set batch 1 / 168, time: 0.63s
predicting testing set batch 101 / 168, time: 34.84s
test time on whole data:57.99s
38.86298382323634 41.398370503637466 1582.4361428391587 -0.000841684221899934 
35.587119806358594 40.74232198302656 1449.5368988442056 -0.001348664853354172 
35.167837645189394 41.093574790484425 1431.505284583708 -0.0015458148733006308 
34.48002519884226 43.24971288904242 1402.6375793789227 -0.001182570936135999 
35.30501228634454 45.81493147681774 1434.7902344384063 -0.0011771793906221945 
37.41395433899104 47.494529517120526 1521.5012204525408 -0.0010505967158549686 
39.31102764002669 47.52878463046881 1599.3990183108274 -0.0008123451092261814 
39.14349821719917 45.58557962367902 1592.8299865121953 -0.0005921961961495437 
37.77405017257189 43.749421170592996 1537.410196892833 -0.0004703676870259911 
37.163469834524875 44.25706852109495 1512.7694700617408 -0.000314169416095706 
37.58959073515334 47.39008908198547 1529.8607879357855 -8.755993987199027e-05 
48.55507037875917 64.1560645098063 1972.518944383178 -4.621000092263875e-06 
38.02947000643311 46.42132347383847 1547.270102892155 -4.621000092263875e-06 
epoch: 0, train time every whole data:222.63s
epoch: 0, total time:293.79s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.81s
test time on whole data:59.61s
3.863726617182472 5.772544311350642 114.46698244974067 0.21360328060913966 
3.946882534788389 5.843784499287709 117.62915709594346 0.1911187810937219 
3.9906690325181753 5.910084462159638 118.63668240597534 0.15714821249570898 
3.963373430280902 5.887696774552962 116.64788139696138 0.1407122678001077 
4.00069048100089 5.92129076981799 117.68826488260191 0.1264774583776833 
4.055197858772728 5.971047340219987 120.10464433901431 0.12153132956367248 
4.046375595108916 5.958979117778296 119.68602743246899 0.12123104542234794 
4.033743035012767 5.945764016600674 119.08995200775708 0.12158258516714167 
4.094908899372975 6.003240763058251 121.92461959577653 0.12008012602437391 
4.147960185088217 6.0536277576349224 124.35296612797846 0.11850428290820968 
4.04699065375062 5.9599058696511324 119.22232232485305 0.1171723368940215 
4.004041220147606 5.9228402347776035 116.64086601383987 0.11537626792897364 
4.016213295252055 5.929647350782619 118.84097223031887 0.11537626792897364 
epoch: 1, train time every whole data:224.28s
epoch: 1, total time:593.05s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.08s
test time on whole data:59.77s
2.574942356429905 4.215939789069299 59.2052849868926 0.4927131287206681 
2.5800664561922173 4.162127126098107 64.04173973773032 0.45139680629625273 
2.648809658698294 4.217717188126406 67.44423016267133 0.41182392944226126 
2.797504488183363 4.314332459114745 76.64927610983007 0.35573964341225967 
2.9886435367513804 4.477753214144948 86.7086137968641 0.3028410779308339 
3.17976901173157 4.667731063162256 96.3315206024308 0.2603300571558761 
3.403519450686783 4.885073278013022 107.73008877586274 0.22572104619090838 
3.629802650268766 5.100288483127226 118.7236428654354 0.19562189883176487 
3.782963957073344 5.2570867317116505 125.50066665235218 0.17310980632557602 
3.8802272698055242 5.372312487279835 129.02853338370085 0.15974165605138177 
3.8552591878014306 5.341725461616412 127.24917628647519 0.15758204517926183 
3.5372550847600435 4.976675152637631 110.54389991885544 0.1640327320052519 
3.238230259031885 4.769826629282969 97.43123486452808 0.1640327320052519 
epoch: 2, train time every whole data:224.19s
epoch: 2, total time:891.86s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.66s
test time on whole data:59.36s
3.0615178320830068 4.676187388569652 83.17712690251327 0.4274519330965373 
3.417396283264671 4.946378662297774 98.41303912832043 0.3863808907412595 
3.8594935373431514 5.393785953439442 115.68903197495769 0.31266513785107125 
4.1633174313551615 5.74398958786896 127.51746672409261 0.25718228621696526 
4.3846040844636125 6.02777215342032 135.51040736366363 0.20899069197800965 
4.555092271783079 6.25191733735427 141.48352484733664 0.17261182212621426 
4.733975252579011 6.471138033157253 148.29334393622486 0.14509035994250052 
4.895536528949315 6.666399270128654 154.5332223795875 0.12376793389554064 
4.994581933025182 6.7950436878665785 158.05798084540186 0.10793308066769045 
5.061880284298832 6.883627171340337 160.20520208792354 0.09538627271774287 
5.043686144467709 6.863258933173916 159.31952786729713 0.08227397639784495 
4.913477052965157 6.728811007220746 153.43336036157794 0.07546473551577042 
4.423713219714824 6.1650554902155585 136.3042165393128 0.07546473551577042 
epoch: 3, train time every whole data:224.27s
epoch: 3, total time:1189.34s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.18s
test time on whole data:60.03s
2.5846170661880503 4.045417800234523 71.79329264898875 0.4821137959461557 
2.6218658588317534 4.061105406181816 75.48825042079844 0.4655116333604862 
2.699154410232657 4.142098985865783 78.52463160522917 0.4309632626649445 
2.806796238303007 4.252056138451832 83.71058203625002 0.39245525814729953 
2.9213508446948335 4.380320485505102 88.55782482677752 0.35345081277755297 
3.0316058189065327 4.515907704390375 92.94602927292289 0.3171838189483157 
3.1418023996005457 4.654511935603401 97.33100967655416 0.28644870308775205 
3.2308766236205897 4.761050446958296 100.46551962089198 0.26111054026855773 
3.287124858812296 4.829325917910351 101.79752119767633 0.24299206526875083 
3.3326556016865765 4.894034852467236 102.48332852164515 0.22985174769906622 
3.345632952240606 4.896943968327711 102.07933259470154 0.22225214285168282 
3.2282234150468416 4.739579016832486 95.35665371395065 0.223582126041467 
3.0193088406803574 4.525163862812031 90.87847855970304 0.223582126041467 
epoch: 4, train time every whole data:224.31s
epoch: 4, total time:1483.97s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 35.98s
test time on whole data:59.76s
2.364561714910769 3.9949355577616403 56.021724289460806 0.5714636093070807 
2.448368733100416 4.106139689462743 61.2354842715664 0.5102520456994504 
2.5132900842838874 4.193587832812298 63.9926380650341 0.46700865977237555 
2.6128274898707335 4.320101466902511 68.97279589843033 0.4190412010999836 
2.7269375126666966 4.475295442316764 74.07870922726622 0.37422822685704127 
2.8424297703168397 4.63165535236992 78.87350122819242 0.3344954084516446 
2.95405769638185 4.773020757215695 83.72936194201358 0.30168701030716794 
3.0393355538497366 4.865865562265365 86.86909267218212 0.2758940345995077 
3.1030547422453583 4.924579136429899 88.48513472439348 0.25554733984995387 
3.1679468437552982 4.992172287765185 90.26724873883873 0.23974249985442034 
3.1885700985988867 4.997707735210002 89.8422726245518 0.2309416836764266 
3.119848566739421 4.870878876452977 85.93577437982766 0.23031764084639295 
2.8401024005599913 4.608723899042345 77.35935751221022 0.23031764084639295 
epoch: 5, train time every whole data:224.33s
epoch: 5, total time:1778.49s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.03s
test time on whole data:59.76s
2.077695306061989 3.41473767944521 59.25950481194554 0.6937198052632485 
2.457677596741578 3.950491552009105 76.05385734456722 0.5480416450512593 
2.733743448797436 4.268871008155403 91.51081544968473 0.497998711233501 
3.08374684457339 4.683480914674193 107.68936157569591 0.4376949568866553 
3.476905571122166 5.166007815254123 123.88337336993575 0.3745873522160883 
3.878520036857664 5.631366937368611 139.4661473499767 0.31960221311268705 
4.301412812652332 6.082285797672743 156.5942803621305 0.27981629498544663 
4.609980758379169 6.389964945788388 168.44005393692564 0.24520156199267987 
4.78651599617675 6.570605234452517 174.7661749345729 0.22232134435363182 
4.912015530251321 6.7171292452477775 179.10149861581817 0.21083671521360872 
4.932493157655976 6.753212692111769 178.6469518815926 0.20674402474004705 
4.5400272896634855 6.18209319225688 161.6907534669412 0.20788351937382063 
3.8158945290777715 5.59588903641818 134.76103792092127 0.20788351937382063 
epoch: 6, train time every whole data:224.17s
epoch: 6, total time:2075.83s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.78s
test time on whole data:59.46s
2.225241824899046 3.070440321268656 75.52142776419404 0.784171738285167 
3.54544463462285 4.496003527497542 137.82658780636788 0.5783708506172421 
4.3936070623710926 5.4215459548895995 175.6857571721932 0.4852316230531589 
5.138658503411781 6.274302480352047 207.24932482217935 0.3987261234361242 
5.547314609437471 6.840030929770623 223.12106365963928 0.3337889335260293 
5.887736924038402 7.326592123526488 236.61404973426502 0.293540043857618 
6.347367784322993 7.854112914868914 256.2570337259535 0.27190971441309847 
6.554650394820563 8.078605587121652 264.40459528297475 0.25469978072661253 
6.597446180554727 8.11526125759714 265.4642604029854 0.24049651044971995 
6.7990829329869396 8.287667404060334 274.05050914017437 0.23231040907084524 
6.934668190285032 8.394118332054857 279.85439967608 0.2263615976911838 
6.476150997397534 7.811182227729591 259.9407092723279 0.22615097384046465 
5.537280836595703 7.022798014709643 221.33590898596412 0.22615097384046465 
epoch: 7, train time every whole data:224.07s
epoch: 7, total time:2372.34s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.00s
test time on whole data:59.76s
1.7935165594587368 2.822187899152304 56.79728089807848 0.7959820514186502 
2.992688983878032 4.117766705300525 114.12629301116593 0.6016364676475632 
3.8055249682294114 5.025107838704432 152.94153553433253 0.5190428379984027 
4.755400680165295 6.017089969309378 195.4471017465594 0.4379621926370263 
5.461762190692038 6.802086789954431 224.00196267473444 0.3588729319862518 
6.15761158188876 7.612526210514735 251.11461427809397 0.292560824913551 
6.90299282639793 8.414911499077691 281.64370256157304 0.2515329956314008 
7.134901939636895 8.65386615282209 290.1447051056498 0.2254858184755652 
7.0512143486938665 8.574830937799778 285.30944383835276 0.20991729264061884 
7.1440497276859265 8.656959758113999 288.9691307652034 0.2050269560694618 
7.076704520707329 8.587682640030504 285.31393891865497 0.2058684219384926 
6.449205455573453 7.84541199087803 258.21831487201376 0.21355522556708334 
5.560464481917306 7.188337562195446 223.67338646621602 0.21355522556708334 
epoch: 8, train time every whole data:224.15s
epoch: 8, total time:2669.12s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.92s
test time on whole data:59.48s
1.451803836445191 2.705696956676519 44.315403575200044 0.8085291514368415 
2.4434859005513467 3.8270207663944076 88.13466642380205 0.6239402100593326 
2.9355635329448573 4.55336160275729 112.7352310427169 0.5543669889770861 
3.4588090077891414 5.240988840147065 137.89548353796596 0.4971610093841562 
3.8462272570987364 5.702025727633683 155.63989969474113 0.44635933261387806 
4.2606677987292585 6.207653587345744 173.96686374908327 0.39531282679271607 
4.759096554988818 6.791592799754478 195.02226381845844 0.34646671829644815 
5.045616884591324 7.073595740296565 204.280550539856 0.30119296102263443 
5.222511146322425 7.209104996200863 205.64565006781345 0.25848046795683743 
5.505308550020769 7.488874344249526 211.42286307051467 0.22251040857094276 
5.504151332364285 7.4919540642208196 205.45603745567877 0.2014448213551727 
5.1028719649486955 6.962744784465437 184.5942625445319 0.18416022206691446 
4.128009480566237 6.124004165399038 159.92881710352788 0.18416022206691446 
epoch: 9, train time every whole data:224.00s
epoch: 9, total time:2965.16s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.98s
test time on whole data:59.60s
1.3336560056040152 2.6923080063945286 42.96968608515181 0.8246378198493561 
2.2449091357068114 3.6894987463220574 80.67286333498787 0.6436013782351394 
2.587306901403215 4.226006916654024 96.58706548902926 0.5749788517324714 
2.957809001350775 4.754940544547533 113.87022367582058 0.5121489284437991 
3.2610872148466075 5.149164076321747 128.3143967239489 0.45705941252931387 
3.609455225267492 5.594577037514118 144.40712956681253 0.40504476850548277 
4.02152725166038 6.1021382031244435 162.26904855908793 0.3588258208749172 
4.257004966132254 6.340211893212146 170.7163594258157 0.31931254484955235 
4.355549957766065 6.396767114654218 171.0094932234538 0.2866545105107478 
4.478267549797006 6.516834887768725 170.70088840538588 0.2561905136840105 
4.31990416478277 6.3435602988251825 157.04218866405537 0.22780778254117648 
3.9844138227455494 5.86956377656717 135.69157267855678 0.18333627229806837 
3.450907599755245 5.436430971711449 131.18984507519036 0.18333627229806837 
epoch: 10, train time every whole data:223.88s
epoch: 10, total time:3260.25s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.67s
test time on whole data:59.45s
1.5705999471583358 2.7242750173795454 54.84874843606411 0.8131285414937472 
3.6516051609157807 4.851769058515492 148.31046606428356 0.578408518853382 
5.316894882824715 6.718045135012026 221.93859274958615 0.45344529110745124 
6.753227350535847 8.274098951095972 280.44211935504273 0.34840444956440153 
7.479221489536177 9.129530045909803 306.79773569871895 0.2741787153929085 
7.955853865879721 9.755415575556471 324.13747350346 0.23607436375736376 
8.52267506976665 10.389782533573797 347.3576714615675 0.21854884554945994 
8.606259996965882 10.507612310603626 350.1036771323808 0.2047542581296969 
8.433793141998262 10.372316727567908 341.69541179359777 0.19569773669359306 
8.60823078214927 10.550484283287224 348.0690520642779 0.19449591937214272 
8.676011214633695 10.642830981986863 349.8458046944058 0.19185431820563917 
8.164751234320569 9.868671039470499 330.306060429717 0.1960049343794718 
6.978260344723743 8.995305826917242 283.6593522647522 0.1960049343794718 
epoch: 11, train time every whole data:224.18s
epoch: 11, total time:3557.55s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.26s
test time on whole data:60.00s
1.3283087972211873 2.549569563825703 45.59187666179934 0.8399178551850542 
2.46498368592445 3.8815334420270258 96.63270795324583 0.6536936414654183 
3.0069281359629794 4.787502029611772 120.57936416611668 0.5757049222431305 
3.5086456067342136 5.488435080645791 143.58899079494483 0.5117653758179762 
3.85411444055564 5.910076008246132 158.89008925636264 0.4561270109297315 
4.252946075846752 6.421718709622881 176.28818966559018 0.40400575164962166 
4.7164235995155535 7.018621653895348 195.77550662720697 0.35817489818207443 
4.86277951576064 7.1686384059683625 199.5041758671312 0.32010270486671155 
4.82088069259535 7.12499563008184 192.35715794378297 0.2850191161459049 
4.8068835107247745 7.188475927959587 184.34277648886896 0.24274859477071345 
4.473867089569214 6.791617206798404 160.26172590538056 0.19646841876516735 
4.074296000260948 6.122333451639043 134.27123243441076 0.15599833046281195 
3.8475880958893085 6.037910637087548 150.67591315053156 0.15599833046281195 
epoch: 12, train time every whole data:224.48s
epoch: 12, total time:3855.15s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.87s
test time on whole data:59.60s
1.3066387532246078 2.4890368350743897 46.27050470178732 0.8404161370538199 
2.4909940285076875 3.7470689534377835 98.816388465965 0.6484655288541533 
3.2255766572048445 4.695367533334122 132.94696680873903 0.5640919626123264 
4.004211779987351 5.605872988499701 166.84103068773334 0.4907240495938805 
4.593827377104688 6.252564723450711 190.081781031251 0.42722275748861466 
5.167692129898107 6.8776750362802925 210.74562745277535 0.3689393762157025 
5.755031736358557 7.531040217085936 232.0427191859645 0.3155947494332588 
5.972662568339103 7.764682987319343 237.5683608147146 0.2701976694455064 
5.977872226633575 7.784322344406885 233.95846304593744 0.23524704214116032 
6.033139096231865 7.875726720535137 232.99837848446865 0.21265337772094162 
5.566067910575441 7.440332620862642 207.8925117912949 0.2003417141269721 
4.590411739055777 6.3791610842221775 159.85529674088983 0.18363313282311688 
4.557010500260134 6.427719644248585 179.17131197701877 0.18363313282311688 
epoch: 13, train time every whole data:224.35s
epoch: 13, total time:4152.61s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.04s
test time on whole data:59.68s
1.6894510652003367 2.7536463314530404 61.87106619241273 0.8029623584957227 
3.7111297874697264 4.822870338602434 150.14539347640724 0.5550799762463522 
5.244110071509899 6.53547082541979 215.9525489742295 0.4265343010999711 
6.19406705550743 7.648804033754015 252.94997143272658 0.3438110324658161 
6.621852808181728 8.2469173079829 267.68649662334144 0.2943808234901441 
6.977399721424228 8.720027495068205 281.260131089003 0.26744840235804007 
7.3924607403192315 9.181459455147728 298.71095095418553 0.2506471906158316 
7.525131755300044 9.326989008496888 303.8551318194224 0.23256713252821168 
7.565014440177275 9.398287247493883 304.57033150287816 0.21628698865359453 
7.793833583709739 9.644377643310015 313.52312004627464 0.2116535319878553 
7.853516877705852 9.729817111673887 315.1506616382332 0.2057157252206902 
7.267187430108409 8.974579372705152 289.9208525699424 0.2032643938688913 
6.319596278051159 8.185128353838689 254.63704460405364 0.2032643938688913 
epoch: 14, train time every whole data:224.33s
epoch: 14, total time:4447.91s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.05s
test time on whole data:59.93s
1.7789612473043657 2.5989008520682617 70.49166741731396 0.8335180002860662 
2.622960383955478 3.898606773802179 108.12536521652427 0.6479654796948658 
3.543743216704932 5.0836651271593425 152.71590743173803 0.5622855152741891 
4.547333809164929 6.26677708646005 198.92765747779154 0.48691564494183776 
5.409923774985241 7.259874643718102 233.50520542935067 0.4149020919217231 
6.201017044399732 8.152715940116549 260.6182024837866 0.34805526113687746 
6.919141722170104 8.921428889795736 284.79131252150165 0.2917169356277389 
7.21689285537095 9.211587934446285 292.12780813374957 0.2399832829912534 
7.3611310251709074 9.361531667724135 293.4145797516267 0.1973471068275661 
7.614789398143334 9.637477047732165 299.67648355579166 0.17160472052949055 
7.4422452420165675 9.485103144928088 287.7141128436263 0.1534943196711216 
6.530521330504279 8.422864309763517 244.9318691461307 0.13028268176669586 
5.5990550874909015 7.703453950482879 227.2577399935887 0.13028268176669586 
epoch: 15, train time every whole data:225.17s
epoch: 15, total time:4743.23s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.26s
test time on whole data:60.24s
1.2632705327012532 2.4015272383796926 48.9479830376614 0.8527439627003022 
2.9173132479364674 4.299326663878283 126.52457234385585 0.6538605743935759 
4.113647866474997 5.818962052088298 182.81134863875212 0.5671726908274142 
5.213142315264614 7.092904613100454 233.39150145383698 0.49210129926465845 
6.030041270663962 8.02764617696574 267.37744849662766 0.42105817461478345 
6.7386012835428115 8.827573961966749 291.9765843392217 0.3564056715875917 
7.373197136317424 9.504762536393542 311.80496853461455 0.29885121741506165 
7.610120177523455 9.720472077164139 313.97900253372444 0.24406451529939258 
7.710994992390364 9.818962239199864 310.69006208624705 0.19849223369232524 
7.956404135930485 10.0829732634611 315.03121474162043 0.1691173481962331 
7.749731957559784 9.885460143292192 299.9292298778645 0.14988529438679463 
6.846216567138121 8.820709694082549 258.25035362305357 0.1309779700339407 
5.960223456953645 8.211902046038158 246.7307192924726 0.1309779700339407 
epoch: 16, train time every whole data:224.55s
epoch: 16, total time:5039.12s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.86s
test time on whole data:59.71s
1.825089592070718 2.716426982728005 67.88254402654881 0.8431343549362206 
4.238129799438463 5.217337939590229 173.4203838203031 0.5994831464248274 
5.840175644975067 7.007845021061204 241.81244025196048 0.47730905037497834 
7.124670137181434 8.47842563366677 294.00752674176164 0.3725639114141902 
7.951232095892674 9.510607660678762 325.3237808429559 0.2945273004509094 
8.577188314910446 10.289815719047926 349.6800526420254 0.2571920496511576 
9.125267772980477 10.904213358081126 372.6011376294215 0.2423589863700662 
9.27498728604331 11.034921826275633 379.3420655236314 0.2297369399679137 
9.385412068777407 11.146919091555446 383.7387504263473 0.21693703022085323 
9.77271243207618 11.55702373434766 399.21817984372893 0.2122695555081672 
9.596685146351211 11.401823972264832 391.33086203290844 0.20994100305159324 
8.518443588557076 10.201696446202082 346.02909040091333 0.20580340975504804 
7.602499489937872 9.506538632155722 310.3710108722391 0.20580340975504804 
epoch: 17, train time every whole data:224.02s
epoch: 17, total time:5335.79s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.10s
test time on whole data:59.86s
1.2451730938300136 2.333818503662542 46.94969889297156 0.867793518850208 
2.7201583488082424 4.2983457867769586 115.33080134959712 0.673615566977854 
3.470388474705851 5.399742419230157 149.86539013239116 0.6015231940238798 
4.191722629766113 6.33387968956523 184.12240337010627 0.53532305419286 
4.847877922771055 7.111368895400247 213.3661980486134 0.4724057614596331 
5.598211691456803 8.000598419528979 245.40924596939254 0.41292459968487993 
6.358592791573278 8.894011469500755 275.6321244412213 0.358634863413405 
6.76818523782811 9.288470922639554 288.5598937729442 0.31220260203963424 
6.993981618514728 9.511317325798878 291.21347306926765 0.27494070971508755 
7.298840390268447 9.89838326637235 296.7708480999218 0.24460000130054702 
7.018799536200507 9.617588689491178 277.9921655069784 0.22094766336116253 
6.178213537526184 8.525739019449416 236.75198926567754 0.19091830918994915 
5.224178772770777 7.7823914316876355 218.50149663735687 0.19091830918994915 
epoch: 18, train time every whole data:224.52s
epoch: 18, total time:5633.49s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.87s
test time on whole data:59.55s
1.4168611773895543 2.366713712824853 51.4495026246501 0.8608642580502979 
2.515268412750037 4.081492052406494 98.10919577271922 0.6680824146830789 
3.1010836964533444 5.040527435612259 124.98657724139261 0.5938958928543939 
3.726016499412734 5.9644649107433425 155.45467430796154 0.5251702151609633 
4.308226656573514 6.745575339382468 182.77874509486747 0.46324964980659117 
4.904655861162093 7.559347830476713 209.4225920400103 0.40684146192792553 
5.472114289092078 8.353563290637847 233.62374558330058 0.3577082248720463 
5.723156206328599 8.652180030173978 241.68189729887604 0.3195509572119014 
5.8441863594394 8.812434497619758 241.47648446770532 0.28752522424352694 
6.032724031533248 9.125143013890543 243.46315629262577 0.25611157900555864 
5.7467455638604505 8.74625485291055 224.5786255350282 0.22515335346966506 
5.087863726905148 7.638846664814744 190.49771047790418 0.18885269190855025 
4.4899085400750165 7.224418223399478 183.13044687987048 0.18885269190855025 
epoch: 19, train time every whole data:224.46s
epoch: 19, total time:5929.27s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 36.00s
test time on whole data:59.67s
1.2925451216865331 2.2806650818579146 47.74873895355384 0.8742598192363277 
2.9761427205180127 4.351637469265838 122.60791875617561 0.6621162087201142 
4.075964612418697 5.679967048650877 173.08705799460807 0.5767799061199723 
5.176747651351172 6.903087592272087 223.4649474364752 0.5023897837790877 
6.146452955121884 7.989175744311984 265.557731250655 0.43326196513331716 
7.027957318727548 8.991921236906148 300.1801031359573 0.37426097608662473 
7.795731402371788 9.868201543674795 328.67356033911824 0.32707720463911527 
8.216691482820416 10.367385216180292 341.8023902110216 0.28447401365923974 
8.453281221169268 10.718796746169057 346.08095558569175 0.24667964730922218 
8.69020052216335 11.079941671106377 350.28231264410687 0.21811878322785702 
8.225999131100075 10.711071861515663 324.14626961936636 0.19803860870384116 
7.065574029812944 9.278762677303806 270.30826682392865 0.180193357680917 
6.261940680771807 8.622058381251128 257.8337772678831 0.180193357680917 
epoch: 20, train time every whole data:225.03s
epoch: 20, total time:6228.60s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.14s
test time on whole data:60.08s
1.246643279591664 2.3063526497911107 47.820681547065526 0.8740568334479153 
3.391023351832515 4.823961421563312 145.0689573687725 0.6515593215148081 
4.965979045854615 6.732416002714004 216.99065823803565 0.560079618865956 
6.413608914820034 8.385503930793481 282.66089112982775 0.4864473314688818 
7.621748058593256 9.7884460881403 333.817544387828 0.4152809874512167 
8.594819648337861 10.925771052330832 369.9292258774856 0.3568780693621659 
9.319702600033422 11.779433421533467 393.99660291958565 0.31031638603451683 
9.664042109435425 12.21486084497364 401.57226583924864 0.2651546781374478 
9.881683614354847 12.561422495557355 403.5274845578807 0.22435673771344808 
10.139075721650162 12.956066395528754 406.8202799462884 0.19480725493228065 
9.747121523162528 12.639922688330003 382.76801088345707 0.1711134711064353 
8.44447915687891 10.941647301513019 324.3761307320234 0.15080543858924836 
7.452493918712103 10.216458443962622 309.1186509547914 0.15080543858924836 
epoch: 21, train time every whole data:224.84s
epoch: 21, total time:6526.51s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.03s
test time on whole data:59.84s
1.2686177809906325 2.258731873504733 45.43469866763579 0.8709210121312354 
2.9779590068367265 4.325577276162648 120.82726867579319 0.6514218488461979 
4.31718203598252 5.953244924835112 182.5508854348544 0.5552077347071677 
5.770729418962307 7.560254204560123 248.65391376984465 0.47510671340039295 
7.184910271120214 9.142661461544991 307.8479602545384 0.396510997386479 
8.389376847488805 10.510880830630253 353.2944959508188 0.33287326880589435 
9.266103667985027 11.520200653311715 384.44547729891315 0.28942881527851533 
9.73182297921908 12.09239845575032 399.3164664098253 0.2552126490898567 
9.990134759565816 12.487889277416972 406.0018892360959 0.23174175110734357 
10.225825407843239 12.839348828402967 411.8699910147966 0.2182007448314238 
9.683112327341167 12.412313941497086 384.65990310519777 0.2006650234948005 
8.474649774941305 10.988698173045629 329.6137246040747 0.1781195280655672 
7.273368689856404 9.937439043944833 297.88328130378187 0.1781195280655672 
epoch: 22, train time every whole data:224.48s
epoch: 22, total time:6823.24s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.79s
test time on whole data:59.36s
1.2712592009664292 2.2078271542726235 46.708944550256035 0.886389566414173 
3.2894355980670524 4.644543818544703 136.7999178371229 0.6577321120966622 
4.891045476506952 6.544832991780135 209.64528221518566 0.5622415331361232 
6.491949209283062 8.389855017361887 280.64046524811266 0.4812269064713821 
7.999761727053051 10.159694159188957 342.70382776144277 0.4041605139461631 
9.274504869980294 11.663521609829951 390.77730146343487 0.34255206429723417 
10.13545442768825 12.715609967573869 421.27167598037767 0.29910644589259583 
10.489575248744428 13.245210583126767 430.9553820617882 0.2615245171898014 
10.717396310975065 13.678342067968142 435.0022598264358 0.23053703810439802 
10.913106432506815 14.057583498330091 437.7949011114224 0.20598179112624915 
10.339527494568022 13.62712746021383 407.7485702555963 0.18078079948470138 
9.176945150442599 12.247974207630513 353.4351386010237 0.1570843308889611 
7.915830095565168 10.941032959562532 324.4641618342299 0.1570843308889611 
epoch: 23, train time every whole data:224.06s
epoch: 23, total time:7120.83s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.92s
test time on whole data:59.60s
1.5202475116600593 2.3500803858957195 58.28690501729895 0.8627778895924328 
3.5624970327215713 4.864031223162826 148.70649548472917 0.6367711964037981 
5.454341232027681 7.046020495480039 233.6295677525463 0.5336256035656236 
7.236575863424245 9.03319055276782 311.3632724889656 0.44770416067621943 
8.800275097409264 10.810067647373888 374.1637960909536 0.36453452780769036 
9.971676102203539 12.180248106718796 416.89217407608766 0.3022388020970539 
10.686389305358576 13.048105366697504 441.95422569724593 0.2658662175278358 
10.924928564447644 13.44721731268381 447.9436491564406 0.2380628549881996 
11.123262911172052 13.792926803151794 453.1195418080081 0.21856733419235122 
11.391978892235795 14.117745435108947 461.41625477027833 0.20857359849776447 
10.994803297572636 13.795827778472214 441.7730643526122 0.19536685364539302 
10.094696264987101 12.7301243857518 401.04050256501125 0.17137849484056922 
8.48013933960168 11.249666011635059 349.1981902451984 0.17137849484056922 
epoch: 24, train time every whole data:224.35s
epoch: 24, total time:7419.22s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.83s
test time on whole data:59.55s
1.2424086300675712 2.167717175159038 46.170841815972814 0.8878973353614199 
2.972487745586339 4.374542577193678 123.80109681093545 0.6678766415634007 
4.290789013998583 6.003727214441976 186.6784740380179 0.5806152989996832 
5.6459075045918246 7.5837152495355316 249.8955798725473 0.5116500557086274 
6.890380156498048 9.038777045480437 304.1208482769959 0.44681874625216517 
7.926821223055766 10.234318839524974 344.71742554716786 0.38949903673696856 
8.672155438462893 11.091041558444482 370.4586229905519 0.3428864052944202 
9.023087666358975 11.552232074017978 378.73798755154183 0.29833638997530654 
9.213718472005622 11.896178866551729 379.9144234900835 0.2585884689809332 
9.401068774333224 12.207311803495141 380.87399719408194 0.22622571999907684 
9.046633319950708 11.939229845953362 358.94198440137643 0.19651065541613494 
8.179005922479467 10.827263032671038 317.92032658865355 0.17094773302841318 
6.875371988949085 9.620340891736435 286.8588474250452 0.17094773302841318 
epoch: 25, train time every whole data:225.08s
epoch: 25, total time:7717.14s
predicting testing set batch 1 / 168, time: 0.38s
predicting testing set batch 101 / 168, time: 36.27s
test time on whole data:60.32s
1.185060543101813 2.1062122923644497 47.710983694608274 0.8903366477119128 
2.7572621041830807 4.3531970665471 117.25621136236973 0.6688863039524463 
3.8817683418114624 5.9553475463986105 170.01974844419507 0.5869684171646338 
4.91098019149048 7.3564150415011245 219.23994029082982 0.5229719855441112 
5.762035074638824 8.513848800916131 258.54739259245446 0.4614761528766032 
6.473964663764817 9.464378294406805 289.49376470169705 0.4017818924184984 
7.013188817206593 10.157844532440553 309.6037036712273 0.3477030504346008 
7.22741398779764 10.471204715959768 314.1122802275554 0.3018166320072915 
7.34511067964314 10.671887715053167 312.13001543117537 0.2656619299376363 
7.470101024586175 10.821926639661855 310.695940813356 0.23753846624874528 
7.105973238652394 10.326089244435025 288.4342759527392 0.20584542018906848 
6.387745067852416 9.191821525399192 252.01950588245882 0.15092826339479806 
5.62671697789407 8.70813058234921 240.77666168850115 0.15092826339479806 
epoch: 26, train time every whole data:225.63s
epoch: 26, total time:8015.39s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.06s
test time on whole data:59.84s
1.2933451522320865 2.2144292601933993 48.963842915085685 0.8810191651830345 
3.485297915067169 4.921471489551662 146.61636094802654 0.6462136300118919 
5.534373539276953 7.292105323481895 239.86603091220485 0.542313232058157 
7.594491211913792 9.591711976660495 330.25401684966937 0.45619312178734506 
9.462561372475433 11.69264397560721 406.08608791793046 0.37039404668505455 
10.863696681131565 13.295203533947593 458.14875846130826 0.3014971218414544 
11.715861645911154 14.293110618026402 488.53009289675146 0.2622898158613745 
12.15242414891569 14.865525071832462 502.86581644814436 0.23364054055968822 
12.478799930919406 15.339333676809693 513.0592004480353 0.21259492839307575 
12.805005772995274 15.71753989620867 522.9899338270767 0.20417772774057008 
12.378918417767755 15.348701471980396 500.83053068600736 0.1977584689451226 
11.16968231987483 13.891039864462183 446.64167330012265 0.1805909003914907 
9.24453817570676 12.328616330993652 383.74657650856716 0.1805909003914907 
epoch: 27, train time every whole data:224.52s
epoch: 27, total time:8311.79s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.03s
test time on whole data:59.79s
1.073617279424199 1.935807595783243 40.96212607296393 0.9095980930654883 
2.7626342161280943 4.290346417885577 116.06983489329411 0.6819372532429832 
4.041069550109673 6.088119620112103 175.84885247569233 0.5945716975166837 
5.369305547792376 7.862267801649654 238.77861416018266 0.5202660120827225 
6.61068253074276 9.47561354679225 295.08373897864544 0.4495102214459268 
7.698853275478152 10.835995780724593 339.96203718708585 0.38518936292492006 
8.513568964049487 11.82001558229496 368.42603367233994 0.3294540914422421 
9.001161677604985 12.427885210483316 380.60516167800387 0.2785753331448329 
9.324778963925938 12.892745034733247 384.9214585232505 0.23695114326648098 
9.579943788248869 13.197111894734459 386.8484550875105 0.20898665022444413 
9.255644716792784 12.823424401075929 366.31183964890755 0.18636331153222432 
8.443652093897175 11.629695847310265 329.5173696684358 0.1633901419252693 
6.806242717016207 10.25704112024809 285.2845460061294 0.1633901419252693 
epoch: 28, train time every whole data:224.64s
epoch: 28, total time:8609.81s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.81s
test time on whole data:59.61s
0.9615288513219428 1.8609183158003781 34.65815917321422 0.9138668833074209 
2.3433205364345615 3.793873745193995 92.39257652825215 0.6923650721105212 
2.9812744824489843 4.795905118223981 121.09042001797714 0.6137098703445352 
3.6989950378072405 5.845666083178935 154.6638512965664 0.541984200704575 
4.435266340030003 6.884114568279043 188.45071223446152 0.47223858934484847 
5.176167321559752 7.880447806388089 221.6298823046643 0.40964992103516673 
5.767238174031622 8.664447863492505 246.32386736772105 0.35861808295529485 
6.076972397010152 9.093202731058197 257.1885634804903 0.3170636207545188 
6.232074243450892 9.348185480621217 259.881736602446 0.2812830754599309 
6.28394785844641 9.435983218462843 257.1987830499475 0.24649444965830594 
5.87697138033958 8.890499788318392 234.044409278447 0.19760801280087453 
5.155066058031239 7.78256165599539 196.60592439162247 0.12491863050783718 
4.5824018900760315 7.406731735567541 188.68152091775121 0.12491863050783718 
epoch: 29, train time every whole data:224.40s
epoch: 29, total time:8908.06s
fine tune the model ... 
epoch: 30, train time every whole data:451.89s
epoch: 30, total time:9359.96s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.17s
test time on whole data:60.13s
0.96442002929073 1.9000364525665139 32.8228286998668 0.9112408444603589 
1.7850036718102082 3.1015343397984445 60.29943602649171 0.744915356183078 
1.8756963300045935 3.311367074488127 62.4935055173742 0.7038860620864386 
1.9594569965273674 3.4748124256893456 64.50080514272707 0.6702698889438395 
2.0353610713352404 3.6272758189693137 65.03184730771679 0.6406091987028176 
2.109398595854019 3.773110996086283 65.38287465209407 0.6085954702177558 
2.1822154040299355 3.917338338332953 64.94339964876559 0.5734509103923713 
2.243911193931564 4.045799199400132 63.43941187618072 0.5504862334996082 
2.2915149495038425 4.151577697847702 61.94978367334102 0.540087867756965 
2.3358606596850744 4.238248848793135 60.71948152825558 0.5369077741201903 
2.4161287185842437 4.369597859844869 60.28559087170544 0.5334602269863306 
2.502253620400049 4.484021609313287 60.27678897867623 0.5184104137890894 
2.0584351034130726 3.761369877754849 60.179040125866514 0.5184104137890894 
epoch: 31, train time every whole data:452.73s
epoch: 31, total time:9884.30s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.80s
test time on whole data:59.57s
1.0245485736038535 1.8653345011635667 34.48539568393563 0.9138908274637269 
1.811426827997058 3.003112718765884 60.98503649875383 0.7568607164028512 
1.910963734918407 3.1911077549568403 64.56800343458518 0.718414133309097 
1.9842557896173425 3.3207635149118317 67.39717599839182 0.6898632619103212 
2.0304104015020035 3.4086319950576733 68.18356502909081 0.6707540270599754 
2.08095047489873 3.482564291380404 68.66848659747815 0.6541243638225385 
2.1326042645366066 3.578251430707482 67.91485897077413 0.631566642605499 
2.166660131805 3.6601113374706444 66.14543997117846 0.6139679096412668 
2.1815209624672396 3.7248133559811873 64.59016588211749 0.6004695217706545 
2.2101920584290333 3.798144645363035 63.60843452021565 0.5825663830782856 
2.241734570571709 3.8914827950875353 61.49450993670277 0.5670642282112645 
2.282117708745634 3.961262359203989 60.18256443207183 0.5510720024221929 
2.0047821249243847 3.4495349609383847 62.352208318867085 0.5510720024221929 
epoch: 32, train time every whole data:452.91s
epoch: 32, total time:10411.07s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.73s
test time on whole data:59.41s
0.9241269425909434 1.8699953241176825 32.09045766620679 0.9154880170655548 
1.7397019307172548 3.0220782829178376 59.91694236584814 0.7678435824454922 
1.809385286691288 3.210100315106198 61.09909111841245 0.7358825825753785 
1.877737895788004 3.359024214350394 61.9919368633277 0.7119621293287214 
1.9344251278292033 3.4813933947619766 61.6145699046558 0.6973528194347925 
1.982215761935161 3.5727520764960263 61.23524476817577 0.6850341053766756 
2.022550304429251 3.655541787072605 60.372479269801374 0.6730047120493144 
2.0423292970573086 3.684655293716651 59.859388999314 0.6746480288598364 
2.048053937485264 3.6906738766307052 60.34150322754711 0.6762475745251133 
2.072542414899649 3.7379154813402415 61.119468912015705 0.6648255731953685 
2.147361002470411 3.875144379154895 61.56362729141948 0.6423574966740584 
2.223397986961972 4.01212372371807 62.9279972027534 0.6075269741041903 
1.9019856574046425 3.473098060674052 58.67796135543647 0.6075269741041903 
epoch: 33, train time every whole data:453.22s
epoch: 33, total time:10936.29s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.67s
test time on whole data:59.34s
0.9224375600099919 1.858188663070685 32.35828990959037 0.9171205084853582 
1.718337703356874 2.9827429448932445 59.376718953440935 0.770559781845176 
1.7800495609881446 3.1640613308197367 60.13590194944278 0.7416634387766677 
1.8383740088411917 3.3007498060455593 60.39519049924606 0.7230622722392961 
1.8955541771000162 3.420565679510042 59.97984863117962 0.7113975335166001 
1.9514328144274298 3.5139740618805995 59.853905323720966 0.7014326684232792 
2.009822060903623 3.628530049204127 59.56791851390504 0.6879414856197189 
2.0567534919965658 3.7136510193522088 60.026247561839405 0.6792618033509848 
2.0900015186628416 3.77038084511168 61.16823466617161 0.6733863065877204 
2.116022648241282 3.824828655608692 61.80878240342097 0.6638658501022284 
2.18687700166163 3.9607341399270624 62.08401005918155 0.6466452680541273 
2.23891836803983 4.056670861093218 62.3907008787773 0.622691790775303 
1.9003817428524516 3.479112085258188 58.26240531234299 0.622691790775303 
epoch: 34, train time every whole data:452.69s
epoch: 34, total time:11461.01s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.78s
test time on whole data:59.43s
0.9050962204731824 1.8409924436407001 31.71433678648122 0.9188449036823428 
1.7239665182042156 2.9769803574054055 59.66369174495222 0.7749027131688628 
1.7934685419381906 3.173031518901512 60.385712890663115 0.7448872482064028 
1.86010692478264 3.3188474915315953 60.738201803288824 0.7254430952490462 
1.923734179155014 3.4415727755355094 60.50960635199621 0.7139946926065089 
1.9769064144267745 3.529120010705912 60.4370031821608 0.7038226091287199 
2.023406839901227 3.62585758908412 60.04669447179578 0.6910719137310519 
2.0498858285154262 3.6761940083556657 60.13052044384659 0.6872036324262717 
2.0673257456067065 3.701798508253074 60.788747670428336 0.6872772285059101 
2.085602580743265 3.730164296147367 61.59543452319254 0.6798167124602635 
2.150620962237939 3.8562078863576574 62.099685963083964 0.6638381882496345 
2.2064810786760813 3.9731574781245187 62.1288399051682 0.6394645439079941 
1.8972168195550552 3.4468422415948305 58.35346276449012 0.6394645439079941 
epoch: 35, train time every whole data:453.74s
epoch: 35, total time:11983.31s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.81s
test time on whole data:59.49s
0.9397591467073099 1.8349589330441725 34.174062423285584 0.9202113538984058 
1.7507339770327366 2.9988179320754784 60.496049982933954 0.7764911335652163 
1.8120954857123572 3.1740764945775006 61.00160672110876 0.7493008050068476 
1.8645669595645298 3.288402792070545 61.24867395843826 0.7329304689984245 
1.913823511294311 3.381084975503911 60.98193617096984 0.7231204983702963 
1.9496653193488185 3.4312715534628766 61.084742191104105 0.7150308964026315 
1.984462558643715 3.501058870716034 61.111124218207614 0.7015291926851469 
2.0014849731211686 3.5321570681302794 61.47179247643931 0.6949140511306061 
2.0115570271454217 3.5425206909870695 62.31535607226954 0.6919382014008477 
2.046070866667444 3.590715622749875 64.00308113405953 0.678757300333649 
2.1154313806855076 3.7394526197899167 64.90977114401798 0.6546780686066167 
2.201969515903188 3.919046800888705 65.35562181185996 0.623611145343183 
1.8826350601522088 3.366159655305879 59.846431197425865 0.623611145343183 
epoch: 36, train time every whole data:453.70s
epoch: 36, total time:12507.01s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.71s
test time on whole data:59.44s
0.9138672912421503 1.814563752491756 31.511553866776836 0.9204369563466742 
1.704408138758902 2.9029980229919277 58.06736375828241 0.7848436681759001 
1.7647041980594043 3.067832373805251 58.51416655313285 0.7587681531113202 
1.8083297837856447 3.168410221356765 58.77108038297201 0.7432863344178416 
1.837137126630527 3.2280080857174642 58.37118568419444 0.7373406418565716 
1.8564905085268297 3.2348473685118853 58.56412031606523 0.7353543288098006 
1.874392349358914 3.2641288335066894 58.53551997177786 0.7278973212008223 
1.8828848728228005 3.271704507843689 59.14840949359911 0.7228896640888932 
1.890196660580291 3.2744168206856834 60.35543146866482 0.7177279530765671 
1.9207048677172334 3.306752551749391 62.46403367995597 0.705654038644013 
1.9710427409196716 3.4215276195680913 62.421395731658436 0.6844236329082645 
2.028199476488378 3.5596780735045055 62.57845110937153 0.6542654795310177 
1.7876965012408956 3.1549946679546097 57.44218092561644 0.6542654795310177 
epoch: 37, train time every whole data:453.22s
epoch: 37, total time:13031.29s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.71s
test time on whole data:59.41s
0.8942535819611734 1.7715821467513289 31.778417586126245 0.9223299639021917 
1.703120829738233 2.8588502225152768 60.702949068892806 0.7829528499177506 
1.7588913139441007 2.99479662204374 62.19571573433009 0.7584018602328535 
1.7923782393393062 3.0643559311363187 62.60994839246836 0.7455707022690237 
1.8115563069011662 3.1062264473612244 61.835059331514465 0.7393556283705175 
1.8371528713813141 3.1303468722628645 61.702873545888274 0.7356819521937589 
1.8636188174532282 3.204161677347054 60.77473360087553 0.7232947479477119 
1.882710533064657 3.2568456881624703 60.35808747473782 0.714446118491525 
1.8952108378341155 3.2866650006317046 60.53773919518437 0.7088323232980039 
1.9189115206860894 3.3263464563626957 61.75345575245031 0.6988088090640281 
1.9587330514615668 3.4294650951355043 61.699202117534604 0.6808607378639178 
2.0088888624848886 3.5416568366549623 61.34621174059025 0.6571555754567678 
1.7771188971874865 3.1113144825663377 58.94142409698483 0.6571555754567678 
epoch: 38, train time every whole data:453.55s
epoch: 38, total time:13556.21s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.76s
test time on whole data:59.46s
0.8958665788094735 1.7540098678921143 32.74297618232663 0.9237216479341646 
1.7177944797433558 2.8437776969424995 64.02618265266106 0.7843903417112559 
1.7819428340526564 2.9929495690107664 66.63211628630037 0.7586616571804192 
1.8252041325598423 3.0762977460475227 67.6666603591394 0.743654237624727 
1.8455701026555507 3.1166424982857026 66.93886399694304 0.7353440824440195 
1.8678085162742506 3.1349034375388753 66.3723249196671 0.7316749958352493 
1.8847356957020682 3.194653490047351 64.32222007332228 0.7214478846788588 
1.8961809203939601 3.2448408854813415 62.8363670840684 0.7148119779695071 
1.9045044243034153 3.2761472346677825 62.2553976803883 0.7115895035963044 
1.9217208058575967 3.309424403684284 62.68178568987325 0.705524511013886 
1.96045662021362 3.419220278897587 61.80826062929473 0.6906239360645506 
2.0118787398922833 3.5448219392188873 61.43599296516299 0.6660434250930329 
1.792805320871506 3.1064867743824975 61.64343818178787 0.6660434250930329 
epoch: 39, train time every whole data:453.19s
epoch: 39, total time:14078.25s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.77s
test time on whole data:59.42s
0.9170145777769032 1.73868427590293 32.548081373864264 0.9253886069777968 
1.6943632379546762 2.8068566133649795 60.17310162121863 0.7923034555050408 
1.7402648038184714 2.9570286605693665 60.27722317495135 0.7677783775454275 
1.7716039107382475 3.040777335834209 59.7566571980339 0.7549427414553269 
1.7993621772962312 3.109041597306577 58.74774524315831 0.7486339710411316 
1.8273299517774333 3.1473235889407696 58.486293471567905 0.7456521304681148 
1.8637222842896979 3.246722344632127 57.86540156051198 0.7325142436112388 
1.900727678338598 3.3437185051671956 57.87999930672143 0.7196214884562595 
1.934542955725498 3.4226715290892518 58.31477790088469 0.707517997536802 
1.9663107863207836 3.4793026701997274 59.53635932895225 0.6920100224701877 
2.0224853240277265 3.6066833215556082 60.01060208871443 0.670230221638554 
2.0640037966610065 3.696713251810232 59.87694041363063 0.6470846273920395 
1.7918109570604395 3.1712160933941553 56.95628918569754 0.6470846273920395 
epoch: 40, train time every whole data:453.29s
epoch: 40, total time:14601.86s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.85s
test time on whole data:59.65s
0.8734021506035434 1.768622506391474 30.850231010081806 0.9255238108604787 
1.6781829960715202 2.8649207508398553 57.838851794522995 0.7977356559628279 
1.7380818706632015 3.057076046760878 57.82727266183456 0.7735178381431563 
1.777573296216006 3.154282334163975 57.8847767759131 0.760979168058443 
1.8137196720021644 3.21412417358855 58.27011889311397 0.7556821336324562 
1.83437709909917 3.208342559267388 59.15782435388615 0.7561908158160434 
1.8695146610699593 3.2706486618149615 59.546198184026345 0.7464159386528139 
1.9071162083861197 3.3520968635236246 60.07682078548048 0.7318054544551688 
1.9342648212453795 3.4132468222095653 60.75149674698403 0.7180950686966872 
1.9550935009391535 3.4463226062507615 61.92341467666738 0.7057690668495589 
1.9975888616130466 3.5425232229712273 61.960287553933604 0.6896470057903009 
2.0271443256767734 3.627751335792672 61.59751918065916 0.6697561204575996 
1.7838382886321698 3.1940857276333086 57.30737348732393 0.6697561204575996 
epoch: 41, train time every whole data:452.13s
epoch: 41, total time:15125.44s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.88s
test time on whole data:59.67s
0.875043442582889 1.7394787771370899 31.347280971971326 0.925938856584021 
1.657241595304172 2.7835238861878726 59.03137529617376 0.8010475251551583 
1.7016316679908583 2.9356528235958566 59.00067772385237 0.7800378460383468 
1.7343845177271537 3.0181923270675184 58.799368123202576 0.7702894002074377 
1.766580941703348 3.0832159253826643 58.4803482688391 0.7670191955071489 
1.7943373309695827 3.106902839947024 58.92129923332588 0.7657206352465802 
1.8389300600432215 3.1979818006438814 59.04313673212172 0.7533258865851371 
1.8894711452065303 3.3117815029828463 59.3600522309361 0.7370753282071419 
1.9364507947945524 3.4146025805929883 60.02542136430479 0.7207153380717828 
1.9775506665104379 3.492052313759151 61.12662702382109 0.7044447421547843 
2.0440899170850892 3.6324108099374794 61.31344437086994 0.6857129284129742 
2.087013532828362 3.7459082322939876 61.49461800738953 0.6625970457698616 
1.7752271343955164 3.161428487875889 57.32890098981617 0.6625970457698616 
epoch: 42, train time every whole data:452.64s
epoch: 42, total time:15649.75s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.07s
test time on whole data:60.05s
0.9473688595667481 1.8189347934126587 32.106878889246545 0.9231281747455164 
1.7096045404176805 2.9182638614379885 55.31442539746097 0.8028579759904527 
1.787273662856026 3.151173055125753 55.12979733644531 0.7757006124718501 
1.8464996744488322 3.2898964624803555 55.13776130450429 0.7586046888842779 
1.9070905519104784 3.411590982189789 55.43935968933892 0.7477467015368369 
1.9482790631763638 3.4619353379963798 56.07457632833551 0.7407816351469396 
2.0006664711323876 3.5501586232175164 56.679506609883134 0.7223959888185228 
2.03938741738908 3.6297947863226536 57.41141571837023 0.7007979469141921 
2.0564411431756757 3.6629889073280686 57.88581963548044 0.686728318122007 
2.0638259026348766 3.6630836765061354 59.01847348270432 0.673186611991889 
2.095468986904426 3.7282250936348995 59.438856476798286 0.6570800910684936 
2.111022161421499 3.7741918476589507 59.53715953909734 0.6418121036955808 
1.8760773695861728 3.378374116691087 54.93144248629118 0.6418121036955808 
epoch: 43, train time every whole data:451.80s
epoch: 43, total time:16172.96s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.71s
test time on whole data:59.31s
0.8592109206718881 1.7149637888644242 31.066387585716093 0.9277797857652159 
1.65126569601787 2.732547510199242 60.53136195204365 0.8069183900047917 
1.6990106086729182 2.878239277763613 61.223962763939646 0.7850817978856967 
1.7363377726579174 2.97236929818 61.390670105365864 0.7714517736505149 
1.7762379609909618 3.0553329783695027 61.21365834755695 0.763537428336514 
1.8182015222760715 3.1184436047390776 61.86259372648987 0.7548319576189534 
1.8714866409348767 3.2495732230839915 61.9717630773855 0.733713850433923 
1.9252787862303888 3.3788482027199342 62.21425717186707 0.7126735825407353 
1.9672385789971976 3.470513110180848 62.726338601388456 0.696428188966299 
1.9997925291090672 3.5228280159803806 63.86668686261546 0.6838839574655873 
2.0467524822211516 3.623019629813114 63.869781801598165 0.6697520962644414 
2.0730398948837427 3.700279526055118 63.58188078380569 0.6525610766239921 
1.7853211161386708 3.160120870557041 59.62691103771043 0.6525610766239921 
epoch: 44, train time every whole data:449.06s
epoch: 44, total time:16693.42s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.97s
test time on whole data:59.77s
0.8809648974524544 1.7905943269727862 30.70412654707411 0.9253312656262979 
1.6731051685834923 2.805940583382156 57.34729363673705 0.8118673927166582 
1.7392860667703762 3.001658057217647 57.68973244990407 0.7871096319457765 
1.7928152923461582 3.120566602733391 58.05693474909578 0.7702350672419477 
1.8452803851040525 3.2216979136982946 58.543889254108315 0.7588428116202187 
1.8811339679527141 3.2781849718929217 59.35711580017905 0.7463313313960941 
1.9238281500122199 3.3819388411717077 59.626828468563616 0.7255841248522901 
1.9520949331889195 3.454053847488858 59.888487469065986 0.7134209792323667 
1.9638421538178588 3.471122525482494 60.10427241212795 0.7124512174363176 
1.9776635072669457 3.48347772673726 61.198925144956654 0.7089515696618844 
2.0276362825512355 3.583328695399138 61.508410480337005 0.6986942157346715 
2.0746918252411164 3.70106022531574 61.92897353478251 0.6809709125749337 
1.811028552523962 3.22803668491566 57.16321743420845 0.6809709125749337 
epoch: 45, train time every whole data:452.55s
epoch: 45, total time:17217.97s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.24s
test time on whole data:60.34s
0.900846986819857 1.7426150022854647 33.07769526423109 0.928456783059767 
1.6526931835609 2.704526053771298 59.88110912870061 0.8166495014466472 
1.6885162834364567 2.829694704664596 60.31274211629159 0.7984540868776143 
1.71315999358786 2.8928729091699905 60.54228357317185 0.7879561813629652 
1.7494848431493377 2.954197737052924 61.218056331035456 0.7794185843436668 
1.7842573710020986 2.9940183497950663 63.04472414572667 0.7713672015201168 
1.8241269422466202 3.075926883130142 63.82287887067063 0.7595488812957503 
1.8583091695707823 3.1485414980839943 64.10671206157063 0.750853890590552 
1.8816154299993955 3.194689280115993 64.46123435592528 0.745564652159064 
1.907176199849961 3.2233331808255623 65.19713652619562 0.742918795037162 
1.963280099453316 3.356609785071403 65.22667343940779 0.7284257017429434 
2.0180507240487 3.508291962928715 64.87631737788782 0.7040246057932552 
1.7451264355604403 2.9994997090273072 60.480971940163265 0.7040246057932552 
epoch: 46, train time every whole data:454.17s
epoch: 46, total time:17742.44s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.94s
test time on whole data:59.80s
0.8768994883211063 1.724365390709441 31.80638262555394 0.9287487982837372 
1.6335261632016018 2.684570113347533 58.36546792258744 0.8184503423303262 
1.6762381998093001 2.8335567622788225 58.588253550220706 0.7982845160581991 
1.705867164810676 2.9202024646839613 58.234757202769515 0.7878205132612546 
1.7460588669631454 3.0033107638701457 57.993126593238166 0.7822339824298432 
1.7803708493792052 3.048550271759706 58.40505286800046 0.778257338975086 
1.83146520989131 3.166043042367261 58.54979953132064 0.7631955299242219 
1.8921295634862922 3.297856015849404 58.993322406684314 0.7476751674659883 
1.9491455572188965 3.414857037613152 59.72603040476915 0.7319816889358569 
1.9919432387086784 3.49326619950691 60.743162128654646 0.7168496915525375 
2.057225864880319 3.63971770536155 61.34133970974778 0.6968988170144209 
2.1005876485939536 3.7580717114349165 61.10637760752419 0.6767635851429857 
1.7701214846053737 3.124966686437655 56.98801823111359 0.6767635851429857 
epoch: 47, train time every whole data:451.39s
epoch: 47, total time:18266.56s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.91s
test time on whole data:59.67s
0.8526604019972895 1.7119214090851507 30.37961959267072 0.9289188231129581 
1.626563078905855 2.6405306095673255 58.775568929618046 0.8198656797026177 
1.6619088041395658 2.757581644752793 59.23802545280407 0.8024142449857667 
1.6845864854451447 2.822253279268359 58.7958361976149 0.7934576680182844 
1.7059275541932633 2.881252073713001 57.97237390594123 0.789008852489347 
1.7307439307181076 2.9215435322498333 58.05393177039427 0.7851017648456093 
1.7740391940825753 3.0273012955024594 57.590219867246105 0.7726981232215234 
1.8139002900686825 3.1262329267903333 57.32739373443767 0.7627766186743014 
1.8484544729664034 3.202631852258665 57.49459429590843 0.7532854384549552 
1.8805043092582907 3.2638848741851523 58.51855717196655 0.7409837476779119 
1.9409202538775723 3.396466379452431 58.85915442543328 0.7237722777097605 
1.9830601713730998 3.520146399141477 58.906383919997296 0.7013836890661888 
1.7086057455854875 2.973277824659534 55.99284632738375 0.7013836890661888 
epoch: 48, train time every whole data:453.43s
epoch: 48, total time:18794.00s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.13s
test time on whole data:60.04s
0.8558924757972183 1.709969251190133 31.62380508467874 0.9290126037890303 
1.625617118337502 2.6468636221381727 60.17444670032002 0.819530974541979 
1.6653722895885863 2.7720180561950984 61.26299683382245 0.799791024376907 
1.6858069482528206 2.832016416739291 61.25633260492034 0.79037180141285 
1.7045837288913983 2.880654854854605 60.65649870866455 0.7852831031397927 
1.7279838510211558 2.91583789769292 60.95985757445156 0.7812777495038794 
1.7639673795535096 3.0142692998027814 60.42455845837349 0.7692732278559944 
1.7979708818092588 3.0960046562732026 60.14082501504387 0.7617444252037889 
1.8297777365526806 3.1577828389578295 60.40751610428209 0.7558076782064361 
1.8668247046965574 3.211163395998593 61.467609746861484 0.7483682725976677 
1.937381626619913 3.3739186754629467 61.682412095531966 0.7293079613354093 
2.015007069867814 3.5671117105113845 62.03921403441976 0.7009955297015412 
1.7063488175823678 2.965023977450943 58.50824755201964 0.7009955297015412 
epoch: 49, train time every whole data:453.48s
epoch: 49, total time:19318.75s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.85s
test time on whole data:59.71s
0.8715619069146259 1.7470479043496956 31.432602975059883 0.9286876273267572 
1.6274457771860595 2.689784189377013 57.76707607784116 0.8225682420663704 
1.6660778785406478 2.8230760281528853 58.22485964801506 0.8048744572038329 
1.6930397481348898 2.88910733170943 58.06255162766208 0.7958442129414368 
1.7268468317014298 2.9577514314477007 58.08611133626046 0.7888206501727447 
1.7474286579918117 2.9836610730137965 58.8380963788562 0.7837011740717162 
1.7753007068823845 3.0500420527632035 59.043005232341905 0.7743148524530473 
1.8012686673775316 3.106623863782338 59.40115323779649 0.7674337373349627 
1.8315417310748072 3.1608846669013397 59.91478019385753 0.7611138604606571 
1.8606269172531154 3.2015568899685602 60.65758901532594 0.7568362754907999 
1.9404218191286282 3.376374317953589 60.926448449911305 0.7399799908848963 
2.019871650148538 3.5709412564336915 61.24138579605884 0.7138364288202267 
1.7134526910278725 2.99445172322737 56.9665812037823 0.7138364288202267 
epoch: 50, train time every whole data:451.48s
epoch: 50, total time:19841.65s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.01s
test time on whole data:59.83s
0.9542590894801098 1.780615316922155 35.0825015157109 0.9287661723287691 
1.711874156315057 2.8180960801632 58.623255845899024 0.8200053633752095 
1.8068837687550556 3.0715097529278554 59.26792248549578 0.7983938204263719 
1.8951261925503966 3.2544603918347645 60.083332974962 0.7820758701445975 
1.9870664625033028 3.4156780392893604 61.52888010264608 0.7682325074894445 
2.042624400658444 3.5060105592645017 62.838184097614146 0.7512241552629353 
2.087357963658248 3.5982915752088394 63.539680289315385 0.7307906052108915 
2.1084895235545758 3.643385333911462 63.967471117903216 0.7220842661848663 
2.1036859367891614 3.624557966917296 64.1283495062001 0.7257105148823427 
2.0954621916584495 3.581489311971294 64.9962423023773 0.7276022410040924 
2.137419118445367 3.6581013782001417 66.22482623621556 0.714954824115435 
2.1810029498404337 3.7638594489390913 66.95596560393501 0.6886131997029583 
1.92593764618405 3.352010544452452 60.60340274822315 0.6886131997029583 
epoch: 51, train time every whole data:449.26s
epoch: 51, total time:20362.02s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.79s
test time on whole data:59.45s
0.8503257081062489 1.7268259430869606 30.332153835468496 0.9283935255084832 
1.6201647625528275 2.6570411180827556 57.63360836716234 0.8234730552900501 
1.6652445632739081 2.800281355677768 57.82577437015556 0.8069437876834136 
1.708626173474338 2.9042040534416893 57.73026378887942 0.7972636523556712 
1.7714301020459582 3.0377204011129413 57.94430215352536 0.787254252221073 
1.831373629502065 3.1564896576124006 58.80316096043878 0.7725868102981264 
1.9067877680176781 3.331841757234938 59.29354018648929 0.7469763009692348 
1.9723986461928913 3.484606994652336 59.59945698899806 0.7257606043252488 
2.009636370319518 3.555738148936646 59.58042831230493 0.7189092080480043 
2.0212706209238442 3.5516633699967732 60.37662960512825 0.719319954282298 
2.0624614038226152 3.6154145439502043 61.01949715958865 0.7127982782796228 
2.0827121385632172 3.6706103177831766 61.333472446980885 0.6949023956681925 
1.7918693238995924 3.169579213653259 56.789646203886676 0.6949023956681925 
epoch: 52, train time every whole data:448.96s
epoch: 52, total time:20882.02s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.83s
test time on whole data:59.56s
0.858303021513103 1.705557456534102 31.41914005387924 0.929817267984675 
1.6179897024782286 2.6356101410207673 58.7017258620226 0.8219056330868187 
1.659327987509203 2.763086343850373 59.1511548521783 0.8042644625715881 
1.6919114308858378 2.844924300849005 58.897771221349245 0.7944005132574159 
1.7267503876800516 2.927256427049505 58.44454501863359 0.7877260617608622 
1.7588781874308452 2.9858339799146942 58.83132138694005 0.7798710799676756 
1.8033821389116347 3.0957020027463855 58.848255367181636 0.764139020180432 
1.8316975079813884 3.165819462689812 58.591347226227484 0.7570737617273952 
1.8486793355808726 3.201283027218196 58.413339457508464 0.7539117249398463 
1.859767920836895 3.2045477823834685 59.334989242727644 0.7499002957573289 
1.8995772614707018 3.289156236853552 59.78998044934922 0.7381707600877395 
1.9309605173091626 3.3743398978992145 60.05039145074378 0.718994083072485 
1.7072687832989937 2.963605405393275 56.70639169579618 0.718994083072485 
epoch: 53, train time every whole data:451.40s
epoch: 53, total time:21407.26s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.85s
test time on whole data:59.59s
0.860439884152086 1.724062004551907 31.134775534638017 0.9300807797796601 
1.6207458173332825 2.664757754292266 57.85023367750984 0.8241346527046404 
1.6707104580172647 2.815805947310372 58.470135061655704 0.8051241056714075 
1.7167599804035965 2.9252587352504253 58.48648384077575 0.7936896300839735 
1.7791354328628983 3.0572455902939506 58.67851061579731 0.782750069136743 
1.8355688193771278 3.174895512085816 59.43266449770942 0.7658147708290138 
1.9006610938898687 3.3306531257389036 59.752010430730564 0.7408754392292997 
1.953761643563353 3.456024125747358 59.987290680788604 0.7224210009049633 
1.979329023362359 3.510184336176982 59.980646646982315 0.7158752853813843 
1.9792338079099676 3.484728985930882 60.57655717277761 0.7170013929780843 
2.0042945372374463 3.5290521453225936 61.10020578604364 0.7091499558335237 
2.0174161953856014 3.5694787430414805 61.73355201958206 0.6934364019925238 
1.7765047244579044 3.1448821959945823 57.265537056853574 0.6934364019925238 
epoch: 54, train time every whole data:453.23s
epoch: 54, total time:21932.29s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.13s
test time on whole data:60.33s
0.8546593406708645 1.7054267405106032 31.33937719308752 0.9299858175861723 
1.6084155333828891 2.6236624143417795 58.64532891704381 0.8246493104636852 
1.6489435986302616 2.757042022451631 58.676892198521 0.8073371917393007 
1.6775440898417895 2.837212826738592 57.86084247088541 0.7986226642930186 
1.7119503097080937 2.9141976924870705 57.28970520393786 0.7928683379439797 
1.733865707276123 2.9535363628579234 57.63575928017974 0.7878865185201424 
1.7657727697901429 3.0328285534173047 57.774167707797666 0.7779006289280322 
1.7914654370626169 3.0912903918250567 58.33535328275523 0.7703670220051525 
1.8111084659878343 3.1238329972518497 59.16016548928978 0.7635540578925194 
1.8285095274122876 3.124094262044367 60.692214089618766 0.7592730238553369 
1.8716503611451814 3.2142177783297052 61.253212301659296 0.7465576660397779 
1.9129613773505247 3.3210441843186627 61.31535320290629 0.7254646578087509 
1.684737209854884 2.919646392425977 56.66512074957771 0.7254646578087509 
epoch: 55, train time every whole data:453.08s
epoch: 55, total time:22458.15s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.97s
test time on whole data:59.87s
0.9040655427693965 1.8146555774773632 31.905866937801434 0.9275500144171788 
1.6865718369784632 2.8366091902922674 56.87079291823829 0.8235952736285221 
1.7632275026232183 3.056328276339244 57.416013957609515 0.8011744039202681 
1.8186252033620007 3.177409177368446 57.805051826610175 0.7879811156953344 
1.8726493941820448 3.273102907675614 58.587376004662254 0.7796810959533168 
1.8982753601011244 3.3025414148548804 59.55736104997966 0.7713306576799994 
1.9311431872395 3.3658978866680105 60.16628296777202 0.7574742573078187 
1.961803619908701 3.4281421634181397 60.79925577781198 0.7465106182428992 
1.9774620444476605 3.4531062842698215 61.30680725274777 0.74079823943141 
1.9824410042733487 3.430426649397123 62.432475518755716 0.7372162432865305 
2.0191819160402353 3.494129685240947 63.29878840316076 0.7257347326308744 
2.0390052487876797 3.560835406102247 63.19727557342042 0.7049801316422258 
1.8212043217261145 3.2153028492356537 57.77894343649997 0.7049801316422258 
epoch: 56, train time every whole data:451.41s
epoch: 56, total time:22981.67s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.00s
test time on whole data:59.95s
0.8453834109970679 1.724829370475199 30.382427919737708 0.9289469824392784 
1.6203033018619533 2.673504949560275 57.041914243744216 0.8267285894348964 
1.6791232594892027 2.8513318420100773 57.24363252906567 0.8084584784049227 
1.7306232003071125 2.9652735514082633 57.40999335828019 0.7975298799676971 
1.7860057690187048 3.0735228776594914 57.95895171175039 0.7880735982423333 
1.8229933077041947 3.139229931063943 59.07436559900238 0.7745812312490401 
1.8625863324332805 3.2270444861646608 59.74263280611007 0.756071775704707 
1.8823020970103819 3.2735151698059726 60.12875793435217 0.7457064577197947 
1.8807596459293827 3.2693615203250066 60.25067128048024 0.7430732128895818 
1.8761792216128004 3.2330374611293986 61.539543235576744 0.7432197035510024 
1.9006742933040397 3.270831856901957 62.08335900019644 0.7381109767969748 
1.9343721450153917 3.352547428592484 62.58824638996279 0.7206323184129622 
1.7351088320569594 3.0352896892975645 57.12069656304236 0.7206323184129622 
epoch: 57, train time every whole data:452.73s
epoch: 57, total time:23506.40s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.34s
test time on whole data:60.30s
0.8375615327838986 1.7006951910718073 30.137361677506263 0.9307476150125427 
1.598179489197919 2.617757024826843 57.59184747848809 0.828115195623149 
1.634380306582277 2.7434476053396 57.993552545432415 0.8119135893620799 
1.663990770875432 2.821032194778927 57.69819262642236 0.8034391227092159 
1.7058960410884272 2.910918346227173 57.58023735883889 0.7963902157016776 
1.7427748779856733 2.975282623768799 58.19845683576887 0.7892691473701601 
1.7945212192789075 3.0892662036215306 58.37564814701168 0.7763522364369543 
1.8445507332928301 3.203825423047233 58.66462152725573 0.7640190369501847 
1.8853566039471576 3.2931028416733312 59.035242032259575 0.7528849702654965 
1.9108446818971563 3.329032543114809 59.95369123616121 0.7451486987710049 
1.9590536171921662 3.4372416348667714 60.31680887545727 0.7308050325657451 
1.9886950785931023 3.5196530389913856 60.491256171789885 0.7131010302955046 
1.7138170793929122 3.006841231250685 56.336680731774415 0.7131010302955046 
epoch: 58, train time every whole data:453.42s
epoch: 58, total time:24031.28s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.04s
test time on whole data:59.79s
0.8437797516694381 1.7055665972199126 30.10059052656627 0.9294933875942082 
1.6000553220511369 2.601349396920747 57.10617739475274 0.8286375273522277 
1.633058308627013 2.7084637958972277 57.42595797097455 0.8149821658589541 
1.658709067718791 2.7819153949807114 57.179423141851736 0.8075684115124216 
1.6922530046445097 2.8629710101270702 57.22269522997472 0.8006553231930478 
1.7174855283982164 2.901647244659974 58.28029522061249 0.7937381718554971 
1.7533490076478393 2.969803346373339 59.06704977877639 0.7814523465668161 
1.7857059580509862 3.038082278931971 59.81306311159605 0.7695050058875067 
1.817367931048164 3.1037595857099016 60.452602677803135 0.757574781643093 
1.8419634565405902 3.1344320317423855 61.7581735266246 0.7500927689052395 
1.8839372903358724 3.2203169554053988 61.725764018261806 0.7386600157727682 
1.9191390846515342 3.3173852912655115 61.86761881470261 0.7207610796646612 
1.6789003092820076 2.890359194508613 56.83360269964196 0.7207610796646612 
epoch: 59, train time every whole data:449.45s
epoch: 59, total time:24551.92s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.79s
test time on whole data:59.57s
0.8534254045152948 1.7302182089459313 30.19733271886581 0.9303836235250728 
1.6150191861424772 2.6822741622081363 56.44771763247084 0.8261998152920544 
1.6688763976740164 2.8556187936466793 56.84625308275677 0.8046367900206643 
1.7062554319882322 2.942354532246508 56.80686308742451 0.795131753334394 
1.7422525423201953 3.0072658078901617 56.93085180702672 0.7909922494651764 
1.761875254624479 3.0247791201673295 57.530861074975824 0.7864575492569562 
1.7922540004688892 3.089044524747922 57.71436875528545 0.7740809304392169 
1.8110876970585612 3.140472596054656 57.80229181361146 0.765324894050555 
1.8221328955132159 3.169452836391412 57.91824630852849 0.7594606481216393 
1.8287758236410363 3.160053536657311 58.919550395987464 0.7563349516324995 
1.8610563772020063 3.2292110651740664 59.482271086063975 0.7456828001537453 
1.893557722204409 3.314029577178135 59.696289422531294 0.7297033529194744 
1.6963807277794012 2.9726520842217083 55.52467411549302 0.7297033529194744 
epoch: 60, train time every whole data:451.36s
epoch: 60, total time:25075.13s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 36.17s
test time on whole data:60.20s
0.8393264413978018 1.6886170108087784 30.61325884908504 0.9308007035655633 
1.5985516148174093 2.590882814658746 58.68759332757211 0.8281932609608548 
1.6316146675782013 2.7001588958429967 59.12206348705522 0.8131118223323996 
1.6561566677544977 2.7640930214118016 58.61540345783579 0.8059449123923121 
1.6909229054457198 2.8469129268162154 58.10945679759728 0.7992457509832663 
1.7245373492578844 2.9152588620990976 58.56260541739202 0.7913461948175496 
1.7740373136897882 3.0276829011010813 58.66972404517008 0.7783710725960282 
1.8184349244954507 3.132900237204622 58.872317554553646 0.7667625303608389 
1.8593583391137598 3.2210171396897573 59.14997492469865 0.7554999335146457 
1.8853646582637338 3.25294220306966 60.10782838247907 0.74932196697343 
1.9332362511649372 3.3558239095819125 60.63385029772973 0.7338066043633348 
1.9572725286786223 3.4272196330634714 60.707024809314106 0.7178097959226184 
1.6974011384714838 2.944553127813365 56.82117739999414 0.7178097959226184 
epoch: 61, train time every whole data:453.13s
epoch: 61, total time:25598.79s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.94s
test time on whole data:59.68s
0.8732965166028589 1.680006446103692 32.199969444139164 0.9302870064892277 
1.6044998077458392 2.5706904454877195 59.41582139340827 0.8288099066494744 
1.6373667738946776 2.679115802527863 59.54999078471281 0.8138115361843522 
1.663034631254861 2.7460084553280493 58.81763021021863 0.8060532484606129 
1.6925609630994676 2.817532168055271 58.2063391799564 0.8001146229666628 
1.719859831130487 2.8656331216039193 58.74095332563829 0.7938492658166194 
1.7565666410582406 2.9484229963316793 58.85175622369626 0.7828122688812182 
1.7865914638316525 3.020019232186754 59.047132957725964 0.7742181349471908 
1.8194711500829353 3.0925152386465213 59.401842691507255 0.7635709025299916 
1.8462584560972062 3.130534405188014 60.69221078665065 0.754914986697731 
1.8913440004173843 3.229235544688809 61.01308284074468 0.7404577087827622 
1.9298338482714303 3.3325468972055714 61.51425230585976 0.7194895430799354 
1.6850570069572532 2.872439108257669 57.28782285993168 0.7194895430799354 
epoch: 62, train time every whole data:451.48s
epoch: 62, total time:26123.05s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.87s
test time on whole data:59.67s
0.8406257977692322 1.6902552064086669 30.852771822835656 0.9311046848244878 
1.6019049613686012 2.6265207489734457 57.691088217396484 0.8287554406451493 
1.6531284060760267 2.786042104962241 57.81255992254655 0.8117839939593163 
1.7058440502985779 2.911497344981296 57.474131886639476 0.8011165306387249 
1.774424465526871 3.051945536294478 57.743487828854555 0.7910246095176541 
1.8283756960052997 3.155530823956324 58.66019011235174 0.7772210498028632 
1.8818925999804799 3.273580394487155 59.21393394085411 0.7596264155175096 
1.9233133183167803 3.360458329138991 59.85973904367745 0.7480859377014004 
1.9506575049463482 3.4094140620968587 60.45566314295956 0.7415498354271357 
1.9617539013184253 3.407569795000802 61.47627083211309 0.7390211255262429 
1.9989284438041173 3.4857887635120104 62.18044083734425 0.727915923370455 
2.0344949706245568 3.5724971768098235 62.63551884431913 0.7103694317282199 
1.7629453430029431 3.1014011882171646 57.17162452033639 0.7103694317282199 
epoch: 63, train time every whole data:454.16s
epoch: 63, total time:26648.74s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.21s
test time on whole data:60.25s
0.8997005849151561 1.74822423919577 32.61215047591658 0.930824395031368 
1.664471789130054 2.77489652833662 56.53755516543286 0.8283737200993426 
1.7436848985558109 3.011952665039164 56.93072107823631 0.805492302000337 
1.8056940681244291 3.1387514290820313 57.45785334783109 0.7932268861647369 
1.8608645177444532 3.2323264161369747 58.35897600997036 0.7843487494082179 
1.8830492079120484 3.2520561682252778 59.32728314113109 0.7769330789475467 
1.9010635950416326 3.281175725867472 59.905631752533864 0.7674202297809702 
1.9072262585896644 3.293423965455376 60.50429748417765 0.7599210221957912 
1.9055011382196985 3.285414821959063 61.19148653219079 0.754199881154123 
1.895893397755921 3.234424588709358 62.625146899266596 0.753007460859906 
1.9178396721945277 3.275468326997083 63.74879292283586 0.7429203550373862 
1.9388055596004816 3.3352492997355574 64.17393348100411 0.7269508690256427 
1.7769828906486564 3.101404932854861 57.78148906398559 0.7269508690256427 
epoch: 64, train time every whole data:452.84s
epoch: 64, total time:27172.61s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.88s
test time on whole data:59.64s
0.8835808038574954 1.6850771630884638 32.70450057647922 0.9308714281988696 
1.603661501704671 2.6023170972047907 58.817316473720716 0.8270447749625739 
1.6473390747080778 2.748379266988856 58.75209251290817 0.8094712261905245 
1.6922392592527682 2.8653093292539245 58.06455490034064 0.7987896543586874 
1.7621608433631204 3.0207233736366517 57.96598834541232 0.7872957150259836 
1.8298606955744325 3.1628014947546212 58.76499866078061 0.7705699206471572 
1.8997118640352217 3.3249859744511174 59.23812140920299 0.7476846038219501 
1.9567064117467297 3.448334110463441 59.77843054678211 0.7309128705988319 
1.9853581120638797 3.510492044816583 60.066039845063266 0.7200609412426899 
1.9806710484466914 3.478383952448763 60.66026813502885 0.7184154648695725 
1.9863750906056237 3.48675606166161 61.212507624120164 0.712173217990908 
1.9834304170850665 3.484546838586142 61.59118280527753 0.7034493646631209 
1.767591260203648 3.1114304340559853 57.3015938328979 0.7034493646631209 
epoch: 65, train time every whole data:451.17s
epoch: 65, total time:27695.34s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.78s
test time on whole data:59.63s
0.8543750753564139 1.678239454676271 31.243014504861318 0.9310559023607738 
1.60157585500979 2.5746867289814532 57.7724449951191 0.8302355449396889 
1.6335093739162243 2.68000879044429 57.9295522939892 0.8167032231224008 
1.6591105518399605 2.745318244224014 57.18068671867634 0.8109651843311897 
1.7016445322083753 2.8518656728408045 56.67030721204556 0.8027188102118494 
1.7471668117242378 2.9550337509050877 57.0849154244115 0.7913623375585636 
1.8149034232972634 3.11995986337927 57.477663954079425 0.7712946213146664 
1.8848276517786795 3.288009275941205 57.99152075713512 0.7521308295431984 
1.9522735546527519 3.437573869945596 58.456519396908035 0.7324546690147714 
1.995158295262605 3.5182055587732486 59.4564906663957 0.7197383096192551 
2.0518234551950756 3.6322264647175184 59.99402937398051 0.703007723998329 
2.0887292280349703 3.722009692823588 60.246187208709046 0.6892767077672772 
1.7487581506896956 3.0661640712354923 55.95886030924445 0.6892767077672772 
epoch: 66, train time every whole data:454.43s
epoch: 66, total time:28223.80s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.13s
test time on whole data:60.08s
0.8493870697348778 1.6965653628358013 29.98490151608176 0.9310832874608809 
1.6121921536945516 2.635428992837706 55.143786323051955 0.830555061633994 
1.660671900039805 2.791479141656047 54.941384609247066 0.8161210846653166 
1.7091521922441288 2.901446836829678 54.50289427832823 0.8088080107982134 
1.7641437908815487 3.019863495020528 54.52423331360304 0.8016618297001736 
1.795133798383886 3.07659044393829 54.88688049282504 0.7945257948548916 
1.8251872117134431 3.1341862898674604 54.99938356397319 0.7862518693481485 
1.839274880159469 3.1672615928530403 55.438554134007575 0.7786248039891349 
1.8426750285128752 3.1661438446251022 55.95254566822819 0.772232233179383 
1.8291690796780444 3.118560627095195 56.93347441281207 0.7717590550160787 
1.8512992546286966 3.164252547568743 57.73752558570779 0.7630781952387886 
1.8874433595786493 3.2576363518755693 58.15649873048604 0.7472079598453862 
1.7054774766041645 2.955909329249644 53.60041847875267 0.7472079598453862 
epoch: 67, train time every whole data:450.08s
epoch: 67, total time:28745.14s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.75s
test time on whole data:59.44s
0.831044800418296 1.6984773787375804 30.27413101036402 0.9310143747671847 
1.5947587748912295 2.6188552875131226 57.28551754238811 0.830337496725921 
1.6372806700797131 2.75454239867117 57.571840366631335 0.8156507487474928 
1.6847848357528092 2.8626946066112846 57.52334011686509 0.8067443813696306 
1.74739917039206 2.9955678204330662 57.97502512239666 0.7953180383274425 
1.7961270731332757 3.0977333870658352 59.00346053096306 0.7782350731687009 
1.8428363928168656 3.2077982726197485 59.43814403140758 0.7576193406615973 
1.8752356055390444 3.2811591942267295 59.61078590802603 0.7456983824392724 
1.8903904789398824 3.3080356829755466 59.50545101945182 0.7433238082698909 
1.8916919273970028 3.283838768071663 60.053901840166766 0.7462466206571182 
1.9228358394693406 3.335677013381301 60.23412971755312 0.742457610847655 
1.9623726661804886 3.425169982069051 60.450145277151904 0.7281020543946701 
1.723063186250834 3.0240195994990344 56.577442491394756 0.7281020543946701 
epoch: 68, train time every whole data:450.88s
epoch: 68, total time:29266.94s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.82s
test time on whole data:59.60s
0.8973134378146912 1.6919500220887418 33.93824971749308 0.9310378866788779 
1.608489731401826 2.591581081412119 59.08028239021559 0.8289738034079623 
1.6458936914048556 2.69590319785143 59.60641039773213 0.8144488167107848 
1.6740936762266572 2.7595966261230966 59.314457741006876 0.8065321585773099 
1.7110608955900228 2.839146001277405 59.49132245828695 0.7973689708539202 
1.7379911776047554 2.88678006503759 60.52685945270192 0.7893702477621942 
1.770440329458387 2.959894005967199 60.922907613645116 0.7790452388444962 
1.794388469708906 3.0182361096619776 60.91237422897221 0.7728000792734445 
1.8092655405201727 3.0577192877346673 60.2042042804029 0.7713224348622207 
1.8176984780740348 3.063073746745052 60.0495501259108 0.7745360848696199 
1.8636064982944656 3.1808638096880313 60.27105147353456 0.7609087293406286 
1.9097142506282598 3.2983576586117374 60.058677509839164 0.7434417143302706 
1.6866630147272528 2.8643840482591805 57.86493014864483 0.7434417143302706 
epoch: 69, train time every whole data:455.02s
epoch: 69, total time:29796.08s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.02s
test time on whole data:60.10s
0.8765116750842759 1.7729588991240324 30.882719571698665 0.9295497451722332 
1.6312144313434227 2.7197759294724446 55.46908268173297 0.8304541786497222 
1.6880120661437865 2.8800138583391366 55.77856743985994 0.8149898798573794 
1.7423735519620103 2.9895465425428607 56.18842011134048 0.8057254405502673 
1.8091511640312772 3.111774318518732 57.11696237535485 0.7944195525972321 
1.8539910314515942 3.195227475751607 58.352149766718895 0.7791893319153022 
1.8999431801321252 3.2978683313499753 59.174079191819175 0.7596209206502286 
1.9327043957847747 3.3639408123074883 59.863075388240325 0.7478426234690171 
1.9423240409793243 3.366759206334469 60.117330876777984 0.745071658668696 
1.930400998953109 3.3151492534909757 61.10861580608871 0.7468577020730225 
1.949223174112982 3.343224961474581 62.021479923592295 0.7398127668667676 
1.9763998885486453 3.403059703037581 62.75380397301784 0.7231656658964798 
1.7693541332106106 3.0948927644510924 56.569197389428226 0.7231656658964798 
