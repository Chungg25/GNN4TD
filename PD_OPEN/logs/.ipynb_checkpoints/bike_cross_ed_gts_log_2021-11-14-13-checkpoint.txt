total training epoch, fine tune epoch: 50 , 50
batch_size: 4
folder_dir: MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_ed_gts
load file: data/bike.npz
ori length: 3001 , percent: 1.0 , scale: 3001
train: torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12])
val: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
test: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
TemporalPositionalEncoding max_len: 12
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder2): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (src_embed2): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed2): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
  (prediction_generator2): Linear(in_features=64, out_features=1, bias=True)
)
create params directory experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_ed_gts
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.0.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.0.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn.gate_convs.bias 	 torch.Size([64])
encoder.layers.0.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.0.tcn2.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn2.gate_convs.bias 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.1.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.1.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn.gate_convs.bias 	 torch.Size([64])
encoder.layers.1.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.1.tcn2.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn2.gate_convs.bias 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.2.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.2.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn.gate_convs.bias 	 torch.Size([64])
encoder.layers.2.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.2.tcn2.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn2.gate_convs.bias 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.3.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.3.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn.gate_convs.bias 	 torch.Size([64])
encoder.layers.3.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.3.tcn2.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn2.gate_convs.bias 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.3.sublayer2.norm2.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
encoder.norm2.weight 	 torch.Size([64])
encoder.norm2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.0.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.0.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.tcn.gate_convs.bias 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.1.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.1.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.tcn.gate_convs.bias 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.2.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.2.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.tcn.gate_convs.bias 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.3.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.3.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.tcn.gate_convs.bias 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.3.sublayer2.norm2.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
decoder.norm2.weight 	 torch.Size([64])
decoder.norm2.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.0.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.0.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.0.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.0.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.0.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.0.tcn.gate_convs.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.1.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.1.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.1.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.1.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.1.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.1.tcn.gate_convs.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.2.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.2.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.2.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.2.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.2.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.2.tcn.gate_convs.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.3.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.3.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.3.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.3.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.3.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.3.tcn.gate_convs.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.norm.weight 	 torch.Size([64])
decoder2.norm.bias 	 torch.Size([64])
decoder2.norm2.weight 	 torch.Size([64])
decoder2.norm2.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed.2.embedding.weight 	 torch.Size([250, 64])
src_embed2.0.weight 	 torch.Size([64, 1])
src_embed2.0.bias 	 torch.Size([64])
src_embed2.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed2.2.embedding.weight 	 torch.Size([250, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([250, 64])
trg_embed2.0.weight 	 torch.Size([64, 1])
trg_embed2.0.bias 	 torch.Size([64])
trg_embed2.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed2.2.embedding.weight 	 torch.Size([250, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
prediction_generator2.weight 	 torch.Size([1, 64])
prediction_generator2.bias 	 torch.Size([1])
Net's total params: 860802
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499]}]
predicting testing set batch 1 / 168, time: 0.72s
predicting testing set batch 101 / 168, time: 39.06s
test time on whole data:64.60s
102.37093798689925 107.34826185557296 4197.5051696366 0.003160775214415866 
110.40092748722141 117.87567235658707 4523.166384807864 0.0014957505579019834 
112.7075160947615 121.65646961883142 4616.134866488542 0.0013739317432638549 
112.22713899524058 122.1619942623926 4596.23871383887 0.0015762941560283714 
111.5901394897773 121.9176408243523 4569.913854066405 0.001692262516105956 
111.33772485938606 121.54940126402536 4559.511641169193 0.0016664208259256409 
112.10057070983459 121.71101223806684 4590.808960739067 0.0015454155509377293 
113.9104246142088 122.85973967251279 4664.228836545422 0.0013457945753884584 
116.08512607876575 124.9980384697677 4752.821611069756 0.0009851547724032221 
117.87142124760408 127.58482421708275 4825.215321204348 0.0006780103071904361 
118.71745380670772 129.68990425531356 4858.729364468548 0.00046682013252358136 
119.21536089921628 127.32863647702312 4879.023596148316 0.00041359915507590053 
113.2112285224686 122.34612003386395 4636.116670859495 0.00041359915507590053 
epoch: 0, train time every whole data:235.77s
epoch: 0, total time:311.16s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.53s
test time on whole data:65.68s
3.138301403787874 5.094044304602178 79.75889394833733 0.25893941927224534 
3.2350615623275796 5.172121590394989 85.47389993387056 0.2443120285222696 
3.2504481388395208 5.19363818541442 84.66543501515402 0.1940635649405181 
3.291015245768641 5.238356299297756 86.22099177499267 0.1778242220318451 
3.309171934869584 5.272552807016818 86.58088696541853 0.17148667096163536 
3.3169351066339408 5.2879226427564285 86.84166557817709 0.1692062844599187 
3.3258438182012844 5.2825116279516315 87.58104498075376 0.1648818503388501 
3.3375823073912234 5.274528806511471 88.5550481457948 0.16164198015680248 
3.361333333956876 5.288357409055658 89.77762802227048 0.15889987492301416 
3.403309527979809 5.3356906592661355 91.51896198060858 0.15824912710460318 
3.395580321933808 5.3638728523971375 91.09790641191654 0.17190134358719783 
3.478635435869119 5.423944546244699 93.84674362759387 0.1487167256993298 
3.3202681781299384 5.26963416634955 87.66010994295121 0.1487167256993298 
epoch: 1, train time every whole data:237.05s
epoch: 1, total time:624.63s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.57s
test time on whole data:65.78s
2.5846715297593423 4.423585812857051 59.90072031900229 0.4468154136181591 
2.61297342552493 4.388037759152588 64.20172963454766 0.40993280762163364 
2.694898198012972 4.4201361353648005 66.6520229184245 0.3654540584330275 
2.778951428911604 4.499966295647476 70.11283734530164 0.3224318348211689 
2.8393888533304312 4.551399663934545 72.05753729970863 0.2878984399276012 
2.893285032153307 4.586048210819201 74.13809963689386 0.26023751002507406 
2.989578608118175 4.640102162316343 79.1944088130465 0.23291932620337472 
3.1241224331618067 4.738420780473256 86.22200752584419 0.21019517395483198 
3.231588549858048 4.831960354170294 91.47806442492012 0.19396238687557418 
3.3292069413465937 4.919732642728406 96.07642274979126 0.18178240259212683 
3.3671627624943143 4.941241398188397 97.69433502103992 0.17522455066800205 
3.2644120434174937 4.820045256329486 91.7818181493922 0.17409815157493513 
2.9758533171740846 4.65063487213835 79.12655680679475 0.17409815157493513 
epoch: 2, train time every whole data:237.00s
epoch: 2, total time:938.19s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.59s
test time on whole data:65.85s
2.593701225026289 3.9839763124065457 73.3942586669042 0.4989631312517486 
2.8140973860336733 4.1048019808135905 87.12575465845792 0.45619861961353886 
3.0573743895451937 4.286463121037842 99.65375010400076 0.4111315177503627 
3.3009131865749755 4.530652926834122 111.176654160144 0.3544039718293525 
3.478339269612694 4.720509589904594 119.21186424282189 0.30807158176856364 
3.6228535724888955 4.884694685964572 125.69523063322391 0.2702166701794713 
3.850999034668363 5.140208688997292 136.14856312530802 0.2339162840738388 
4.118103288013931 5.449396563957825 147.97745920115145 0.20322097173645698 
4.3183223107993784 5.689668032351278 156.43088132507043 0.18137805649210073 
4.496981800325481 5.900631545688455 163.78973392078245 0.16630506755340924 
4.5125594200045755 5.919243621681169 163.90573311734084 0.16060657975826106 
4.161559559166786 5.47663781528878 147.08004661037927 0.16509601991121894 
3.6938170368550196 5.050610124868942 127.63422560909818 0.16509601991121894 
epoch: 3, train time every whole data:237.64s
epoch: 3, total time:1252.13s
predicting testing set batch 1 / 168, time: 0.43s
predicting testing set batch 101 / 168, time: 40.28s
test time on whole data:66.73s
2.7903588048819277 4.619394318613508 83.2628302546805 0.38941748574231805 
3.112886033619 4.834294408147741 100.84758251127238 0.33868413760379845 
3.3646105383591993 5.035724279258035 112.82787215753685 0.2949087536146532 
3.713721384775869 5.3544808178939185 129.28917106348385 0.24721610267680746 
4.014788066660365 5.633356380569223 142.89529579947248 0.20719130573167885 
4.221798873234274 5.845061721790155 151.88399321949495 0.1784892866910279 
4.483360219363567 6.131136479399584 163.4055448625398 0.15393927062201965 
4.774210730592587 6.454548091672064 175.9767858915247 0.13338631996687803 
4.971418437608917 6.680427236013965 184.20659029534414 0.11857881457927714 
5.117326225250871 6.85423906821491 189.81248788994284 0.10955870953716798 
5.163444269536329 6.918602379438783 191.05525039884634 0.1049669089111345 
4.705014038430527 6.374331320405365 169.82005872494264 0.10697656727787304 
4.202744801859453 5.944167493624571 149.60904973302726 0.10697656727787304 
epoch: 4, train time every whole data:240.32s
epoch: 4, total time:1570.71s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.68s
test time on whole data:65.70s
2.4551884987729466 3.9343790789698616 66.84645475448299 0.5163684958559162 
2.712945614562858 4.126723425766435 80.43769026139957 0.45031918756826056 
2.973509648234539 4.361493547264054 93.58699722685336 0.39786239140464996 
3.2560027826858597 4.661729307807452 106.86414165295453 0.3415450612383677 
3.5175571324085553 4.955969223549947 118.48105364628651 0.29036404322608433 
3.764689278374914 5.24205695528439 129.3118654810856 0.24689868604212756 
4.080186424099796 5.610337155945143 143.18064565103953 0.20891502449523663 
4.407671463919094 5.993102236072124 157.08033329169626 0.17822822227728555 
4.66335415544148 6.283165458253702 167.69016205194362 0.15664180143426923 
4.887889291945313 6.536380678394309 176.94713524198056 0.14237513220732376 
5.0166704438203675 6.679220578691092 182.12423665445263 0.13390310880795767 
4.630645311805376 6.162184280234479 164.75229268674013 0.13647932774367375 
3.8638591705059246 5.45805539047235 132.27749757369557 0.13647932774367375 
epoch: 5, train time every whole data:238.29s
epoch: 5, total time:1886.57s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.53s
test time on whole data:65.71s
2.666949697008445 4.151718344726082 75.98640698273135 0.4552921984993641 
3.1618743819533184 4.622025949986257 100.07568596114147 0.3607347055980537 
3.6368040574159295 5.15431615397385 121.72391373098117 0.29421040648065205 
4.126892099852097 5.7746121078531685 143.29836390012284 0.23369424997251628 
4.516471007118622 6.271032607036308 160.22045315187952 0.19280362738888107 
4.80109791379484 6.610258102924302 172.2361437660822 0.16759941608151382 
5.151184121049231 7.012839543547785 186.90006519789105 0.14599530737414065 
5.416434455517414 7.311619877129062 197.8701403096706 0.1308025716586201 
5.557658284698835 7.464716122691324 203.33947781441472 0.12146220744946715 
5.731450441333243 7.670984387157896 210.0397693619341 0.1173292381381194 
5.940708064919604 7.9073778381775455 218.72071957141247 0.11347672952610387 
5.527169543995389 7.341642116212741 201.0733454958407 0.1163401142823398 
4.686224505721414 6.552139577033631 165.95970813302344 0.1163401142823398 
epoch: 6, train time every whole data:237.27s
epoch: 6, total time:2201.76s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.53s
test time on whole data:65.75s
2.248371831934012 3.791796962778151 64.51009540269094 0.6414210742167028 
2.4925619446079113 4.105804898176028 80.04357111138819 0.5432853971984346 
2.6984364872032867 4.383805192156468 91.97809214508057 0.5029106737592267 
2.988787923827856 4.761261778655302 107.32090456152899 0.4600709656121039 
3.275393264332875 5.143602299277588 121.38542553993065 0.4178859387428026 
3.559609724834118 5.505638414815743 134.06025995368384 0.37508667306542465 
3.8305339879748366 5.8496470585481415 144.6217530704393 0.33366817418588424 
4.0174817049691365 6.065269777874536 149.92584392191634 0.29142434613577856 
4.136990251936728 6.1899984210125645 151.28409020064313 0.25764093528198434 
4.304150783896535 6.399485426845208 154.5649717172101 0.23621514750077416 
4.428104876689968 6.550937475929669 155.4643314763253 0.22171511076174139 
4.126321464138638 6.073741264086869 138.7658268153001 0.2106814768227315 
3.5088953538621586 5.477011775361965 124.49557720226139 0.2106814768227315 
epoch: 7, train time every whole data:237.70s
epoch: 7, total time:2516.28s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.99s
test time on whole data:66.07s
2.365283470697967 3.4815955101799965 82.19732843254698 0.6640363105674616 
3.700356937868964 4.905134052984134 146.50784613949338 0.5103543470974887 
4.772149450015632 6.203804262078911 192.60447962855847 0.4136871114793337 
5.777311551015026 7.413543908906729 235.2192536441072 0.3333866204602771 
6.496971933261181 8.337018542553876 263.79441367232977 0.27320329086688433 
6.9760651428834315 8.931920946810544 282.2435999121388 0.23704644452417015 
7.366877607189651 9.288511098421793 298.74222440459283 0.21884994715392028 
7.494478440767509 9.324861460316399 304.90799644050026 0.20132297859809156 
7.556168328379236 9.375759059394298 306.9885137118475 0.18589899349393382 
7.801406081700875 9.729080356191956 315.20003329679224 0.18274362009171985 
8.137280816286358 10.210705499578628 327.14792434330246 0.18280120034374098 
7.388856457906464 9.270321437305002 295.5789590046969 0.1905589539960095 
6.319433851497691 8.291066811692676 254.2652423690446 0.1905589539960095 
epoch: 8, train time every whole data:237.55s
epoch: 8, total time:2831.65s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.50s
test time on whole data:65.66s
2.2239655674282637 3.2683858479923287 81.75776522945561 0.7031614300546031 
3.3449777501393996 4.497365615538425 128.17887102980018 0.5079069742452935 
4.4178559175027265 5.671011756364709 175.4318180867408 0.4015001433200185 
5.462476267737736 6.894761871836347 220.42439467267005 0.30409349538050084 
6.2759776663652485 7.87973123075199 253.583722424215 0.2350574630560821 
6.769367145466485 8.476480696091828 272.8693627820434 0.19529981197012325 
7.060461986171614 8.775602899194523 285.5552513658843 0.18145869995441902 
7.073826497588218 8.76559400586738 286.2381660288042 0.1646950618032753 
7.038566914903443 8.726392403099537 283.8536739382098 0.14982976026085543 
7.297667029024412 9.02861271535925 294.5357035171547 0.1482350086731004 
7.736159254311008 9.581829918720391 312.24427463168956 0.14662542385242602 
7.187681725574569 8.88238037279837 289.10072544296656 0.1562829641533081 
5.99074864351776 7.782289779453694 240.31853709861832 0.1562829641533081 
epoch: 9, train time every whole data:237.37s
epoch: 9, total time:3145.57s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.93s
test time on whole data:66.45s
1.4812310524330963 2.707488714472924 49.56543014256924 0.808210450998095 
2.65787532096365 3.9023914966625766 102.50333842776192 0.6246646409579023 
3.616486966406749 4.9303425138314045 147.64315251522007 0.5426312961707905 
4.682621399058206 6.084844102772504 194.1423577462698 0.4498478852793403 
5.626282075420111 7.115286064845423 231.87421946696972 0.35840010933307226 
6.395166053990257 7.961259516725502 261.14038712849606 0.28249465086423203 
6.976041315979457 8.574695331893926 283.89227490539827 0.2322715327482264 
7.181209467872978 8.765819133280903 291.7831691492393 0.19682754046767487 
7.211970575253169 8.789981822688047 292.1585633830354 0.17555131042477948 
7.511465115439591 9.112750354552466 304.18920171293746 0.17261341986368942 
7.877793324988424 9.568621500228415 318.6275908271393 0.17354795183665148 
7.205392582362518 8.7653036663997 289.51763833266637 0.1842653114432655 
5.701961270847351 7.510890752915678 230.59142531394136 0.1842653114432655 
epoch: 10, train time every whole data:240.49s
epoch: 10, total time:3463.37s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.51s
test time on whole data:66.36s
1.7319638641694828 2.72459536306197 60.80095350605562 0.8234801616822075 
3.478216368996494 4.480504161204849 139.6392294473143 0.6238731336795122 
4.826564025810609 5.899015698494904 199.3455740422739 0.5247389384609975 
6.043950835427269 7.278653681294031 250.68549628360284 0.4161671162024422 
6.94372003757049 8.343003718277592 286.6072224731193 0.3251697151242811 
7.626579637082471 9.086057129940029 314.100069642079 0.2664607073590107 
8.1270938259388 9.55676752017296 335.3912474743592 0.23289028295490966 
8.193309382269957 9.559034077626384 338.5845499526476 0.21488223031518375 
8.10020724280764 9.433069516720412 334.572317647502 0.20901976972148348 
8.358400154686134 9.717379385719681 344.97409875861723 0.20948765013264656 
8.751715000056528 10.187728829365083 360.62331085497794 0.2055037050354341 
8.002931782976946 9.361036583913842 328.6693516212377 0.19875866378087031 
6.682054346482735 8.291429779483249 274.50461735504774 0.19875866378087031 
epoch: 11, train time every whole data:238.18s
epoch: 11, total time:3778.88s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.48s
test time on whole data:65.68s
1.6745741960098524 2.6954714695538575 63.91418017151806 0.8176775863055269 
2.619854421982808 4.011527877501148 106.03743983575026 0.6447513018598072 
3.4119238644665373 5.093701166737932 146.88518581826514 0.5731690691822144 
4.262005177333685 6.146017185418257 188.07276309078557 0.502283506058448 
4.995386604997196 6.964922428237338 219.31989134940554 0.43206170022007767 
5.703818006417137 7.652855389082912 244.18530300999365 0.36228967420835984 
6.464554494597195 8.344575232787168 269.34635281699116 0.2944360505152402 
6.9434819319493 8.77782577977304 283.14212960959094 0.23642379362666144 
7.146448603781828 8.93778746218622 287.6411739029289 0.2073590006500351 
7.512484991809974 9.291759299351426 301.3418059323847 0.20007330667633552 
7.896577587948136 9.74328169252515 315.8175889586208 0.1951881775784068 
7.2237544724142975 8.954278865447348 286.8136510426393 0.19346370834828977 
5.487905362808996 7.5397127829043376 226.0478446905358 0.19346370834828977 
epoch: 12, train time every whole data:237.83s
epoch: 12, total time:4093.92s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.41s
test time on whole data:65.55s
1.3112214630311798 2.5058610238831456 47.54116130421712 0.8387405362457885 
2.6956787997607496 4.152259017445446 113.04087147578157 0.6604837107223808 
3.5840455943662675 5.342401188151847 157.1995754623803 0.5892074316937866 
4.433907011650858 6.360857993173652 198.3515543986168 0.5215620851200439 
5.095038523178991 7.063813571807534 227.10972492256212 0.4564548062590478 
5.719694954349437 7.655775920000227 249.1684256060731 0.39022903883867127 
6.408279960706209 8.268345086305942 269.99579188252727 0.3241344612088772 
6.865469236564866 8.661831205292925 281.1449883381272 0.2628458400905077 
7.145006391071404 8.928496531210325 288.09086528865123 0.22528564688995717 
7.616981766722032 9.399970580300996 305.7922263753449 0.21318210124163364 
8.067858128727103 9.877924365110088 323.4812844534706 0.2104871628371949 
7.461198235923602 9.105403408989721 298.5257918121746 0.2066396691123891 
5.533698338837725 7.599247961228526 229.95830149703363 0.2066396691123891 
epoch: 13, train time every whole data:237.40s
epoch: 13, total time:4405.12s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 42.05s
test time on whole data:70.39s
1.24401247786766 2.4631948184077346 44.387363442313266 0.8436185282974973 
2.7138390377663253 4.1493569972672955 118.827454576684 0.6636094319999578 
3.8333715239550386 5.57166895925134 175.44645465194463 0.5856960278442068 
4.936911655199138 6.864369227016241 227.9954110704149 0.5106421068838423 
5.762486689337218 7.7034617176334645 262.2367941231828 0.43872697401654925 
6.501971700412177 8.34924015670432 284.90350444680587 0.3664126435114656 
7.28394389689084 9.020978757441146 307.8944202662852 0.29486007838064826 
7.795250202784936 9.44975008843998 322.87594109997525 0.23495212359729542 
8.009557166018656 9.60996588919663 328.48610741677487 0.2056150801004418 
8.399737943309137 9.981219772344277 342.70643179678666 0.1950265310437301 
8.720588516839559 10.317586777825099 354.95507037930616 0.19235387870549955 
7.820910828591635 9.29482381311639 316.7859058227514 0.1941694607565391 
6.085215136581026 8.093202185353997 257.2971857910179 0.1941694607565391 
epoch: 14, train time every whole data:237.33s
epoch: 14, total time:4724.82s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.37s
test time on whole data:65.40s
1.8516110744603156 2.8184119869728965 71.9963571781711 0.8284317389179356 
4.253025505634469 5.13384999941094 177.89039303699005 0.6273536818480835 
5.869311100202302 6.888449897261262 247.30083482706507 0.5105248509005796 
7.138986991114205 8.329401912445201 299.4214424034811 0.3910615945206041 
7.837136830397305 9.199194268238895 325.7078137630154 0.2924716032837431 
8.235650319603227 9.64785851383826 341.3422881153124 0.24297963350174853 
8.55006139318778 9.908110168335023 355.17354489940146 0.22696871351340026 
8.51268739497697 9.814552025305373 353.90590947906503 0.21777088477960643 
8.321536765984925 9.60990301637883 345.5315916340695 0.2101876099196939 
8.493576204322101 9.783818279847228 353.1712511993563 0.21236639922409756 
8.675810308335526 9.983177765135906 361.1400334946824 0.2114648555641004 
7.856450126278968 9.038515305185832 327.8264764983864 0.20728475947914363 
7.132987001208175 8.625369079705525 296.7051217792107 0.20728475947914363 
epoch: 15, train time every whole data:237.96s
epoch: 15, total time:5040.25s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.92s
test time on whole data:66.86s
1.829207638045773 2.734683359538054 69.52042705043755 0.8414740067011821 
4.11837396206689 5.238778260369679 178.74257456649195 0.6390649532171276 
5.930872871558581 7.188988997988414 258.2502859163444 0.5358929629073909 
7.276870726710274 8.61290254364343 312.24846266320765 0.4366163626693231 
8.119578858619024 9.493569290090916 341.7223662881268 0.3412307044063598 
8.738321771444753 10.120747118467248 364.41804850594156 0.26968619969486873 
9.160254025332453 10.51275042456829 381.86786145472576 0.23640399415204044 
9.214973648567197 10.51651202471059 384.150457598648 0.22012961068374448 
9.13203476841925 10.41416359498239 380.29408313466246 0.21248065709587594 
9.386828963237061 10.662862814496119 390.97864737055863 0.21633152582663928 
9.61254483788079 10.881479623173593 400.79059913959793 0.21878996487237823 
8.55659702028343 9.749177160938816 357.2594808725417 0.21916137459957077 
7.589704924347123 9.174528922617553 318.3588704401648 0.21916137459957077 
epoch: 16, train time every whole data:240.14s
epoch: 16, total time:5357.93s
predicting testing set batch 1 / 168, time: 0.42s
predicting testing set batch 101 / 168, time: 43.08s
test time on whole data:69.54s
1.1230292282297853 2.420594122064061 38.9440055647108 0.8582134469575602 
2.2060883673772747 3.5813978159127915 87.96696056151815 0.6860424463864564 
2.751093464204598 4.445044183104645 114.11179862290766 0.6019458795000338 
3.3596093558157305 5.336923917015506 142.14391671299867 0.5164993662611588 
3.7854605391977265 5.910763608511203 161.7291729798791 0.446043355612262 
4.114318915775667 6.272803017657306 177.48269618152779 0.396780456256394 
4.433043090876369 6.5962940417213485 192.45372256715277 0.3615840138445611 
4.638292070282623 6.739821940434771 198.59745492008184 0.3308904421019644 
4.82255854239164 6.855684849584982 199.82661771077792 0.29840842350215957 
5.2656719600465145 7.250189160822651 210.03931172272638 0.26429181517266426 
5.785424680568367 7.731974131591901 224.1637597465713 0.23160567216853215 
5.719256183933644 7.505004102800565 216.79997206392895 0.20376136058564537 
4.000320533224995 6.0952338490291265 163.69147825612146 0.20376136058564537 
epoch: 17, train time every whole data:237.51s
epoch: 17, total time:5675.99s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.51s
test time on whole data:65.70s
1.3599442619864075 2.5167651100947523 53.094469812371024 0.8408035159748253 
3.2239917697842633 4.522468013300317 140.97325857649307 0.6465498508731682 
4.78449506466339 6.297771035611135 213.43413047621752 0.5491009506179858 
6.2008972596445435 7.7120583944824945 272.2368610480669 0.4544638949905813 
7.275576265114492 8.73744222278227 308.41727000135904 0.3408382974510868 
8.14093647990899 9.609061266859305 337.33459470325477 0.24330000873637733 
8.669280833359009 10.117054687372312 358.6658625609984 0.20186451996466492 
8.677002401272988 10.087333989125895 359.4465275886945 0.1933185817620836 
8.488148313281615 9.875410033526723 351.2900493014349 0.1953965872939172 
8.55289166724611 9.907223114085461 354.47556283596737 0.20160550882445735 
8.503013811713085 9.83270392951529 352.83684714777365 0.211958896415517 
7.498660596374422 8.69475577601809 310.6235171121699 0.22175485969576322 
6.7812365603624425 8.498373783467983 284.4076464071592 0.22175485969576322 
epoch: 18, train time every whole data:237.47s
epoch: 18, total time:5991.76s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.49s
test time on whole data:65.71s
1.4899199763846893 2.6405901314349602 58.61254217896881 0.8347562591537585 
3.585304999827984 4.8554609868672 157.73434668678118 0.6333143638595072 
5.105976628331201 6.625696222632713 227.9425546783302 0.5344435530020339 
6.424849961668608 7.988596443517804 282.38347425985705 0.4479828163067316 
7.347761987776894 8.900836001296208 312.26695574677035 0.35384180389495057 
8.046834286829101 9.595355172301735 333.7019253665539 0.2743177872762695 
8.529994449322777 10.024562657457425 352.0562926560125 0.23771449152182433 
8.548858330395072 9.948938568462244 353.28154971065953 0.23082043496467794 
8.395020075667738 9.745521257633305 347.0113938850004 0.2320394098949881 
8.498477189510883 9.821050716152739 351.8998735765222 0.24187473434777623 
8.50480683820136 9.827093291905095 352.86686624541164 0.24861777934882037 
7.674998858973118 8.886026745950208 319.0442561937568 0.24803720441366936 
6.8460669652407855 8.54398499128629 287.40502706557123 0.24803720441366936 
epoch: 19, train time every whole data:237.49s
epoch: 19, total time:6305.57s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.53s
test time on whole data:65.76s
1.2084091249061304 2.300320678637843 45.86977734073858 0.8684130343500644 
2.4437910477314144 3.8428877090394056 104.69393834341044 0.6797914474917435 
3.386813693880769 5.182042190995189 150.59969427611904 0.5882060839343092 
4.298762583305172 6.314129500757978 194.40640735949714 0.512725467379637 
4.843588959351803 6.917886484492996 220.83588646218232 0.44572109338875326 
5.205273233292891 7.262799972157355 237.06670074558338 0.3866330203674371 
5.568676632840807 7.575328203584061 249.34590865683393 0.33165607277971954 
5.788107803323085 7.691381604047379 250.52451210318557 0.28108095927340954 
6.03396266591815 7.829704908939102 249.68903710523725 0.23500200894128495 
6.632751778204171 8.360106413340313 265.7644390643561 0.20452489566545717 
7.274531444808291 8.946156189633252 289.3421464183457 0.19635001668012947 
7.180411303760839 8.69838096953031 287.3815486306445 0.19688068752345447 
4.98875668927696 7.016769314015325 212.1307440139924 0.19688068752345447 
epoch: 20, train time every whole data:237.57s
epoch: 20, total time:6617.00s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.44s
test time on whole data:65.55s
1.6884813485079933 2.5324368138272053 70.47712109985592 0.8569640759595839 
3.4411471788231283 4.903024958342622 158.86456304860778 0.6504913036938673 
4.7462468399575775 6.614325382077502 222.53225756784713 0.5594994599599488 
5.534722492510364 7.544876838410482 260.5510524449523 0.4937902916285539 
5.812413108661682 7.828128094000303 271.26169856959524 0.42627615825934656 
6.067531678001973 8.048943591602255 275.5575167054776 0.3539513411138426 
6.403286341156633 8.331032644209415 281.48716939239455 0.28952092035015914 
6.595785209302568 8.430667114658768 278.066667646295 0.23249899973939653 
6.77977575753097 8.53580825865486 273.7769195756706 0.18624351452646953 
7.226978371519684 8.92775622644238 285.79503535652907 0.1617725964165661 
7.702366887547075 9.356792384834424 306.6893013491926 0.16665980053225082 
7.8015835235196915 9.315632028580838 316.1796488764772 0.18826446078066927 
5.816693228086612 7.770511082135016 250.1065764887483 0.18826446078066927 
epoch: 21, train time every whole data:238.28s
epoch: 21, total time:6930.96s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.05s
test time on whole data:66.55s
1.2093700064132433 2.4132054167124655 43.832235084274025 0.8704414549560651 
2.371890627217967 3.831896029616195 87.78905376981803 0.6736330774326659 
2.794550279148455 4.60919438751703 102.65353869295745 0.5720162223840617 
3.1746838487193343 5.27093737757303 116.22506325532652 0.47571699760533753 
3.435937602335676 5.696543745032655 125.80370136667909 0.3970398733668302 
3.634604143587163 6.007021897923032 133.38661107167758 0.3350293966159758 
3.811706847783178 6.271971924298128 140.81271336054047 0.2878484669802424 
3.9031484540543917 6.36456327640764 143.79526467642106 0.25367493969988963 
3.959119749917782 6.400721618898834 144.54934205418039 0.22654071139627052 
4.07320944418971 6.5243546817035325 146.85438596516542 0.20203030093674265 
4.128279267532396 6.541816186092842 144.79587440421724 0.17882231948231062 
4.148686468225061 6.391875524464519 141.49631925127105 0.15625340997668932 
3.3870988949270298 5.664969362473877 122.66782481000263 0.15625340997668932 
epoch: 22, train time every whole data:239.33s
epoch: 22, total time:7249.41s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.43s
test time on whole data:65.50s
1.4671124041108858 2.6416611423502476 64.1650728184381 0.8485734425195381 
3.7446188705425176 5.532432885605644 174.03078099811495 0.61772232282113 
5.14212180330285 7.353281543377243 241.9016607017507 0.5309494787797159 
5.9073077971027015 8.155616095599392 279.2845714613001 0.47421428413286304 
6.076937079936266 8.227618988099913 284.57440460200576 0.4132156553747413 
6.264130421739515 8.310668707494386 283.77885800338254 0.3532209633229253 
6.624501569890107 8.543495085192628 287.7899331676574 0.29653303937686054 
6.937867150675239 8.706507571682655 288.83093490756164 0.2397341540336105 
7.387292160819683 9.098607820787963 299.1427469205152 0.19564372808401917 
8.035885272336149 9.727042838361855 325.0393417194338 0.18393657140097838 
8.318882930189371 9.913973234453547 339.8801274696845 0.19733881257026925 
7.893506563562457 9.318910727714082 326.15932588666766 0.21481589051008526 
6.150013668683979 8.196826167414077 266.21846675568025 0.21481589051008526 
epoch: 23, train time every whole data:237.28s
epoch: 23, total time:7565.46s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.45s
test time on whole data:65.54s
1.2049252126207133 2.1593351916769654 42.43680688632953 0.8818848714070867 
2.6405298424385313 4.068349976294544 104.69340640017715 0.6558952632206708 
3.7549906673095235 5.599086231761719 157.7553542511039 0.5533588289052511 
5.0778297363980895 7.022452819335291 219.47969742360658 0.481608570719907 
6.177561372973912 8.008953866809632 266.43366595328195 0.4045637944263054 
7.24676158397148 8.968181542297263 304.5488538818472 0.31533216053889096 
8.315039154303216 10.005225086101369 341.6970639754162 0.23598037283008394 
8.697775139694768 10.311318076002763 355.1525745679478 0.19924800530473571 
8.557487965909942 10.128497540776028 349.8305175133939 0.19229674675138803 
8.453458124215759 10.007344664087915 347.1644522103044 0.2048302875532347 
8.142192668292317 9.651334791727722 335.20859323346826 0.22250333345372741 
7.427477172692084 8.752324563311273 307.28727388732 0.22794987355119326 
6.308002386735028 8.293386421909604 260.9800382844861 0.22794987355119326 
epoch: 24, train time every whole data:237.73s
epoch: 24, total time:7881.12s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.37s
test time on whole data:65.46s
1.2960946758367298 2.3315790972490587 52.4963070891564 0.8736040307724405 
3.4900007216494884 5.078904970474528 154.83492597584316 0.6231169344246966 
5.013269276777282 6.945620239440336 226.2299439894312 0.5255174104451148 
6.093075112370331 8.020468329303545 273.7406238400625 0.4619459247715575 
6.652596781414534 8.42144005139287 290.8744779899426 0.3877577243881986 
7.247601909745129 8.936308633259982 305.0030030319288 0.30627651654562693 
7.938371238714794 9.617609516682856 326.6419149774274 0.23624362541931462 
8.184643923151794 9.815516437051564 334.5739076461817 0.20058481846754014 
8.092297646735927 9.676591585321866 331.0306443521358 0.19405360310159586 
8.060185758478939 9.615237312983561 330.9268492426872 0.20333834339084025 
7.863331973487688 9.354598816902653 323.42319265152753 0.21317994796600348 
7.241674155277333 8.514924754502056 299.01764150621307 0.22647262892584347 
6.431095264469998 8.31426387642228 270.73719208741096 0.22647262892584347 
epoch: 25, train time every whole data:237.44s
epoch: 25, total time:8196.23s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.29s
test time on whole data:65.38s
1.469140274838766 2.424387697083088 59.37752843397816 0.8773723396454015 
3.8001641987198522 5.316866860017881 167.25526888960005 0.635144388852578 
5.386428643015524 7.272284311871419 241.35735732401474 0.5350693969343757 
6.493718933832876 8.423632818219156 291.31860431566304 0.4665040175703939 
7.050656787126015 8.851757767387072 309.13606466415115 0.3918451210877734 
7.689816556504972 9.375518617178823 324.12994241513513 0.3153517484210236 
8.50793772305211 10.120032960101971 349.5926260239941 0.24514455461370221 
8.856023434411291 10.392655146727424 362.13494888692725 0.20184545413484942 
8.75751559115352 10.243306565260237 359.60753097605067 0.19824677972803476 
8.65704382933561 10.085161403730998 357.4008645504538 0.21348640021690157 
8.356757088485573 9.682996863189144 346.0911936597273 0.2309500642956608 
7.642017957782107 8.80614576448968 317.82511553723214 0.24517490254117874 
6.888935084854851 8.720718588104269 290.4403379438994 0.24517490254117874 
epoch: 26, train time every whole data:237.40s
epoch: 26, total time:8512.40s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.28s
test time on whole data:65.33s
1.1430453430761893 2.1134270161519773 44.75577049891574 0.8921628928280914 
2.9760597607281416 4.5824137554152 129.2883123282131 0.6513933124460922 
4.266151633970262 6.317388172236493 192.48021719696683 0.5554696634117812 
5.448021125654912 7.666729966152552 249.03776479757704 0.48528425378413975 
6.143900721760556 8.32519735670318 277.54927081709695 0.4134017749891042 
6.702559840894526 8.774392032958094 291.7151019580499 0.3412252491758406 
7.381709253897269 9.305676855660025 307.644789312654 0.27443699277244826 
7.924814847855163 9.72457855459691 319.6223964316279 0.214261689108755 
8.322720976915477 10.076353346659497 334.0726660111241 0.18760978850136484 
8.602817439221024 10.267665203773847 349.5045273210044 0.19505464243416076 
8.537372977110335 10.054068010683196 350.8633054298947 0.21617031340665238 
8.038882654699895 9.320493495410522 333.92128451200864 0.24027084148207478 
6.290671381315312 8.399051909385275 265.0432033186453 0.24027084148207478 
epoch: 27, train time every whole data:239.32s
epoch: 27, total time:8829.21s
predicting testing set batch 1 / 168, time: 0.43s
predicting testing set batch 101 / 168, time: 40.27s
test time on whole data:66.84s
1.0535831996885439 2.200695160377721 34.226164427238196 0.8895368271567818 
1.994357879795134 3.368606323642048 73.76975135627396 0.6951627036819737 
2.2709543904063425 3.818765513535894 86.15035624674744 0.6113660949367167 
2.609814004000188 4.339606322735689 100.82184077558995 0.5207965097988617 
2.9195547455913786 4.796465174706077 114.03016246833182 0.4409842970144133 
3.230505965480226 5.189199200255133 128.08368543548062 0.38025084575094603 
3.551903766084995 5.546444570167247 143.4876363155854 0.3365782788166257 
3.7777708651881134 5.740029802831406 152.45034008493806 0.30845417508212003 
3.955618620055772 5.845048959783759 156.31997329376063 0.28983649914766296 
4.24319350962802 6.037965329748516 163.5893806158424 0.2735633359381549 
4.648204980240514 6.271454487638858 176.19144423007523 0.2652624321210973 
5.42424642315436 6.98621283470354 206.7981446809638 0.2527626375243423 
3.306642362442799 5.180174632541627 127.99586177521451 0.2527626375243423 
epoch: 28, train time every whole data:238.91s
epoch: 28, total time:9147.20s
predicting testing set batch 1 / 168, time: 0.38s
predicting testing set batch 101 / 168, time: 39.29s
test time on whole data:65.29s
1.0577673042740319 2.035850067607233 39.97333336730879 0.9052622411266055 
2.345240795493924 3.857053512161611 90.98943085831698 0.6746625795140586 
2.8691308001020834 4.800349263202816 112.2789173992767 0.5712860207574956 
3.4219069068548933 5.7204280875792906 134.82460555231577 0.47196636488674487 
3.7966131768478526 6.276768480658304 150.2551130979885 0.39596858055395057 
4.077125121156552 6.631677143166206 162.3694035803338 0.34199541191003563 
4.339434547138356 6.936698530019609 174.29257566702873 0.30151732595359243 
4.473868037658611 7.036028469033776 179.4333977962743 0.272533480525232 
4.56190806347584 7.076440187027445 181.65999280123492 0.2511400558455405 
4.72508663744373 7.218264895589847 185.19614448828315 0.2311636905811355 
4.858359934522016 7.189177383178708 185.1456301126482 0.21761075879264724 
4.957151178159884 6.977507143874599 184.36551536892998 0.21443332915778485 
3.790299375260648 6.180196035623737 148.40120784577755 0.21443332915778485 
epoch: 29, train time every whole data:237.40s
epoch: 29, total time:9461.69s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.50s
test time on whole data:65.60s
1.061412364800505 2.0041352385204783 40.84736647725296 0.9059066128647707 
2.26160470629448 3.7370359964278284 93.11843028343024 0.6799543814253137 
2.956568325097983 4.855328330237594 125.8590953942068 0.583281424054173 
3.681080201081133 5.887021004215947 159.9292086677584 0.498562947370625 
4.165898763263243 6.4837060944654 182.9162338198874 0.43032071122829 
4.511208178197433 6.823557854964951 199.3777853239516 0.3824655265015508 
4.828247916867513 7.101204127656107 213.8343567373366 0.3477681066014267 
4.9984224927915 7.200985444451867 218.65530726237515 0.319583916982956 
5.130313515823157 7.251527384203434 217.03410306297263 0.2936862957085323 
5.381724840736992 7.391528586131148 218.6243324628719 0.26624620969948026 
5.715151154911589 7.520538028067019 225.120348979867 0.23745988307734717 
6.079431804057566 7.767331607587588 234.6139652487105 0.2124055361614985 
4.2309220219935915 6.397937039402825 177.49757293984067 0.2124055361614985 
epoch: 30, train time every whole data:237.80s
epoch: 30, total time:9776.29s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.45s
test time on whole data:65.58s
1.4211475066874353 2.499197089904098 59.50169586086717 0.8685722449536304 
4.123320567142662 5.785059450436928 180.95177566184594 0.6091584735854881 
6.128182361956686 8.195614726202491 270.94839735709104 0.5037016541810013 
7.619672826685721 9.696525627004572 333.86631435188394 0.4256943804689659 
8.478387215987292 10.376372393861951 359.11598134169645 0.33472048284330513 
9.267657326116183 11.049737761712292 378.85916055077985 0.24238555170552556 
9.88783511292553 11.62205603440471 401.32559763655485 0.18576333232410414 
9.858903729290125 11.560699160181324 401.78059496054806 0.16791344418085513 
9.557179009996975 11.240952117502042 391.003893304778 0.16958145615233491 
9.365048010304038 10.978344475996805 385.4467495263779 0.18070965343306455 
8.915731253505818 10.319852735796859 369.97431804220867 0.19955389505742374 
8.081478319120993 9.298786161368659 338.259568503777 0.21202177777288536 
7.725378603309955 9.74355002731537 322.5912722735935 0.21202177777288536 
epoch: 31, train time every whole data:237.41s
epoch: 31, total time:10091.00s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.41s
test time on whole data:65.59s
1.0256908235737965 2.075213650761809 40.502474926926126 0.8951726275857012 
2.927702082690827 4.703608968284317 126.79922525529945 0.6483183162861739 
4.331500752662885 6.655993683559601 194.0501276684246 0.552215916567688 
5.5376363224095355 8.126782872583522 251.5599231891955 0.48192809538464254 
6.100080762862804 8.61984525146447 276.81334780274705 0.41952407355117255 
6.501811449998812 8.922727934751618 287.93609341809236 0.3524141471230849 
6.988421793879054 9.303608096185352 297.6816831031354 0.28608584971345263 
7.324168944771446 9.494482616112371 297.0361876564209 0.22317336339690894 
7.673334444078838 9.700522554594816 299.64565260325594 0.17198554166171967 
8.242015901950232 10.160533125862715 323.79004943063956 0.15443565184792238 
8.466729833513941 10.156952411155956 341.59244457967844 0.18149132690039835 
8.178901862550438 9.64734265060272 337.38756937895494 0.21101724746832753 
6.108166247911885 8.470379292191174 256.2376679493553 0.21101724746832753 
epoch: 32, train time every whole data:237.62s
epoch: 32, total time:10406.92s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.38s
test time on whole data:65.49s
1.2300591016105775 2.3061025958138273 47.904428092521165 0.8755362120055671 
3.6262084618039605 5.279819466620272 154.96174801777153 0.6100688424474299 
5.59662631364965 7.57946851422931 244.4210678506714 0.5023323261513618 
7.200419145173703 9.175497376453336 312.14773016478796 0.42115272148869587 
8.218319057363928 10.063342330879944 344.8082012143692 0.3218629399627486 
9.042135617306013 10.870264804005345 368.7141860586121 0.2245531343973662 
9.557971001295462 11.3413504604466 388.1869240132932 0.18305274418115935 
9.509930148046463 11.239915896690096 387.2479409696 0.17570436338109158 
9.221583067087012 10.920463154054849 376.91127705869405 0.17848908624535603 
8.977000269779403 10.589830285907658 369.15810212851585 0.19168061195898173 
8.447602587752753 9.863236839216366 350.26401361888634 0.20199116788095603 
7.622379719563538 8.824507407143638 318.21692389236136 0.20863408844982503 
7.3541862075360385 9.379874493490652 305.2504503855481 0.20863408844982503 
epoch: 33, train time every whole data:239.34s
epoch: 33, total time:10722.63s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 40.12s
test time on whole data:66.56s
1.1811579715124376 2.227432465629718 48.398349813174754 0.8890138162179798 
3.209237289646197 5.091956310174293 139.96182419448894 0.6411914811122813 
4.617801389103844 7.060047226185424 206.01177879828518 0.5484623649102149 
5.656723221057671 8.303832199987045 255.9712281655358 0.48145439044993454 
6.034643291893814 8.556995617553083 273.3256081173848 0.424274533879282 
6.270414278572425 8.663001331665885 277.6355747453392 0.3638641196355605 
6.59697503139123 8.859590459975063 281.59914359531575 0.3079200759091039 
6.831268620387429 8.924756949696135 279.16086661297464 0.2588858402385435 
7.114247043142273 9.049474958554923 280.53803090471666 0.22092975663872966 
7.654279453068883 9.501535062181834 299.3528989222973 0.20562929901354485 
8.02318587564136 9.747266393993357 318.72115507971756 0.21067674770304334 
8.057730466019894 9.623811867243601 327.69958970298075 0.21407200492026263 
5.9373053276197885 8.245705391652008 249.0353324468012 0.21407200492026263 
epoch: 34, train time every whole data:238.67s
epoch: 34, total time:11039.05s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.44s
test time on whole data:65.53s
1.0141799409211214 1.9900352258215648 38.33927668629036 0.9052524574347716 
2.7000895201706636 4.573975312967983 111.83730249935901 0.6640740681958893 
3.8426484633617637 6.334375755923264 164.11373831773582 0.5761461500793039 
4.935496811719168 7.785073562969827 215.91534249249267 0.5014932195847308 
5.540245079455453 8.404756538486668 246.0831159777003 0.4396267713717254 
5.880091734997751 8.655591522528129 261.3836822711515 0.38583224790430143 
6.173195156030534 8.85531557794804 272.0386822419277 0.3369501993819946 
6.295678616873832 8.833710124624494 270.22994820114764 0.29367314277698764 
6.3754113966160055 8.737996114573036 262.99933513369956 0.25491230694731937 
6.601357571144278 8.815629841331402 260.78909320315734 0.22362770695786302 
6.819737074074468 8.798567502445866 262.14089163079274 0.20129258449236562 
7.001256448270398 8.829005118989382 269.72451946107884 0.18707043447926128 
5.26494898446962 7.837184069510514 219.6368312596136 0.18707043447926128 
epoch: 35, train time every whole data:238.23s
epoch: 35, total time:11354.82s
predicting testing set batch 1 / 168, time: 0.38s
predicting testing set batch 101 / 168, time: 39.50s
test time on whole data:65.61s
0.9379895785008335 1.9367397663347203 36.82995752916116 0.9084123672684433 
2.493443824737466 4.246141233075966 102.4434525049281 0.6697960379361151 
3.37948074886327 5.665232765228066 142.90011087605455 0.579851014126946 
4.266295206971466 6.85878604598981 185.8537953658403 0.5049635868001953 
4.88331414848637 7.52937460878849 216.11002080806801 0.4434279650448688 
5.3473159596500475 7.969544194337579 237.30330011065072 0.39311425138396283 
5.750524804772898 8.332075774483513 253.46624235953183 0.3507510328385097 
5.955258913230062 8.452311944908846 257.0931175006732 0.3131777375511057 
6.074276455628464 8.446339336137873 252.8657913851907 0.2803416088305781 
6.2921199884108665 8.528000102663837 252.05135180014597 0.252257935665778 
6.510299410738228 8.527356672921181 253.43423392592334 0.2282078419873104 
6.885037031952558 8.771622966820932 266.311276836637 0.2144201534754896 
4.897946339328544 7.392075038088984 204.7259104006963 0.2144201534754896 
epoch: 36, train time every whole data:237.53s
epoch: 36, total time:11668.32s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.60s
test time on whole data:65.79s
1.244104537795963 2.0451812519208894 45.92961943320494 0.9066639560736449 
3.1448179516529753 4.691879208932758 128.529638617228 0.6538820291597494 
4.541178423147914 6.534819362619445 192.03104625043937 0.5556125207395505 
5.7866993645037565 7.9751048158030695 249.44490197554873 0.4796769965823469 
6.511883094318478 8.658283070563554 279.5760955865586 0.4113745884475438 
7.072188916826 9.136125072339306 295.7442379569121 0.34392587084813825 
7.717143280298316 9.691157701762677 313.44875457453924 0.28532681674555954 
8.218910506803276 10.095587175081299 326.6113316207342 0.2410787606560259 
8.596551503140567 10.421573355530068 341.145034419745 0.22060552731840008 
8.846255063440296 10.588532552564006 356.1858867572814 0.2230211562348685 
8.573201414173646 10.125792888984044 350.60247342652065 0.2305476098121648 
7.921932955665425 9.239871784844595 328.477139765449 0.23730599449215495 
6.514572250980551 8.638141427796835 267.31587008547706 0.23730599449215495 
epoch: 37, train time every whole data:237.58s
epoch: 37, total time:11982.46s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.66s
test time on whole data:65.85s
1.1791667786947495 1.8967798953063006 44.54624936490724 0.9120158508721165 
2.5063441920034766 3.939271287493684 101.56449910182721 0.6733882521897819 
3.396034426538362 5.20906194223029 144.0179905380086 0.5843674442577163 
4.408898256659951 6.48887205470935 192.2276980071492 0.5064738467069664 
5.177285131678695 7.313067280642902 226.7136905636915 0.44202505412002174 
5.75361899908366 7.815384979012024 248.42234516947735 0.3855717906447545 
6.305966412196113 8.297461669609147 265.85539393810524 0.3280410441376005 
6.675079972341391 8.570079379564852 272.18723053678383 0.2755093714849824 
6.957991691166358 8.741159655263845 275.10642227633036 0.23450322655251385 
7.3453898276598855 9.045413558342482 286.796924113519 0.2081594110883785 
7.530853851556955 9.056316184970985 297.7407422024291 0.2006197669933759 
7.519584047618987 8.885971186134404 303.9498188544228 0.2146413384633596 
5.396351132266549 7.440964714467355 221.59877201614523 0.2146413384633596 
epoch: 38, train time every whole data:237.73s
epoch: 38, total time:12298.32s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 41.63s
test time on whole data:67.80s
1.0507359366735354 1.927053428284633 40.69839700644709 0.9082168531452244 
2.6527663179109138 4.335438549820187 112.07799396210432 0.6683156156347351 
3.8725560582325396 6.126516702426975 169.22741940561622 0.5746575737182372 
5.041012640729813 7.63064533573682 225.09224867126298 0.49779176032320244 
5.673708563579512 8.236659840084043 255.31794702038982 0.4390347909515733 
6.021336648564431 8.475877350310338 267.79692696256114 0.38220198277200235 
6.366364735504346 8.747978971059556 275.9030474835328 0.32611376429904154 
6.556966959142437 8.797025994715261 274.6070844840028 0.2826146576361651 
6.749698449277363 8.836355803129047 272.66654099762593 0.25314828235292824 
7.178869084417998 9.161961070160014 282.2376578744759 0.23182735542557611 
7.475598270251105 9.343963294464816 292.171123802094 0.20981256081950983 
7.461363680514552 9.18192853465773 296.0738520302588 0.20775044667254827 
5.508414778733212 7.8819866271279695 230.3267727878754 0.20775044667254827 
epoch: 39, train time every whole data:239.31s
epoch: 39, total time:12617.10s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 40.08s
test time on whole data:66.62s
1.2512307969103789 2.092890476705845 49.00084431554692 0.9042801453069358 
3.4856364313408377 5.089886940378556 147.76326599384788 0.6443167747180547 
5.3037728959904715 7.439672956431577 230.3041145719423 0.536793871341105 
6.747992104139179 8.99392400075042 295.7983979980185 0.4632671536871743 
7.461557914508063 9.57519636826882 322.09630973158994 0.39297709398395536 
8.021391930302693 10.000485078013147 334.04400136626583 0.3213527898255082 
8.737280980583812 10.621417709046192 352.38094072524564 0.2580756969150637 
9.245821904692798 11.042474198987575 368.2286315872688 0.21481011633806088 
9.460395455405116 11.190063999869801 379.68975853457476 0.20333676949772705 
9.560709247340315 11.216580722160577 387.8945982555839 0.21961492902303356 
9.254455078308693 10.752226733701791 379.4354277112416 0.23536396378530036 
8.64722065509368 9.951844967803993 358.76406767707607 0.24825147724951213 
7.26478878288467 9.392595459230716 300.4556876905396 0.24825147724951213 
epoch: 40, train time every whole data:238.81s
epoch: 40, total time:12934.59s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.35s
test time on whole data:65.51s
1.2863319169439908 2.2498104684506903 48.25356887882438 0.8968373390530812 
3.9170460865519763 5.784456972811295 158.82421381834124 0.637853036435564 
6.088953773617479 8.390470948184404 255.187350653294 0.5411796140388446 
7.687420956925356 9.990643097348725 327.974608213051 0.464026594589647 
8.487205188474485 10.628784114892298 357.95251952822287 0.3818869753120923 
9.115571720807768 11.196257770942696 374.39604718510805 0.3055742054975045 
9.697695921735306 11.762023028274589 392.0705196427693 0.2562940478475937 
9.76307183705563 11.706826669627223 393.9500808295489 0.24091344364328643 
9.548689752116623 11.40924535722838 386.5131639340641 0.23523850103113822 
9.444655504437874 11.23140287568479 383.7236831958156 0.23999438273645357 
9.027557696103162 10.635408522079059 369.225542570548 0.24200814427829243 
8.372679378241566 9.753618609055803 346.0261418330539 0.2424708262888426 
7.7030733110842675 9.94575721371401 316.18020699646064 0.2424708262888426 
epoch: 41, train time every whole data:237.50s
epoch: 41, total time:13248.09s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.52s
test time on whole data:67.35s
0.9012682609347006 1.8048882766313816 33.383590686652596 0.9190488290524403 
2.3891355964190195 3.869189820887043 98.98421249246174 0.6903390199122577 
3.3904407196671125 5.398296476147851 146.47558065558286 0.5919407902364432 
4.478344908845389 6.852240823752932 198.09151887305163 0.508705592942835 
5.2741371597195315 7.7192723571163375 234.3621420010544 0.4429527511405022 
5.8634239179299525 8.230171536212971 257.65390887692143 0.38256562445596876 
6.413512538167426 8.71685143556003 274.67153168210785 0.320899170532234 
6.711841267689531 8.91854602852162 277.0940704320913 0.2646878411864304 
6.857202440999714 8.940450482224811 272.25890943955903 0.2215471545286884 
7.099885661356151 9.058293087741065 274.33388905040783 0.1932409567264792 
7.2434766348823905 9.002675024909673 279.27928864804016 0.18110169706349605 
7.560521811717855 9.253332719242108 299.08968306853944 0.20089120862124402 
5.348599243194064 7.66934680328801 220.47781766174847 0.20089120862124402 
epoch: 42, train time every whole data:237.98s
epoch: 42, total time:13560.77s
