total training epoch, fine tune epoch: 10 , 50
batch_size: 4
folder_dir: MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_ed_gts_detachpe
load file: data/bike.npz
ori length: 3001 , percent: 1.0 , scale: 3001
train: torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12])
val: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
test: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
TemporalPositionalEncoding max_len: 12
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (tcn2): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder2): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttention(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): spatialAttentionScaledGCN2(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt): Spatial_Attention_layer(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (tcn): TCN(
          (filter_convs): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (gate_convs): Conv1d(64, 64, kernel_size=(1, 3), stride=(1,), padding=(0, 1))
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (src_embed2): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed2): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
  (prediction_generator2): Linear(in_features=64, out_features=1, bias=True)
)
create params directory experiments/bike/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEcross_ed_gts_detachpe
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.0.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.0.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn.gate_convs.bias 	 torch.Size([64])
encoder.layers.0.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.0.tcn2.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.tcn2.gate_convs.bias 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.1.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.1.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn.gate_convs.bias 	 torch.Size([64])
encoder.layers.1.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.1.tcn2.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.tcn2.gate_convs.bias 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.2.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.2.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn.gate_convs.bias 	 torch.Size([64])
encoder.layers.2.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.2.tcn2.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.tcn2.gate_convs.bias 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.2.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.2.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.3.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.3.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
encoder.layers.3.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn.filter_convs.bias 	 torch.Size([64])
encoder.layers.3.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn.gate_convs.bias 	 torch.Size([64])
encoder.layers.3.tcn2.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn2.filter_convs.bias 	 torch.Size([64])
encoder.layers.3.tcn2.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.tcn2.gate_convs.bias 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.3.sublayer2.norm2.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
encoder.norm2.weight 	 torch.Size([64])
encoder.norm2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.0.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.0.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.tcn.gate_convs.bias 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.1.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.1.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.tcn.gate_convs.bias 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.2.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.2.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.tcn.gate_convs.bias 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.2.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.3.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder.layers.3.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.tcn.filter_convs.bias 	 torch.Size([64])
decoder.layers.3.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.tcn.gate_convs.bias 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.3.sublayer2.norm2.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
decoder.norm2.weight 	 torch.Size([64])
decoder.norm2.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.0.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.0.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.0.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.0.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.0.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.0.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.0.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.0.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.0.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.0.tcn.gate_convs.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.1.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.1.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.1.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.1.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.1.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.1.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.1.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.1.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.1.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.1.tcn.gate_convs.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.2.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.2.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.2.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.2.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.2.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.2.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.2.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.2.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.2.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.2.tcn.gate_convs.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.3.self_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.3.self_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.2.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.2.bias 	 torch.Size([64])
decoder2.layers.3.src_attn.linears.3.weight 	 torch.Size([64, 64])
decoder2.layers.3.src_attn.linears.3.bias 	 torch.Size([64])
decoder2.layers.3.feed_forward_gcn.Theta.weight 	 torch.Size([64, 64])
decoder2.layers.3.feed_forward_gcn.Gate.weight 	 torch.Size([1, 64])
decoder2.layers.3.tcn.filter_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.3.tcn.filter_convs.bias 	 torch.Size([64])
decoder2.layers.3.tcn.gate_convs.weight 	 torch.Size([64, 64, 1, 3])
decoder2.layers.3.tcn.gate_convs.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.3.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.3.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.4.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.4.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.5.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.5.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.6.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.6.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer.7.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer.7.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm.weight 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm.bias 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm2.weight 	 torch.Size([64])
decoder2.layers.3.sublayer2.norm2.bias 	 torch.Size([64])
decoder2.norm.weight 	 torch.Size([64])
decoder2.norm.bias 	 torch.Size([64])
decoder2.norm2.weight 	 torch.Size([64])
decoder2.norm2.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed.2.embedding.weight 	 torch.Size([250, 64])
src_embed2.0.weight 	 torch.Size([64, 1])
src_embed2.0.bias 	 torch.Size([64])
src_embed2.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed2.2.embedding.weight 	 torch.Size([250, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([250, 64])
trg_embed2.0.weight 	 torch.Size([64, 1])
trg_embed2.0.bias 	 torch.Size([64])
trg_embed2.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed2.2.embedding.weight 	 torch.Size([250, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
prediction_generator2.weight 	 torch.Size([1, 64])
prediction_generator2.bias 	 torch.Size([1])
Net's total params: 860802
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499]}]
predicting testing set batch 1 / 168, time: 0.67s
predicting testing set batch 101 / 168, time: 37.32s
test time on whole data:61.92s
62.363445328909904 82.27543939051799 2529.2768073891148 0.0011432972280246397 
59.702938537078246 80.032242493533 2424.7334217107655 0.0030370474583060894 
59.76494210678561 79.98888630915559 2426.2109979740444 0.002646259089126714 
57.14157591855322 76.2373570945549 2319.073601462095 0.00261726030987649 
54.20325653744897 72.13729365485659 2199.54749017498 0.002528387767265207 
52.29712955283551 69.7643675457435 2123.405920009586 0.0024110610178200015 
52.28594127430998 69.88417655377114 2128.3240989612464 0.0022919022221642073 
53.518546584862776 71.22057969250574 2184.082520802907 0.0021787857407582524 
54.57291058153943 72.29447232478303 2229.4977967203613 0.0019697851956555546 
54.726134209143 72.61837302478277 2234.5895239710567 0.0017124090790139543 
53.61317923644824 71.32459100769431 2186.556531703101 0.0014347282001505 
54.41431657622364 72.77333998002732 2211.9673492203915 0.0012063808876319195 
55.717026370344875 74.32746501023742 2266.432520349194 0.0012063808876319195 
epoch: 0, train time every whole data:233.04s
epoch: 0, total time:305.43s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 38.56s
test time on whole data:63.81s
2.830017682729289 4.737824657609131 59.322407892146714 0.3616208340706016 
2.8672936203030073 4.780469490603195 61.4787170775405 0.3009238709121581 
2.9020608519446105 4.845122804548145 62.04049424424173 0.2708276383285166 
2.9117298865249115 4.848617390966767 63.10389443260084 0.2509875355801826 
2.918081971360459 4.831487807806413 64.13408248882332 0.22843402321719924 
2.9235860939709735 4.8208487844496135 64.79394769245881 0.21395892312765344 
2.9216807846742725 4.816735542850561 64.32813869747653 0.21373187364647594 
2.9212632216149497 4.82397695293591 63.6925653989465 0.21774384509868056 
2.9229168982913807 4.826941258240041 63.53955293900153 0.2174516873537935 
2.922550188497596 4.815280430138647 63.69865763608814 0.21440081761557403 
2.921719110399396 4.793883784968575 64.10346207243272 0.20788051951069703 
2.9108582290921006 4.786387512769228 62.39578191526509 0.2138670610205966 
2.9061465449502455 4.810724509721804 63.05270707007599 0.2138670610205966 
epoch: 1, train time every whole data:235.41s
epoch: 1, total time:616.18s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.22s
test time on whole data:63.10s
3.6388112805193913 5.45986305014344 115.90577492388661 0.40770435862881876 
3.7590689931964234 5.5328485602072766 121.53659708397836 0.3958174311391445 
4.094298414231145 5.775106722733911 134.6380521923262 0.38623429407692805 
4.3153004084612645 5.94763914708495 143.83737370228314 0.35414603493151897 
4.521730800620945 6.133865613220235 151.95870826641945 0.3169378775068136 
4.6974916288167945 6.3092551561158885 158.8223324676692 0.2790594952561003 
4.855382730640737 6.476140102676589 165.09061096696084 0.24468767748168505 
4.986747321625817 6.621066809267432 170.17049942529238 0.21293441588298948 
5.086047822214131 6.735241058271526 174.062851997379 0.18650196145912892 
5.144781465844029 6.808727230179225 176.4184782848514 0.1669783046243072 
5.133401817118749 6.8026916403196065 175.6848835984249 0.15203339905300664 
4.809884238368344 6.524500337221917 161.43341525326085 0.1439341213681888 
4.586912243471481 6.277782702511342 154.13117429507653 0.1439341213681888 
epoch: 2, train time every whole data:234.70s
epoch: 2, total time:924.89s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 37.88s
test time on whole data:62.98s
2.4866431193442216 4.342736095199499 61.63167398258733 0.48696817812570914 
2.5789819152111275 4.4228216985579625 68.3492527158394 0.44103395865127015 
2.6884263577294494 4.535252370480758 74.31116170885372 0.4092496252398187 
2.8394853258792843 4.679323116630286 82.16416503114907 0.3670001778218893 
2.999100557916576 4.8475863922743 89.73828300666807 0.32745751620033875 
3.150583465217746 5.015263785590723 96.58656591710239 0.2902211300606477 
3.3040054387000524 5.199664461751801 103.32924895444096 0.25595911529815857 
3.429869460095696 5.365353345315094 107.84344862633893 0.22445804052485516 
3.5169507443310604 5.470560459965134 110.37062758644677 0.20097422949955865 
3.580419731513109 5.532826514343615 111.64445156012029 0.18191975416003053 
3.5661121433055296 5.494987509581811 109.69777688939901 0.16721181600959273 
3.3667667719602052 5.2082209301949725 97.61944783993083 0.1607099634603244 
3.1256120859336716 5.026765304681641 92.77485569445804 0.1607099634603244 
epoch: 3, train time every whole data:234.86s
epoch: 3, total time:1233.54s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.14s
test time on whole data:63.10s
2.7576599623763136 4.248343686587093 77.70356781836374 0.48596375080444393 
2.976689400232885 4.41566864289711 87.51136623097665 0.44437634167642637 
3.254411562198418 4.68503563739048 98.8765367628814 0.40126994189457177 
3.5047089120028097 4.952478397200741 109.8404731600583 0.35512060499998654 
3.734402053850481 5.228231588536365 119.06076814574654 0.30983204549785903 
3.9177840808121753 5.462417325825892 126.34851577251993 0.2700174042417521 
4.053916631855869 5.649134479259546 131.66659465717296 0.23727390165607143 
4.12373238360261 5.757171383673811 134.36588272237535 0.21093023223578397 
4.187146208826009 5.838942923422418 137.06829260795686 0.18855388220684086 
4.259590013206094 5.9273487789834505 140.05331830738402 0.16827739151578586 
4.267298111094428 5.953612084670859 139.21296066303518 0.14775341093497132 
4.159010539514944 5.813782990927736 133.43171372818088 0.12563709642639795 
3.766362488297753 5.359769846296518 119.59622833822685 0.12563709642639795 
epoch: 4, train time every whole data:234.87s
epoch: 4, total time:1544.86s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.27s
test time on whole data:63.38s
2.559925051968722 4.204032177302134 66.98577834584685 0.5247883292422012 
2.6934116272781754 4.324072146303584 72.823623266162 0.48386479321767883 
2.836543996729755 4.486858901071173 78.3329992169737 0.44542239727724664 
2.9718219100414287 4.637943181399729 84.54584590400418 0.39773478816580277 
3.114288116201936 4.814859195888761 90.48476965824244 0.3518425044036791 
3.2391031436972497 4.975456540297829 95.81233434398217 0.31158701086289153 
3.347043829342173 5.118041421181496 100.21655954248511 0.27652364456481515 
3.417360132887516 5.219563242205787 102.7660408500704 0.24408880904230232 
3.4850167370843037 5.294088810622144 105.21875444659818 0.21555525807348258 
3.5508600912557116 5.36095439019802 107.55406391114248 0.19023235798165694 
3.554234921835097 5.3688108179572405 106.23828409560994 0.16905265851841886 
3.485963181583832 5.274564336164513 101.62812413746572 0.14931429338125068 
3.187964394992158 4.939656688130239 92.71807890683623 0.14931429338125068 
epoch: 5, train time every whole data:235.14s
epoch: 5, total time:1854.42s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 38.20s
test time on whole data:63.35s
3.1620427278624406 5.017660802081584 103.29798225591784 0.43673873529194174 
3.353366662799425 5.171010242559455 112.1945902383794 0.40280855595940457 
3.5157113741624744 5.3164928608551625 119.52371090281927 0.3784637799617851 
3.6532298726686943 5.459107930722503 126.4625917795108 0.34718433314398944 
3.8336668996183825 5.6539518159029685 134.9940518910571 0.31233497523512904 
3.993107534159862 5.838282448992653 142.7594981572127 0.2798292275625501 
4.123118215319213 6.008546637682882 148.83508485535407 0.25261584629839257 
4.172063706584275 6.1041633210870625 150.68260806621274 0.2304232035804465 
4.175163136184571 6.135682820731153 149.93463843474603 0.21365111704940315 
4.205684457509557 6.173098306288988 149.89903850163296 0.19627644711681375 
4.20707633055986 6.145774516505228 147.8950436744453 0.17820885705981765 
3.9907587453331796 5.878095974577692 135.18605560786852 0.16305099816564433 
3.8654158052301613 5.755256157783652 135.1396593511692 0.16305099816564433 
epoch: 6, train time every whole data:234.76s
epoch: 6, total time:2164.54s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.47s
test time on whole data:64.19s
2.267991141811102 3.754042833258362 61.680591070907006 0.5799502677037957 
2.707642552546004 4.017215198870306 93.11203854607919 0.5370480677419135 
3.0351614401839853 4.388176078685386 111.36119860062624 0.4999018248360434 
3.4291685592095766 4.868983736022065 132.1106863462829 0.44994436928630993 
3.7371516025797242 5.296466620828116 144.12978057828286 0.38344255531293736 
4.109167718277623 5.7526115154338875 156.6467269007918 0.3246143036380065 
4.559050097938805 6.265849076302532 171.96875210478856 0.2738877752611538 
4.90553279009355 6.663287959631834 183.0510708517747 0.22590958687928953 
5.116192143699598 6.917610537056819 189.80000775302193 0.1958565652168241 
5.289753563885799 7.098868746321682 197.03485777791505 0.1905378884606282 
5.274115110335606 7.038679503070109 196.58023759251293 0.19519095267827363 
4.8467619644438935 6.460898962174311 177.29355296817388 0.20260307006157052 
4.106474057083772 5.82761253306517 151.23329667423783 0.20260307006157052 
epoch: 7, train time every whole data:235.02s
epoch: 7, total time:2474.70s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.41s
test time on whole data:63.75s
1.8719977776346994 3.297048951402078 55.4342001876307 0.700553671965137 
2.6566647324966532 3.9859122465448382 97.92771975884627 0.5669842622599419 
3.3035027026287502 4.679523874417002 127.0011489622523 0.4836057015121737 
4.070332174420179 5.503444092353227 158.39758902837426 0.3972038824549657 
4.677665597208199 6.192491319708724 181.53782992752014 0.31747693688223794 
5.13520641854574 6.771650414132923 199.18702542852958 0.26517200801151397 
5.547226128755581 7.318298434404681 215.5390612160721 0.23470134406491291 
5.783875076485354 7.670071382359093 224.22642816713184 0.20926049834973887 
5.916828926169269 7.8617154857389 228.8438779448138 0.1929645104115246 
6.0603537137744325 8.003749500314049 234.71765474772332 0.19048293598763782 
6.092059818091403 7.976742817093711 236.0138709015185 0.19417424666300015 
5.683970606672976 7.409294970454812 217.56786058948109 0.1883316609055339 
4.7333069727402695 6.5823387955139046 181.36956297258294 0.1883316609055339 
epoch: 8, train time every whole data:235.27s
epoch: 8, total time:2785.55s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.11s
test time on whole data:63.41s
1.697041132545897 2.977133555455176 59.926360353431775 0.7817992253282827 
2.41933038072971 3.897051973094313 95.70130890323384 0.6186276583600409 
2.8397469104444166 4.462330901411684 117.03306588171768 0.5612912843110974 
3.345643023757353 5.125912794532642 142.26499421999267 0.503103324611933 
3.751896148480209 5.658595868663466 160.57131173948093 0.4424666212400772 
4.193095276982479 6.2261433189353586 178.16569670905096 0.3872813515976967 
4.684244569183371 6.852734578661679 196.94417134628353 0.3394165779978184 
4.958345403432491 7.240171028613835 203.74992622002733 0.2915018448582975 
5.136478565395144 7.469133037242142 205.36669465781284 0.2546121775150089 
5.376326702407694 7.704792173050248 210.26720651233305 0.23393075496568538 
5.507450331923951 7.7522008722250915 211.38221955462146 0.2253295041733473 
5.210929350653308 7.220490694866715 195.95934405401897 0.22025745122929613 
4.093377316328002 6.243571413173101 164.78057595589607 0.22025745122929613 
epoch: 9, train time every whole data:235.97s
epoch: 9, total time:3095.05s
fine tune the model ... 
epoch: 10, train time every whole data:1101.31s
epoch: 10, total time:4196.37s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 38.34s
test time on whole data:63.33s
1.2970821094683238 2.622137973076664 39.749402537657545 0.8425248756330298 
2.00948543165837 3.56055270968135 62.00312732167911 0.6788209908995322 
2.130838991463894 3.796948250127475 63.58618790507522 0.6243864842249329 
2.265476758894023 4.041122393108706 67.0423127604297 0.554343304965118 
2.404791826771129 4.285020078591132 68.74377226452664 0.48864546008765797 
2.5282379728824433 4.4772066299838515 69.59503324169522 0.4289115083649458 
2.6322957412932246 4.628105174534239 69.53464045776686 0.3733321272494235 
2.7265530925704433 4.770879569376945 67.95990462335618 0.3283718646227214 
2.7999355027875197 4.8523676259662345 66.93611151202535 0.3016525380481553 
2.846325865193137 4.878171452234534 66.65786714552239 0.28501231074009153 
2.881535864870729 4.878233267695754 66.32733513698294 0.2709386325981058 
2.9444780763172145 4.956393327665893 67.54741883509321 0.2653128545567328 
2.455586436180871 4.3645644870513305 64.64054860299149 0.2653128545567328 
epoch: 11, train time every whole data:1100.13s
epoch: 11, total time:5369.11s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.58s
test time on whole data:63.77s
1.256313286183668 2.5079064502655277 39.90194636190702 0.8490816225565014 
1.9701988750045143 3.4926667843052477 60.67654463398573 0.6846796636537592 
2.097564958739937 3.718049192061016 62.26110173309883 0.6321926941594418 
2.215830840098627 3.9466118030709025 64.6867791898909 0.5704761484222264 
2.3455452353726365 4.185272562832928 65.76906246986812 0.5085347436081471 
2.4676690563168378 4.374633709619976 66.31578015894502 0.4507558902379598 
2.5697571889759 4.524733905896089 66.4455921667636 0.39650866173195126 
2.6617308047164587 4.661897233581387 65.33638190458741 0.35227097394419704 
2.7304765326543046 4.742811361387055 64.76477064116001 0.32833069807203813 
2.7813260858557407 4.777776027215807 65.05098286992805 0.31442944224527425 
2.8226145296486185 4.789136200994812 65.39341531684383 0.3027976924949204 
2.8652967703698113 4.837155884888366 65.05842645415898 0.3015837906622357 
2.398693680328088 4.266181799774352 62.63866009841301 0.3015837906622357 
epoch: 12, train time every whole data:1099.23s
epoch: 12, total time:6542.12s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.57s
test time on whole data:64.19s
1.230524271223428 2.4967548158659145 38.52830266069995 0.8554981111778198 
1.9533814170790746 3.503696972485883 59.586696698551464 0.6920875197529297 
2.0688811658215487 3.733554106505591 61.187298535895195 0.6382559273613688 
2.196586058035759 3.9678314564192023 63.97030645115649 0.5745963200484108 
2.328158521365729 4.202322446373753 65.27471006006287 0.512150901698662 
2.4445550238498974 4.381225432082318 66.10900293520113 0.45519824538486664 
2.5410276859743255 4.5163478143961555 66.54760615765916 0.4038728260825874 
2.6280371035229235 4.636805484250933 65.50504687472973 0.3625394055800926 
2.6961601436331515 4.713080710495599 64.66553583966504 0.33878151487999186 
2.7481805233968686 4.755172434869062 64.69748722424231 0.3230266907407806 
2.791848428694975 4.772627188823066 64.9516504606186 0.3132063722926656 
2.8301520337946715 4.805268201192078 64.0114052923968 0.3134419585858958 
2.371457698032696 4.258698648703009 62.08653793442035 0.3134419585858958 
epoch: 13, train time every whole data:1101.69s
epoch: 13, total time:7717.87s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.93s
test time on whole data:64.89s
1.2204055154739568 2.493584414222973 39.196028632384674 0.8516450143435481 
1.9810230860719902 3.5284790627760856 61.73577554182399 0.6899636430350954 
2.0959024210486206 3.783164511749735 62.964791879638604 0.6383215014921637 
2.233513545898898 4.042287013622267 65.0914776387061 0.5780405407402227 
2.3811683990761106 4.297260254665862 66.32582609288963 0.5175188208175194 
2.5047692427079062 4.4832737298572285 67.25229473759204 0.46315113030670435 
2.6015325027016836 4.618669071584998 67.91390849372921 0.41764099217548284 
2.6942615710982962 4.750034510941914 67.84540137433872 0.3798385593593036 
2.775648695517243 4.8476075584275 68.16337497582627 0.35442909547913815 
2.8409292089445075 4.909898046225026 69.15439593234007 0.3378276813573964 
2.8947099260506883 4.946407173648063 70.21701819067847 0.3247483187854201 
2.938940714340036 4.996524978808704 70.6752468548953 0.32595811247149226 
2.4302337357441615 4.366732858481506 64.71164902899706 0.32595811247149226 
epoch: 14, train time every whole data:1099.88s
epoch: 14, total time:8892.36s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.52s
test time on whole data:64.01s
1.2216745801827915 2.465766478438161 37.66511441455093 0.8546538012730136 
1.9452268551091354 3.4716653204062897 59.18593004099548 0.6889668765182612 
2.061016881193611 3.685667128313291 61.059568650449236 0.6392421364338678 
2.1808831927918253 3.9043314627478938 63.091100487883146 0.5826774194789824 
2.301744061936669 4.114081977585775 63.86307189940494 0.5265663021055723 
2.4016443905015255 4.262299603356698 64.63332330368138 0.4756588826732776 
2.47682815243082 4.359885657629999 65.44686340830476 0.4339442068498263 
2.5515455322189347 4.461614826462423 64.701823439489 0.398247245713671 
2.6169911466176132 4.5427326202358875 63.388509882674946 0.3750550403284163 
2.6708311704226015 4.589670707457585 62.52820081085676 0.36822010814936473 
2.723466945020571 4.637780330012819 62.16588661585541 0.3587930305921868 
2.765411366937682 4.694565456942471 60.960023606573735 0.3599539374587447 
2.3264386896136484 4.145452700349725 60.724367242455045 0.3599539374587447 
epoch: 15, train time every whole data:1100.78s
epoch: 15, total time:10067.76s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.19s
test time on whole data:63.69s
1.2387470410896377 2.4365689714200753 40.54738151796299 0.8511324016524309 
1.9401728392203472 3.4121050016251577 63.07086800964507 0.687987902209608 
2.0569619343095416 3.6416435705570582 65.58790737987276 0.6377233953973188 
2.1842376424354457 3.8748518404508543 67.42369845906921 0.5837590635254278 
2.31101536958709 4.10212408205472 67.6298271469821 0.5301482238644563 
2.421520789040341 4.274644820255228 67.95773501086119 0.4804436992352893 
2.5062574621187967 4.396387674936103 68.41046300980267 0.4394005335590849 
2.592140718882371 4.524688061820542 67.71006026172103 0.4040264465305337 
2.671755360481817 4.63414890788103 66.96252635688612 0.38025322803510986 
2.740252515503516 4.711007665274919 66.86979469867539 0.3712170622693508 
2.808576364178654 4.7853112637739565 67.60448603807397 0.357597761007866 
2.8646646495690304 4.8681208313497075 67.95765732721473 0.352615439987852 
2.3613585572013824 4.193478510204504 64.81129738509802 0.352615439987852 
epoch: 16, train time every whole data:1100.69s
epoch: 16, total time:11244.77s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.32s
test time on whole data:64.11s
1.1957418908690591 2.3763482324046885 39.322607214060575 0.8568468756237128 
1.9113158258409904 3.3504935540336964 63.60106432582619 0.6916618155524652 
2.0038860428458345 3.565988656499035 65.05176804015338 0.6435563664643903 
2.1206905612121556 3.7833055892530134 66.5853847014665 0.5920113190147718 
2.226342766282193 3.9803637305666864 66.15654048384481 0.5456441998380256 
2.319958706597221 4.1296544397136 65.73848879167672 0.5057985642183659 
2.3945411682319606 4.238465748096013 65.81449186880263 0.47082677167053183 
2.460398216421228 4.342178367734578 64.5788691163726 0.44473840264421904 
2.5151500774786055 4.419044018520265 63.1828367399528 0.43149360131544967 
2.5590349380130037 4.46525283984031 62.251139485972516 0.42916298764976674 
2.594596072620934 4.497641274606419 61.88663143342098 0.42700654708452396 
2.634764872495706 4.558699759525605 61.26238067357319 0.4238506060944036 
2.2447017615757408 4.021773176982524 62.11950586994266 0.4238506060944036 
epoch: 17, train time every whole data:1099.74s
epoch: 17, total time:12420.22s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.50s
test time on whole data:63.70s
1.2511517134121664 2.4204399929290776 41.05752014820749 0.8497316790309455 
1.9358630987410212 3.3526851225300045 65.40463098377307 0.6864924785203348 
2.0168867914194153 3.5693936839603295 66.61388768110473 0.6396973281384256 
2.12861526078243 3.789305717463518 68.21071147187602 0.5894038460310288 
2.2296047981914486 3.990410723195144 67.82915259478305 0.5446027668687362 
2.3232992217878676 4.148976454208514 67.4440904423483 0.5040209965254283 
2.402019985799988 4.263148629347211 67.66711972019668 0.4677730721804497 
2.4690217273624704 4.367372476205782 66.58240141472443 0.4397405909715352 
2.523356423628029 4.442463547085966 65.27593429621284 0.42377931348240794 
2.567864908248452 4.490031015092313 64.33349451638733 0.4193194097581969 
2.608663874217442 4.528713295715029 63.69928724073988 0.4176969193323605 
2.6426291684590812 4.570164302760838 61.970564095234856 0.41796815728740244 
2.258248081004151 4.040170222007859 63.84089138258939 0.41796815728740244 
epoch: 18, train time every whole data:1100.44s
epoch: 18, total time:13597.52s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 37.65s
test time on whole data:63.07s
1.2094761914382024 2.372431108807743 39.62211469879473 0.8572786041132961 
1.9020484013159182 3.3324577365621657 63.69742288930221 0.6966857824616932 
1.992545742926232 3.552665724712501 64.87588918117606 0.6498119599514028 
2.1043623124225332 3.773646819755224 65.98511685563457 0.601082907599748 
2.204849405805802 3.974063072273327 65.14815923960067 0.5607157001730905 
2.298034834270853 4.133189920718765 64.4809617029204 0.525570238362072 
2.3733942498475136 4.250604398147749 64.59693349468634 0.4948982790872671 
2.4446583690889003 4.363122458111522 63.82336351121006 0.4745125980418231 
2.5050045218114696 4.45269292534929 62.971506250585506 0.46480777163676523 
2.5579761098944362 4.520159430887723 62.74010273730168 0.4620135618331797 
2.6053876424767966 4.576818062139457 62.920489201303056 0.4595314501992793 
2.6568710936841864 4.650527404005924 62.79620903496674 0.4573076532883107 
2.237884072915237 4.045670976883913 61.971683547871315 0.4573076532883107 
epoch: 19, train time every whole data:1099.74s
epoch: 19, total time:14772.12s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.56s
test time on whole data:64.25s
1.2235658975702135 2.416567203866805 38.80714958557076 0.8551301125093724 
1.9294417847842864 3.352948022198166 61.81220323897571 0.6951294575381209 
2.017989549676134 3.5595076217472177 62.7947711331369 0.6495130626055216 
2.1203370754688624 3.754147053203729 63.8459356597587 0.6031743045584174 
2.211965094478446 3.9268247596819834 63.212066045809635 0.5650431331404436 
2.2914503968766935 4.051751325377548 62.91537619315262 0.5339786298739029 
2.3433933923115866 4.126030943349869 63.326178925788646 0.5093023995683331 
2.384586584818239 4.194055979444161 62.47760113144479 0.4917869137037889 
2.4152269725082887 4.236543003024646 61.403016701293545 0.48388709822900944 
2.4340247930677696 4.252520354250735 60.23794719074827 0.486178956166408 
2.4615059045815753 4.281133309049772 59.54208188076361 0.4831133833854361 
2.480917177662342 4.313615822168086 58.89803440360898 0.4795105926714785 
2.1928670519837032 3.90794340998532 59.93950756375334 0.4795105926714785 
epoch: 20, train time every whole data:1100.39s
epoch: 20, total time:15947.17s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 38.26s
test time on whole data:63.58s
1.177546863760267 2.414459787497251 37.05565776478713 0.8605799249594126 
1.9158921593703506 3.3927144485406684 60.00536204114911 0.6996008487975507 
2.007344810571167 3.608204417983806 61.47497178621647 0.650965742591915 
2.110207685921341 3.8126761147379855 62.84389997917115 0.6047864019160616 
2.2116859099146513 4.004109864441023 62.47139430187856 0.5685056507545387 
2.2981437145507053 4.146079848585659 62.031078410277814 0.5439435925292729 
2.3597352723070375 4.237067107993868 62.21727779812702 0.5249779582836386 
2.428302306642667 4.339559954313577 62.20205546681491 0.5065306096138581 
2.4912572768998467 4.431647667531375 62.11707336460767 0.4921335877358946 
2.5397381902606715 4.50106618384584 61.925393274095185 0.4866982468642497 
2.592538026487987 4.5711369397712245 62.34907250001416 0.4725709518970509 
2.6379999690457647 4.638929588485915 62.31520778263564 0.4663694319647508 
2.2308660154777047 4.0539817500235165 59.91759039409092 0.4663694319647508 
epoch: 21, train time every whole data:1101.64s
epoch: 21, total time:17124.82s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.71s
test time on whole data:64.48s
2.694058833895872 4.3244505064275485 72.49286111096347 0.3915238814851898 
2.694988996495537 4.489930871107008 60.426551018344846 0.3468555649021222 
2.691465509166409 4.559630484570827 58.54262270151317 0.35470951417564595 
2.7115001077959757 4.6265014879212565 58.66408713200683 0.35325199303381016 
2.7532397918211564 4.710744218038783 59.47109058453097 0.3324869566968304 
2.7825365635754453 4.759190528487411 60.05856334627927 0.3105194930760731 
2.795456498515393 4.761694027364831 60.372445357990316 0.29126638638170305 
2.8157756556733733 4.788284429706705 60.40212297708484 0.26408083349884176 
2.838213426354474 4.820017366179671 60.695572388981425 0.24472716513120185 
2.8539995930320803 4.830292830533196 61.45508144366359 0.23981405895436933 
2.8752288503384307 4.839860758224153 62.83331809332409 0.2339143527054004 
2.9243112835327962 4.93666315569342 64.9230216381232 0.22991599186982403 
2.785897925849745 4.706880535082154 61.69473837806687 0.22991599186982403 
epoch: 22, train time every whole data:1099.71s
epoch: 22, total time:18302.09s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.29s
test time on whole data:63.89s
2.324655823133442 4.117494639992705 57.87932124036338 0.6284692297659014 
2.5089705572877787 4.431573083638354 61.80538694614314 0.5068590278068011 
2.5389071859489416 4.525178715083839 62.81767808219908 0.485650419912495 
2.577762796296339 4.589624552152445 64.07837924719901 0.46316378440766376 
2.6389820446743673 4.682997476902657 65.41928328017774 0.4225127882239145 
2.684351885434861 4.747395182313465 66.31679247810123 0.381204260903547 
2.7033035426403265 4.7596450301286115 66.16179720981866 0.3500192456178647 
2.7416090409583633 4.7976218360627945 66.14691202094363 0.31163842935218183 
2.784313062346141 4.835817260506971 66.49382912988756 0.28536822079829527 
2.8128062194737473 4.8475394790653805 66.67644495611972 0.278538212126865 
2.8451448781022 4.853792701186792 67.29098241633355 0.2732715244986044 
2.9066559387531488 4.90185989924979 70.15868752330637 0.25578069601224734 
2.672288581254138 4.6792576170057085 65.10395134503415 0.25578069601224734 
epoch: 23, train time every whole data:1100.65s
epoch: 23, total time:19477.73s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 38.96s
test time on whole data:64.27s
2.141338000685065 3.820385561953814 56.817273252990844 0.7226586195429142 
2.366348444844197 4.198158870075326 62.66417494249099 0.5800282323148586 
2.3919253459265013 4.302214635150587 62.75869672841885 0.5455693940658103 
2.440430677744338 4.388360932638446 64.26428435219333 0.5083597703306808 
2.5147066331347006 4.507989571901444 65.70277494033486 0.46025239782760324 
2.5779218734626084 4.602449457576045 66.91192704780879 0.41446432327342275 
2.617915482717699 4.645333843689564 67.49891324878507 0.3794560363236994 
2.672489433233227 4.708769582808667 67.5815637426192 0.34117363020995595 
2.7267624298955893 4.764168756440838 67.68159401410361 0.3148800443877195 
2.767922598963958 4.792032915661526 67.85906493118468 0.3051323443076135 
2.8100783421072575 4.8120199619010515 68.31261468607731 0.29790617161212474 
2.858569231694387 4.847321028296529 69.26520572941332 0.2883405661927805 
2.573867374534127 4.541986857324899 65.61002620343032 0.2883405661927805 
epoch: 24, train time every whole data:1100.75s
epoch: 24, total time:20651.93s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.16s
test time on whole data:64.88s
1.8715325428963239 3.4375759962124306 54.386878888616984 0.7408833539517279 
2.2152239976247685 3.946524423275369 64.68418328729211 0.6107158050392093 
2.2497151060413922 4.065361486510451 64.5289773853338 0.5806382858752092 
2.3276020723050017 4.192440563702241 66.53237359621683 0.5413505032877828 
2.4230622944809674 4.354304037723247 67.87428733725139 0.4927474307313548 
2.5062969165111757 4.486731134602721 68.8733011110676 0.4482227472163654 
2.5677974953322362 4.568952239139679 69.43498517635722 0.4141654735681561 
2.639231473330231 4.666224637131733 69.2641020165682 0.3804766047296048 
2.712008503053958 4.754776787083303 69.25416920110648 0.35661574347692615 
2.772103699225844 4.814649633430504 69.63368845772605 0.34528731256399614 
2.8258454908111266 4.856083858870929 70.37351207002239 0.33798253047025956 
2.868305724040117 4.88865861204724 70.17840767779964 0.33483907071842206 
2.498227109637762 4.439450879721253 67.08511632119148 0.33483907071842206 
epoch: 25, train time every whole data:1101.18s
epoch: 25, total time:21828.17s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.90s
test time on whole data:65.10s
1.7383305263206186 3.18904650838035 56.18286308819103 0.7643289444013877 
2.157653162460242 3.803936368911975 66.99268759115698 0.6279404042482764 
2.1987421007096946 3.9361879261170225 67.0724021274208 0.5943093523095304 
2.2814447458443188 4.080951854363993 68.9049621274883 0.5532966842318919 
2.385999049763062 4.259308312574445 69.80586670907375 0.5044273922159387 
2.4838820553114194 4.4060130256708945 70.21294165994398 0.46087093430295456 
2.552566143436712 4.5035349918466085 70.12442368391089 0.42969848779852027 
2.6333819451905964 4.617871351087802 69.57438953767996 0.39750500473142847 
2.7140600561881882 4.718103023935351 69.3874350252758 0.37405956223316345 
2.762665679437951 4.773974455887668 68.84397889118551 0.3694365807858841 
2.7977881946192964 4.8050180039395265 68.6399438980176 0.37376312881633944 
2.8421380604407083 4.846319523453001 68.29944790287642 0.36785477555001145 
2.462387643310234 4.354836903491552 67.83691428675343 0.36785477555001145 
epoch: 26, train time every whole data:1099.04s
epoch: 26, total time:23002.72s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.49s
test time on whole data:64.36s
1.5403065413439736 2.9391438262008975 49.05483895076961 0.7956471045505394 
2.030654163485304 3.6155977500023995 63.62951230869559 0.6610947233149623 
2.0887885135957704 3.768611548435695 63.76214738705107 0.6220142516806436 
2.1929030205119577 3.9387270222624093 66.32582212383872 0.5741667536302103 
2.287790824679303 4.1167113828769155 67.05215609271455 0.5291579725337263 
2.373070790125057 4.2616544248917 67.27800994223429 0.49005199935822524 
2.447088950911448 4.3688893779646945 67.47535269260658 0.4575961379484677 
2.5186642912829384 4.475892004161096 66.3009818457597 0.4321007031415012 
2.581620607127746 4.56105881085027 65.09925650742507 0.4176428516776482 
2.6287182120788133 4.612489881145289 64.64818361864974 0.4096170206168533 
2.6646118544922874 4.638680513084194 64.28603122563634 0.40950257155017733 
2.6994936227680495 4.677092481642169 62.5823834718813 0.4087752579378812 
2.3378092827002206 4.194518279063133 63.95802890103995 0.4087752579378812 
epoch: 27, train time every whole data:1100.55s
epoch: 27, total time:24177.33s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.46s
test time on whole data:63.91s
1.4275037151301901 2.798812107479985 44.44352477351034 0.807376617009123 
1.9753212677151675 3.5126271385427486 63.276556182213795 0.6692543455083969 
2.0438420604744127 3.664864751721394 65.03838210574669 0.630060903995075 
2.142829765267315 3.829410239092434 67.28818937321867 0.586574097756989 
2.237878918257143 4.000260309793325 67.51565674656855 0.5463386478808645 
2.3261280337611123 4.14119318391347 67.79052227820384 0.5095789057380672 
2.399624265970219 4.251835980464099 68.12760366512944 0.47513022368003405 
2.466846970201869 4.356084349033252 66.83016838236074 0.4474893070308586 
2.5180204857733277 4.427595202839659 64.99095054702147 0.4328414475577001 
2.5442635821873054 4.453867617966729 63.53523271907891 0.43192155001113797 
2.5626182914199753 4.471176119421199 62.16113262920228 0.43829252702593935 
2.6062262446000464 4.549321206576372 60.96346763488836 0.43696615322949484 
2.2709253000631735 4.068178687676766 63.49693130645417 0.43696615322949484 
epoch: 28, train time every whole data:1102.20s
epoch: 28, total time:25355.48s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.52s
test time on whole data:64.09s
1.4327846541783462 2.858673500374037 43.976207526979024 0.8121092060909133 
1.9991716882762987 3.581921469965823 60.99151848866613 0.673875821203279 
2.0667532012948677 3.7280632611180353 61.58622134647089 0.6336673477794861 
2.1537060152473195 3.88466553049504 63.28908563375102 0.5921741466550238 
2.241663160667444 4.047814299355564 63.45588708487958 0.5557431846382148 
2.32202106163429 4.178377801539212 63.38030612922807 0.5255121184349829 
2.3835552789734766 4.277008826669136 63.18021738750924 0.5010023997076765 
2.446846763249664 4.377827861198686 61.94369148525296 0.48100897960851247 
2.4962224810454283 4.448480308463643 60.94435801911347 0.4680955232451713 
2.514333835805101 4.471717560418128 60.495003996246 0.46251877469955543 
2.527127037134376 4.481774195753994 60.36854688274115 0.45866755697253586 
2.552376685722243 4.51615039235933 60.502464688551385 0.43463506012739117 
2.2613801552690713 4.09837599752113 60.34291744917426 0.43463506012739117 
epoch: 29, train time every whole data:1101.01s
epoch: 29, total time:26532.19s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.10s
test time on whole data:63.62s
1.4299619931185708 2.718517991809942 44.00834548499649 0.8116245757003743 
1.9583310547990813 3.421332810285835 63.69052664542208 0.6747052064054774 
2.0247097528435822 3.5829574870584664 66.29453062977461 0.6355722370131365 
2.1126305389678373 3.7446727132280593 68.13084006235785 0.5964845040830493 
2.193087046017693 3.9088237995279775 67.55681952180325 0.5592550076881472 
2.26987942234409 4.038791477083991 67.25460071422123 0.5276920102103472 
2.3350815704961616 4.14298312151662 67.24926071270599 0.4984720680777168 
2.393041756677929 4.238994846997844 65.4496968955352 0.4779247247602567 
2.436166548637496 4.308064541505466 63.4522316879889 0.4663627978610459 
2.455755982435885 4.33575207983636 62.18703165242193 0.46570528329074873 
2.4745936941713804 4.363287790030656 61.02991059094999 0.466681093942439 
2.5136191327352786 4.436561218930488 60.45700337810135 0.44928878705666525 
2.216404874437082 3.966102882369701 63.06350433545752 0.44928878705666525 
epoch: 30, train time every whole data:1101.01s
epoch: 30, total time:27706.69s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.50s
test time on whole data:64.43s
1.3586222989524581 2.688124012121629 41.81832196539519 0.8176728272149228 
1.9405278562925579 3.4133921271664582 62.403750294652426 0.6832661195923563 
2.017807327276805 3.596544853529353 64.29718335913557 0.6437736119338882 
2.115405871384467 3.7841535282262653 65.83281003850303 0.6014128831062285 
2.2031334569373477 3.9669154998194043 65.13502762060119 0.5649937002550379 
2.2849067527398113 4.11284643749593 64.79693463651888 0.5356829269912342 
2.362785582492483 4.234494329070758 65.31836270005961 0.5068816496470936 
2.4307246421509023 4.347462326637909 64.37809264738034 0.4896888797638377 
2.4831730083958026 4.433107857864377 63.38312597991679 0.48221681921561804 
2.5282627500786137 4.495119276456265 63.75845269995547 0.4732801852015002 
2.559422352422029 4.541973309469192 63.88941750883054 0.46843011822642605 
2.6033317056221974 4.610970513471523 64.50953992675792 0.44545907339920066 
2.240675300395456 4.055496453802371 62.46026630410585 0.44545907339920066 
epoch: 31, train time every whole data:1102.39s
epoch: 31, total time:28883.77s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.46s
test time on whole data:63.82s
1.3852108684839415 2.744369821004084 41.990770153427626 0.820914448715218 
1.9449075562215987 3.483428072823566 60.34146724071145 0.6882177144173908 
2.041254301328744 3.67391671246374 61.70599349116256 0.6412440627564137 
2.1534362189206515 3.8691584497737974 64.03828079971812 0.5912080409671739 
2.252559291220758 4.052477388929206 64.32443375657259 0.5508345696700406 
2.336161526868741 4.190625561689752 64.45166729435846 0.5177922954763281 
2.4118060962639394 4.30045580902861 65.26715070053136 0.4893580470299206 
2.4814672531496202 4.403109250355317 64.54909029106967 0.4743208688557611 
2.5409884328563654 4.486095346077957 63.76483916592485 0.4704321554103897 
2.59399627035395 4.55186015204721 64.30587440387202 0.4662135957571148 
2.6423694495564947 4.616534205002107 64.66052717025873 0.4658550384633929 
2.6968081077409995 4.71188385929328 64.82588477915704 0.4594480285633445 
2.29008044774715 4.126719991529923 62.01906341322203 0.4594480285633445 
epoch: 32, train time every whole data:1100.70s
epoch: 32, total time:30061.77s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.42s
test time on whole data:63.81s
1.6193922450737583 2.7224235546397715 53.19894495830485 0.8136693567273007 
2.0491128546838606 3.419686347784631 67.4531299883164 0.668121509338987 
2.057480867989805 3.5682729205205166 68.42056435893453 0.6317827562235445 
2.1178303361287605 3.7189667200389542 68.85611691520862 0.5975999361030846 
2.191016709586428 3.8855599332063244 67.25566311460094 0.562963839133539 
2.265575992163448 4.021129128179359 66.40769981865714 0.5342926584322266 
2.330691229194669 4.141033720577706 66.09035044966747 0.5060259886236091 
2.3985766792908487 4.266841328930943 64.68398251103527 0.4800570327724185 
2.460060839516421 4.368444085508346 63.755822594368226 0.4593179785038373 
2.497058420292146 4.429535447595942 63.22383993226984 0.449462269125025 
2.536636167848749 4.4903989306924945 62.740657879316664 0.44332127458488757 
2.58413009333238 4.576863009194613 63.09751994073024 0.42758765593658904 
2.2589635362584395 4.001101161564202 64.59869341131743 0.42758765593658904 
epoch: 33, train time every whole data:1100.44s
epoch: 33, total time:31239.03s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 37.57s
test time on whole data:62.58s
1.4953358436458346 2.7954853068780348 47.60085602732052 0.8350468585555563 
1.9855326987872166 3.552207283378639 61.323188128268704 0.6949154026044916 
2.0535719446089296 3.7415211643198916 62.753444818760464 0.6476603461662179 
2.142500746253257 3.907802603051447 64.59986364436388 0.6051378481340829 
2.2343958425743593 4.073217221141874 64.75043160023914 0.5690943007577788 
2.3094037801857508 4.191574358985814 64.60703758580757 0.5437196778588352 
2.3688111105978136 4.280573590597284 64.90121892481102 0.5236358091872032 
2.4326386296221365 4.382110237845357 64.63171185020495 0.5075846936028107 
2.49016284101687 4.466986502648607 64.51456766262532 0.49859894026298607 
2.53386980777429 4.527969937614935 64.97998262702433 0.4979057308103612 
2.5863683094992522 4.600014412409052 65.59207040206199 0.49481677261831203 
2.6279617433014018 4.6739321920795645 65.85994417284124 0.47473356988907345 
2.271712774822259 4.131703300219219 63.00971227101291 0.47473356988907345 
epoch: 34, train time every whole data:1108.37s
epoch: 34, total time:32421.85s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.48s
test time on whole data:64.45s
1.3004346698768259 2.6457432949273096 39.0098749016667 0.8270486510977479 
1.9117824484181722 3.424645794727508 59.762652723986 0.6894432728684466 
1.9941349369061312 3.6175464178556176 61.539611810365905 0.6490822920860183 
2.0808252417626125 3.7783589912709385 62.910230215580064 0.6125261983360089 
2.156420301131904 3.9225597543498583 62.38096272164606 0.5849679060232109 
2.2169363733001406 4.019481309014367 61.89170766449307 0.5676660090992609 
2.2593392990023076 4.088130821415605 61.815532281858765 0.55685573036502 
2.2994987821853052 4.16231789527273 60.70714316039444 0.5524396011297668 
2.333158965310614 4.217332019688114 59.80879992798047 0.5514309171409295 
2.3550027744757633 4.25365082175378 59.60389709025711 0.5496323505291596 
2.4007893439368124 4.326017181806163 59.74929493860881 0.541846485335671 
2.4606987199002788 4.431098140114421 61.00966236914716 0.5093234082714249 
2.147418488017239 3.9359422221245546 59.182602042577415 0.5093234082714249 
epoch: 35, train time every whole data:1106.01s
epoch: 35, total time:33603.99s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 39.22s
test time on whole data:64.86s
1.268428039957104 2.573531737817598 38.43191969029648 0.8340837909583958 
1.891308460897633 3.3838764303955244 61.11617354904647 0.6944837385414864 
1.9802655722240785 3.5896667394434263 63.85939453546652 0.6531040526560655 
2.0775930255932646 3.7744836350497084 65.54508056474468 0.612859794658782 
2.164855102804711 3.9504674879107124 64.9428513183574 0.579757479046905 
2.24435643028122 4.096601562220715 64.45795719057568 0.5537642642436458 
2.3167831289935856 4.222139950428045 64.4812452455334 0.5333462244360917 
2.3917432719875302 4.352579796273077 63.56544369121247 0.5201534506733354 
2.462492759657669 4.461858614683985 63.277779236445674 0.5113283447301453 
2.5259469169611557 4.552060198999611 64.1962888058082 0.500863124356115 
2.5898119227863137 4.639276184540626 65.0795564997353 0.49066749687548356 
2.655282465702455 4.729303255696395 67.03524810442003 0.46540568802057874 
2.214072258153893 4.071081969650407 62.165978600390204 0.46540568802057874 
epoch: 36, train time every whole data:1107.62s
epoch: 36, total time:34786.42s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.41s
test time on whole data:64.34s
2.069777614407596 3.6291220716602925 52.88739847271589 0.6480925333882598 
2.3044495321923777 4.273455287949533 58.28790231080492 0.45282853571319065 
2.2569414407628634 4.267766413035821 61.87434231062441 0.46011900703302505 
2.306373627943563 4.339107752857764 64.20132196593484 0.4410159304038814 
2.383742263426738 4.456832793231618 64.88489667140497 0.4039499416982662 
2.4566117291253593 4.552165541608213 64.933546626372 0.36679620067353985 
2.510143170945701 4.619923369266919 64.98507376423541 0.3344525368480722 
2.5680692175423636 4.702402624218961 65.00627641919668 0.2976674549381153 
2.6201666109608928 4.762121382865738 65.35949772887882 0.27169026323802875 
2.6618338607457424 4.7925780517388725 65.93620106171223 0.25995922878835925 
2.7131771753597116 4.837860828311618 66.3927137901128 0.24806644402824438 
2.7531983909975026 4.872359605828573 64.91650797467759 0.2299131042848567 
2.4670403862008676 4.521318717745509 63.305660464562806 0.2299131042848567 
epoch: 37, train time every whole data:1108.23s
epoch: 37, total time:35970.26s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.94s
test time on whole data:64.98s
1.5952750130650544 3.0926254708163126 42.46999656682754 0.7728648167347271 
2.046377753111401 3.780042555183797 60.25772973734976 0.6168426903130118 
2.0777775313460403 3.8603488880359618 65.47173020598956 0.5769615963904994 
2.1641922929823574 3.9718317335481705 68.51374372027232 0.5407920405176224 
2.2503382936536913 4.1000417052707885 68.61141364987743 0.505439738928524 
2.3302535536807207 4.221778055305568 68.0732289380199 0.46945669215046343 
2.3991478927620644 4.334696343929818 67.81917217612428 0.43587965163168974 
2.4595573848311214 4.450679784622511 66.51094392989556 0.40608343706405636 
2.504306928101395 4.522639367396326 64.79824988331063 0.3904509040713606 
2.5288194884107047 4.551126594753922 63.09444636323025 0.3868286709249653 
2.5630395018122205 4.590605281939506 62.06788901789365 0.3852985158309548 
2.6289164721949825 4.676331176406158 61.62758004644163 0.37153060555635875 
2.295666842162646 4.2018987772024685 63.27651970789068 0.37153060555635875 
epoch: 38, train time every whole data:1105.10s
epoch: 38, total time:37150.98s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 39.25s
test time on whole data:65.30s
1.4619259038016732 2.89351200425862 41.91665298132739 0.8156835355551915 
2.0061584516824 3.660491504403999 61.13223692280966 0.6676602287423653 
2.0722856116973394 3.802887471860668 64.74431668255018 0.6228377213787157 
2.173198547289308 3.9587358005842628 67.23604541944312 0.5833952039291471 
2.2744116273852333 4.139762388888863 67.22591702237088 0.545983391216961 
2.3694693992435045 4.298139185717191 66.8455389568623 0.5154740753467175 
2.4621185131373684 4.441296129974369 67.057856102408 0.48855427732046813 
2.553809396437236 4.586177167532037 67.22222353302834 0.4689819807538793 
2.6289683223511315 4.695484271266725 67.68558478268851 0.4537757338210101 
2.6844420861430645 4.757882742225123 68.64120508538949 0.44328575010819676 
2.7345929681057375 4.80519115617515 69.69438275589745 0.44085460923275555 
2.792515712801633 4.8713964204416005 70.66072302731789 0.4320068390633503 
2.3511580450063025 4.279605738464611 65.00552439821809 0.4320068390633503 
epoch: 39, train time every whole data:1108.95s
epoch: 39, total time:38335.78s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.05s
test time on whole data:63.70s
1.360091646288566 2.7279377746801945 40.352204552599154 0.8241845387819097 
1.944482201982467 3.5035705863468256 61.29863077056748 0.6733558443947986 
2.018329000064837 3.662694056358114 63.9343125040139 0.6320013909018737 
2.1057376849297436 3.818690383554232 64.85435963470817 0.5973713520237389 
2.194756895737013 3.985693752933685 63.49797029693277 0.5694894032740816 
2.276810136337454 4.123871685987456 62.31083973658548 0.550335837665464 
2.340636294575053 4.23772195307477 61.847913659997765 0.5338543448560243 
2.4057146103392753 4.360047172505023 61.26316663724252 0.5175317687541531 
2.4494800846323903 4.433808875301291 60.95765361327995 0.5089856358804852 
2.467287882525651 4.4584930551371285 60.831375492023675 0.5087674580005658 
2.5115287743981574 4.510745374296983 61.54844079901983 0.5054847362822031 
2.5787037093530274 4.605278434410085 63.24972327260227 0.48960926162441903 
2.2211299100969697 4.068855092654727 60.49568895073564 0.48960926162441903 
epoch: 40, train time every whole data:1108.30s
epoch: 40, total time:39520.17s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.50s
test time on whole data:64.18s
1.568849382959927 2.856108098775339 49.79736092875575 0.8348977587479168 
2.053306538275842 3.6365159273444734 64.15489889600836 0.6881507157368987 
2.1343703686476108 3.818034897126078 66.41755783620017 0.642431072885256 
2.2331022741014936 3.9905718973720155 68.42723547998811 0.6022130102092139 
2.325475645138749 4.16433023957247 68.59001977456901 0.5696038023389237 
2.400753075739369 4.293463919265909 68.3497743181168 0.5460145210525158 
2.449681505934291 4.379559881903562 68.09723814413631 0.5309061774856347 
2.502430823855989 4.474005807073673 67.93495822030276 0.5162375372559161 
2.5420179690435707 4.535529438747569 68.17574977588053 0.5084413161004175 
2.563295496826725 4.559910124895137 68.24201797936963 0.5128720819777133 
2.6138691370072995 4.615643767496814 68.99873509947413 0.5127479626183384 
2.662437369787445 4.690662425280794 69.436698443577 0.4997134094274714 
2.337465798943193 4.198478857573421 66.38538440789378 0.4997134094274714 
epoch: 41, train time every whole data:1105.82s
epoch: 41, total time:40699.84s
predicting testing set batch 1 / 168, time: 0.38s
predicting testing set batch 101 / 168, time: 38.66s
test time on whole data:64.41s
1.3135996419890297 2.671925082349397 39.719997314906244 0.8336318150902294 
1.9144846555667796 3.461560819657447 60.80980500461271 0.6894998680275979 
2.0006916852404495 3.6520216089635484 63.901717388139446 0.6437091411660988 
2.096939937847977 3.8246881804168558 65.81702979733967 0.6051485947816624 
2.1890758861802695 3.998586797842926 65.59582090579414 0.5733790126755008 
2.2690343223473146 4.130872189279898 65.28938536020678 0.5501427372504399 
2.322624276221774 4.224451907531139 65.05714598024741 0.5343632213276217 
2.3775697573535144 4.3207730908392445 64.19223885979214 0.5235919011009021 
2.4225026438940493 4.388582092887634 63.793398878597294 0.5198855708773583 
2.450441397309392 4.4274741197336525 63.620826437851086 0.5247997602202487 
2.5109143703271237 4.503454101934741 64.2776784966714 0.5238428495872205 
2.5721676246837846 4.600635648620852 65.09044577660981 0.5089660868521334 
2.2033371832467883 4.051379302120095 62.26400918020162 0.5089660868521334 
epoch: 42, train time every whole data:1107.62s
epoch: 42, total time:41884.10s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 39.43s
test time on whole data:65.01s
1.3423229588611671 2.6177224261860808 40.71103633950081 0.8297753472803608 
1.930937152086979 3.381860919069075 61.177570950613905 0.690583818193551 
2.008887215116017 3.571425013779173 63.111171507649956 0.6465859940332129 
2.0922267862684314 3.737860568743341 63.80314807986647 0.6092937438364809 
2.1653014243238915 3.8975023356278555 62.487828748883004 0.5813114325981389 
2.2287226470802866 4.018091910080585 61.42586641860057 0.5637824370319321 
2.2832141791796756 4.118119791617135 61.169210899309 0.5485363252897166 
2.3314636556829016 4.21660716579167 59.49264294219761 0.5419194167807965 
2.368910564157136 4.284444837578857 58.2324199698195 0.5402364315765323 
2.404049272921912 4.337056077710021 58.527161871710774 0.5335035525039405 
2.458496551070451 4.418057752786625 58.89593537146452 0.5251924012081802 
2.5210571014246947 4.509674528742084 60.41242674100622 0.4987418199025476 
2.1779657923477953 3.9593068369485747 59.12062097997658 0.4987418199025476 
epoch: 43, train time every whole data:1109.00s
epoch: 43, total time:43069.29s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.96s
test time on whole data:65.02s
1.3404688376864153 2.615885910428116 41.616466460978664 0.8264843321157319 
1.9627781236494581 3.424882159741796 63.97045179575757 0.6726492626920092 
2.0243795936371956 3.6125003512050458 65.89950841773867 0.6298509195677383 
2.1000810290223786 3.7747379180657985 66.70297117030812 0.5962890194571804 
2.171337325277605 3.9346787674711914 65.50334780392984 0.5707524456515968 
2.2309852418410814 4.042109516888627 64.94738719938724 0.5603214421263215 
2.2694190525723 4.111985512843905 64.46744548184039 0.5599785638382795 
2.302675098372712 4.189480247237187 63.205458951904134 0.5603572105645969 
2.3264040601495655 4.236291822231038 62.477787427911366 0.5585395834509986 
2.3396570519502498 4.256780774962454 62.14488272877545 0.5590118067496653 
2.392903317814072 4.335835077320152 62.73951363857717 0.5507535446782199 
2.4529139376288014 4.433186032460794 63.53893275272113 0.5272380232208759 
2.1595002224668196 3.944098473002522 62.26797252289197 0.5272380232208759 
epoch: 44, train time every whole data:1106.52s
epoch: 44, total time:44250.30s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 37.78s
test time on whole data:63.09s
1.2830460796595684 2.6246602099273337 38.90425832455061 0.8355930626201797 
1.8939126957876697 3.4037205062695803 59.4804963231227 0.6988612107819352 
1.9746880046072461 3.5798075386140837 62.73508209822091 0.6525050448911308 
2.0592074987635547 3.7110412771935493 65.78845913550803 0.6141689497601511 
2.12873586115924 3.817803818904719 66.95797952832352 0.5843326991640995 
2.183014170190497 3.8759592328588792 68.52071271047457 0.5636491949968069 
2.22422022071942 3.9243820364962656 70.33611880445892 0.5447500684641308 
2.27041349272767 3.9964215968034704 70.3170056329455 0.5226431256696054 
2.3086616892155614 4.042983544317223 69.88302277753652 0.5084696800220971 
2.3301106473649185 4.0594328436602325 68.91695145852854 0.5047699459588005 
2.3633981405550704 4.122959383472898 66.24402886394051 0.49658636666076705 
2.412408792623186 4.251857447419484 63.298249446871345 0.47900295034833273 
2.1193181077811336 3.8071604311468006 64.2822174479812 0.47900295034833273 
epoch: 45, train time every whole data:1107.32s
epoch: 45, total time:45435.10s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.61s
test time on whole data:64.09s
1.2436371208978374 2.5502441985476403 37.57577197475483 0.8386910430777599 
1.8969091335448125 3.3732482487991997 60.06753885124147 0.6961273072849107 
1.9721801880428655 3.566851581163845 62.53533095730008 0.6538751339331682 
2.0618484798367356 3.730432681455253 64.46051057740195 0.6161074290957733 
2.1319587090343592 3.8712497961881547 64.10387282054839 0.5886183978035073 
2.183022920913994 3.956871775212314 63.852503503377825 0.5709447581418489 
2.2053435836249875 4.006241226044203 63.514645212137054 0.5614251853515908 
2.226563508580749 4.05336387516082 61.98721738425973 0.5557255206142209 
2.238389133074986 4.069095723415813 60.904492090160254 0.5552771710207537 
2.235470127972641 4.059150009186878 60.27053141377105 0.5618206723572372 
2.2687772346534545 4.108345550375053 59.64511614038431 0.5601301516345754 
2.3225982221241686 4.209967829028646 59.37891294267753 0.5469734364004935 
2.082224863525133 3.8218757959387806 59.8582010198981 0.5469734364004935 
epoch: 46, train time every whole data:1108.40s
epoch: 46, total time:46617.38s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.56s
test time on whole data:64.80s
1.2592078812045178 2.5867008614583478 37.41995118080885 0.8419597965273496 
1.8918611520809077 3.406690317431053 58.92291092886182 0.7020590522602457 
1.9724363871729444 3.601806603832309 61.16625269361566 0.6585214433592633 
2.0528174799608867 3.7551614167182037 62.574392162197476 0.6242138935690468 
2.1154053861316116 3.887618990759472 62.00827776011118 0.6059475198280416 
2.1638226130975498 3.9767785850604978 61.65341915250541 0.6004943948463934 
2.2156405264658057 4.059732131910751 62.08178044509422 0.5951655502249329 
2.267943355656362 4.152585792054312 61.99200221798353 0.5883466497521024 
2.3064124912064345 4.211799952797113 62.19595240544311 0.583611861219243 
2.344481126894021 4.259974460888128 63.06300050554029 0.5768693976566733 
2.401816343611195 4.34684717850131 63.64759832101612 0.5689392670066025 
2.467502959026822 4.445951798007604 64.82815166229204 0.5444637659980311 
2.1216123085424217 3.921869881945868 60.12972343427607 0.5444637659980311 
epoch: 47, train time every whole data:1106.14s
epoch: 47, total time:47801.41s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 37.93s
test time on whole data:63.31s
1.2403042640272706 2.5323542241745454 37.74706229312552 0.8421054302768454 
1.8844378170490796 3.3679714923526745 59.41549761649429 0.7037108342374269 
1.9715319681228805 3.5715354742492016 61.29453159856395 0.6595019341455031 
2.055235731083456 3.7294143233052566 62.7433758176859 0.622833191967999 
2.114627277390499 3.8515074520730024 62.12012881492925 0.603324535040267 
2.157632777321108 3.9275429112709968 61.646631018854215 0.596621233195737 
2.2032506019199003 4.00339859937161 62.02523629206198 0.5893806956791275 
2.2536129305126766 4.091867307069539 62.01482226801102 0.5799332694078434 
2.289536625814491 4.144665535086976 62.27078383447584 0.5739919946101585 
2.3237653457773404 4.184736854398297 63.16621740661642 0.5673531035768802 
2.3728470321813866 4.261401342141078 63.22652331338395 0.5604664801519406 
2.427726087997978 4.355268532077044 63.652649595215195 0.5426657358103927 
2.1078757049331722 3.865110050499353 60.11052381237174 0.5426657358103927 
epoch: 48, train time every whole data:1106.52s
epoch: 48, total time:48983.40s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.63s
test time on whole data:64.64s
1.2699350278693295 2.4985385984384707 39.53283752364037 0.8438966524156719 
1.8875500231006492 3.3328979708111603 61.590556155035806 0.701406102685177 
1.9651195354040358 3.5428594645523086 63.93333168650338 0.6539228141723193 
2.0487971898390067 3.6985525680448217 65.55970799585798 0.6174517609808726 
2.1010200903927463 3.811850165144829 65.00241270651375 0.5954900219959791 
2.1317250153087968 3.8570952297851093 64.4436415674815 0.5890670027974504 
2.1357643983731314 3.8661289492653927 63.52059768765481 0.5918574847402028 
2.1402308518854634 3.8804324677452295 61.68363913773685 0.5969041241368273 
2.149713388859516 3.8889515794670753 60.63828718050952 0.6003808922350415 
2.1576757815565384 3.9025528404942085 59.972415914413205 0.6032033869730679 
2.216826398531241 4.001421126686063 59.76837145646804 0.5935216118728167 
2.2933919092826547 4.148187164867948 60.544095276689156 0.5682539260213122 
2.041479134200259 3.72566362798027 60.515947337112294 0.5682539260213122 
epoch: 49, train time every whole data:1106.86s
epoch: 49, total time:50164.39s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.85s
test time on whole data:64.79s
1.3807884696366355 2.5633008535495434 42.568211068217906 0.8362325643206721 
1.9709310738125905 3.3862170393588884 62.47022341153566 0.686081946003351 
2.0067579647327465 3.5973625657141204 63.560566239386276 0.6418185635237239 
2.0835762753597504 3.7702885498414713 65.01484330948986 0.61110180705369 
2.167576318489033 3.940187529619829 65.27004469739607 0.5869582877354987 
2.2376278313431177 4.052366517528784 65.23064365989299 0.5757126921670129 
2.286801733394109 4.142933446945142 65.0199349264452 0.5677979307862292 
2.3509431753493844 4.2552392661479725 65.06551744982362 0.5510591892678449 
2.4118474236280614 4.352862618098228 65.59360346756236 0.5328784812163899 
2.462433393487263 4.438816719949347 66.05167631137473 0.5190127971190259 
2.5444813828321973 4.554228612034138 67.10179884470266 0.49715252111624336 
2.6139011597082553 4.649584804450868 68.86334674168837 0.4725029887902422 
2.209805516814429 4.014774019693316 63.484444829212485 0.4725029887902422 
epoch: 50, train time every whole data:1107.33s
epoch: 50, total time:51345.41s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.00s
test time on whole data:63.17s
1.2353468076171619 2.5160747269468646 36.90142333746873 0.8492865480020264 
1.8902096238875141 3.3864003542146746 58.140062335762025 0.7096647520405645 
1.9874038205628417 3.616374343407762 60.2350272566878 0.6624657952191126 
2.0748795110234726 3.7840822142115123 61.776810937521965 0.6266082546924852 
2.1448820994827185 3.9192203154006306 61.72591987363908 0.6073194238889941 
2.200171248448303 4.0159136811401055 61.77189028352568 0.5957308171721416 
2.2594187578217437 4.106925509590335 62.61452817311154 0.581596457541741 
2.310338494550259 4.189561871149173 63.03338684002404 0.5695991194303469 
2.3470740222899864 4.240291817934161 63.48475345157312 0.5632114244913236 
2.387949388627229 4.283836302563522 64.60067456311526 0.5581637850753647 
2.439890164834048 4.3592935263390045 65.06874065769826 0.5541051091785282 
2.497340775857103 4.4493642933944475 66.01459338247028 0.5404250046523099 
2.1479087262501984 3.939485036220038 60.447620476157645 0.5404250046523099 
epoch: 51, train time every whole data:1106.78s
epoch: 51, total time:52525.56s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.93s
test time on whole data:64.72s
1.244183573782444 2.4639158706997892 37.915622582082605 0.8495307526726761 
1.8830212100732717 3.306029840781048 59.638448473662976 0.7085969277405544 
1.9557131161334969 3.5410850636457307 61.16963558429988 0.6608945899226546 
2.031426118297857 3.706798021468709 61.98624664264294 0.6283324820091978 
2.090522416971003 3.8356564756712754 61.30708951443488 0.6095487346308845 
2.13017766226766 3.9065410699231577 60.68908719774638 0.6052570928296737 
2.1589056585481656 3.9602410606117484 60.378966011584524 0.604010136548577 
2.1952205070517072 4.0195657573703905 59.85575371457368 0.6000570740784503 
2.218527264240863 4.056454411571523 59.540188258354846 0.5970262109183372 
2.2375355538198103 4.088973293116865 59.65399026289211 0.5960103231550511 
2.2955127653990473 4.177240337502452 59.84171702731127 0.5894531044626338 
2.3538602082862385 4.276394784663666 60.75062433913906 0.5696874453560761 
2.066217171239297 3.8079467976941848 58.56077567122557 0.5696874453560761 
epoch: 52, train time every whole data:1106.19s
epoch: 52, total time:53704.86s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.86s
test time on whole data:64.42s
1.2174602662022447 2.435502273773855 37.569523675289865 0.8519129159034824 
1.869130928194239 3.26657803412111 60.97708502573318 0.7130627283709808 
1.9502103103352266 3.5132666412716276 62.52684160868566 0.666152901009787 
2.03291438000596 3.698690984557084 63.53900222503069 0.6322512327818729 
2.0974026646802115 3.844117380937435 63.02814055052267 0.6099847795962031 
2.147399031566988 3.9316297371460593 62.63933850647484 0.5987983672186349 
2.1874825605640984 3.986749435889473 62.904251212605224 0.5915813198873574 
2.1981138805441027 4.002750076240794 62.20785161492811 0.5944123617853039 
2.188586286731685 3.983845908418301 61.54460568283013 0.6012902404826922 
2.1918035143118884 3.9725081944768728 61.82737454330128 0.6033777070356652 
2.216408615896567 4.018001132282855 61.257771689898746 0.6010215231490328 
2.254549554231976 4.0931122354947425 61.505860809878385 0.5863967070408472 
2.0459551661054323 3.756399443892531 60.12749212310051 0.5863967070408472 
epoch: 53, train time every whole data:1107.48s
epoch: 53, total time:54887.02s
predicting testing set batch 1 / 168, time: 0.38s
predicting testing set batch 101 / 168, time: 38.04s
test time on whole data:63.14s
1.3641415673767527 2.5782454267888557 41.435688985414416 0.833321484495968 
2.2288605028542556 3.857529281361574 60.500865685267115 0.6209324532683802 
2.3105748815625198 4.145498321718616 60.720667898592474 0.5603284094051606 
2.329369368565579 4.174645865891857 61.946511069029974 0.5261121738837586 
2.3760394877771005 4.249905581667295 62.87850496761907 0.4845778690523283 
2.4278157234740934 4.310318552482369 64.63738648665561 0.4511344444253428 
2.4736762200061766 4.362760850684275 65.43353457228011 0.4310795266061252 
2.5304656167395767 4.448130471267623 64.48832092347587 0.4154969475086566 
2.587339357071867 4.527296391034353 63.684401945143286 0.40808140664785314 
2.637391245990193 4.586458270126901 63.56100730783576 0.39842296944889183 
2.6876519943589257 4.643145202723305 63.37715361084631 0.38682965238362355 
2.7363044057303063 4.71985077546733 62.46030562715191 0.3944209318438699 
2.3908025309589456 4.252142570488074 61.26059716817418 0.3944209318438699 
epoch: 54, train time every whole data:1106.56s
epoch: 54, total time:56066.84s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.78s
test time on whole data:64.97s
1.3116048687574056 2.559019981422475 41.4851110387032 0.8414492156823549 
1.9300280392289695 3.401000171207767 60.33680869018889 0.697519324770372 
1.995032316944961 3.608899605734542 62.506874293023444 0.6533042574841811 
2.063776568383599 3.751193377146607 64.59778979315323 0.6199006370786888 
2.1131417387174 3.8603869126610126 64.65193308333453 0.5989426676591356 
2.1488653017442143 3.912314114807438 64.10391315744654 0.5940918374136847 
2.1696805577206293 3.950701705201378 63.084619142015555 0.5950839213853626 
2.20402435191907 4.014499180442942 62.634953120948055 0.5877146891530671 
2.24232066217056 4.072654917562176 62.88546972952352 0.5767788053312644 
2.2693255097120115 4.115739516433468 63.067416169076374 0.5728045848709751 
2.3281198773217344 4.206492737809541 62.89521330220866 0.5652669024313418 
2.4169455043762214 4.3615093247351275 63.526472906265894 0.544258279548386 
2.099405441416398 3.844771391298726 61.31490407821742 0.544258279548386 
epoch: 55, train time every whole data:1105.52s
epoch: 55, total time:57246.60s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.77s
test time on whole data:64.92s
1.2616363440360874 2.5131565338871624 38.50201023403918 0.8435091594096903 
1.8953860329467627 3.2888816026264744 58.61791468049676 0.7132773969069847 
1.9835905120448165 3.498580508715902 60.06635488652373 0.665799421834041 
2.0628539458059127 3.63154778993061 61.383839172490894 0.6300623189533447 
2.1041060311769444 3.698441339041941 61.476614342885085 0.6138147382732289 
2.1300441971001702 3.7175064770645965 60.86040004977489 0.6111313287526019 
2.1439305555360124 3.7254679101434998 60.480132437316826 0.6103790503677257 
2.1438657002071184 3.7190333694902034 59.26452035644309 0.6156962268586624 
2.1537812216318257 3.7172895751483894 59.08637482208169 0.616129494981822 
2.1771916277219674 3.734553450406734 60.16298612006332 0.6095758526685693 
2.2125890885166646 3.796780559463677 59.68655925426638 0.6002402680809397 
2.237184009839914 3.8829281906206603 59.30069183454513 0.5920902405020979 
2.04217993888035 3.5942935234707223 58.24086519590428 0.5920902405020979 
epoch: 56, train time every whole data:1107.34s
epoch: 56, total time:58428.43s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.27s
test time on whole data:64.26s
1.3595576842283563 2.4784054949451675 43.86839518342301 0.8427047723692855 
1.9390854288607482 3.2481218701294727 63.65200842004049 0.7086144694548819 
1.9963142105234521 3.461869217194431 64.51449659400913 0.6619705717895171 
2.0545912032146005 3.608192233756034 64.56614796835382 0.6291246450067806 
2.088572211482518 3.704068242436566 63.55661475415489 0.6106840616685416 
2.108513360914109 3.7466353412921003 62.546775017540924 0.6062619639494393 
2.1242282152151066 3.7730979828434648 62.16570392663735 0.6045914646653426 
2.1192194334786563 3.7790920577733056 60.424192377879585 0.6102142191557872 
2.1181015525389286 3.7816385475952092 59.149582549765555 0.6133463591974264 
2.1388008916245864 3.801967323428849 59.47151692227075 0.6104426448769371 
2.177114279945514 3.8721081920374476 58.86765043838887 0.6033700795812381 
2.233536849788878 3.9784490397492744 59.409173733140804 0.5835745787630235 
2.0381362768179545 3.6234031712097705 60.18273085810025 0.5835745787630235 
epoch: 57, train time every whole data:1105.89s
epoch: 57, total time:59608.77s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 39.15s
test time on whole data:64.67s
1.3050842212397782 2.4446168488080358 41.64704050838265 0.8483342272337756 
1.9022444536691265 3.220732106392967 62.66613139651995 0.7167590191047052 
1.9736715916706515 3.4469337685759927 64.39195462810919 0.6681605914342159 
2.0362097867021247 3.5952186904511856 64.97750345034468 0.635846634749375 
2.076381310888167 3.6964743440686165 64.40957155082211 0.6163150285511039 
2.1060408400897646 3.755468751246107 63.860195853903775 0.6073801763319107 
2.1336970035223555 3.8068794214582957 63.80991126755988 0.5986204700352129 
2.1409232444297523 3.838831917263756 62.471651327821164 0.5960753851499384 
2.14750515283591 3.85852765375082 61.38843215928665 0.5946249151006721 
2.17269988172457 3.8874159051590227 61.55143377209367 0.5907204538268449 
2.2083966120836283 3.9535172103245406 60.57224985771982 0.5860247063343286 
2.2581112708097235 4.050180708980643 60.73484990714266 0.56772111746361 
2.0384137808054628 3.6536185988960446 61.040196021045844 0.56772111746361 
epoch: 58, train time every whole data:1107.03s
epoch: 58, total time:60791.30s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.25s
test time on whole data:64.16s
1.3867087564300746 2.558468974922422 44.73398084282614 0.8616956020582721 
1.9287260825635777 3.4032990903330704 59.23977316915222 0.7259320624217653 
2.0028742746741823 3.6416820210942293 60.025344814971746 0.6749992648691902 
2.0679518034640876 3.7840807661888927 61.196104394944975 0.6420874283100881 
2.1088120372815147 3.87065913525641 61.39813279382187 0.6304951295959885 
2.150175469606849 3.927594370652003 61.44529130709034 0.6311309451087259 
2.1926956648416818 3.9866129634098177 61.83998408821913 0.6317248766188033 
2.2324311517477033 4.039619913769487 62.27607146810442 0.6293368576718965 
2.2605967573288472 4.075723964478504 62.60699144831737 0.6266803287575113 
2.286552322561897 4.1121119610327534 63.184320298353136 0.6251412852014654 
2.3409697271024896 4.196456980839199 63.499997317528226 0.6176979312932821 
2.3868660528054018 4.28689161263012 63.945745183158664 0.6021487376920298 
2.112113341700692 3.8496495136607454 60.449513950365365 0.6021487376920298 
epoch: 59, train time every whole data:1106.37s
epoch: 59, total time:61971.04s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 38.86s
test time on whole data:64.46s
1.2555587335027576 2.4287933056814737 38.50574911571726 0.855130594509717 
1.8796105735957445 3.246567375535656 60.231544448269204 0.7209028031165523 
1.9426039843055465 3.4877409929027756 62.15073399180909 0.672679960343418 
2.018668869029819 3.664112995340665 63.218537286939444 0.6385891489661056 
2.0833157667912365 3.805987316012428 62.904971209570185 0.6168662524081674 
2.14350926272917 3.918961943873236 62.64158296982835 0.6028681570921616 
2.203168250303272 4.024046461210631 62.98135535649062 0.5877393773385476 
2.2533740787934513 4.115704978904761 62.88355341636063 0.574097607932825 
2.3037432845819388 4.194924513693861 63.01644955125535 0.5638325719154856 
2.365831458228951 4.281497697705399 63.88663788153374 0.5561023773891004 
2.4460377913410998 4.39815772038319 64.32615263316865 0.547506964040413 
2.5063121340992374 4.484050572252881 65.42910461266898 0.5330608077483231 
2.1168111822751854 3.877114396937559 61.01494038403247 0.5330608077483231 
