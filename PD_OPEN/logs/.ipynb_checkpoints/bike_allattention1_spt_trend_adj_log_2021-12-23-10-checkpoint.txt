total training epoch, fine tune epoch: 30 , 40
batch_size: 4
folder_dir: MAE_ASTGNN_h1d0w0_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1_spt_trend_adj
load file: data/bike.npz
ori length: 3001 , percent: 1.0 , scale: 3001
train: torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12])
val: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
test: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
TemporalPositionalEncoding max_len: 12
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (src_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
  (prediction_generator2): Linear(in_features=64, out_features=1, bias=True)
)
delete the old one and create params directory experiments/bike/MAE_ASTGNN_h1d0w0_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1_spt_trend_adj
Net's state_dict:
encoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
encoder.norm2.weight 	 torch.Size([64])
encoder.norm2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
decoder.norm2.weight 	 torch.Size([64])
decoder.norm2.bias 	 torch.Size([64])
src_embed.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed.1.embedding.weight 	 torch.Size([250, 64])
src_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed2.1.embedding.weight 	 torch.Size([250, 64])
trg_embed.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.1.embedding.weight 	 torch.Size([250, 64])
trg_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed2.1.embedding.weight 	 torch.Size([250, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
prediction_generator2.weight 	 torch.Size([1, 64])
prediction_generator2.bias 	 torch.Size([1])
Net's total params: 1140994
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411]}]
predicting testing set batch 1 / 168, time: 0.65s
predicting testing set batch 101 / 168, time: 33.78s
test time on whole data:56.16s
64.40440951206322 82.14988957134732 2608.5856014778315 0.04070020501179843 
62.60464344895099 81.335636341837 2529.0056515845936 0.034606346574931404 
62.84609168721141 81.09494363557243 2540.491543534291 0.027767365648878464 
64.32255633213309 81.7029518471734 2605.3741395799493 0.021581889317455607 
65.59002394976706 82.05826115084446 2662.351930345851 0.017275475831976063 
64.20850282367275 80.0823401007066 2610.874155785655 0.015189327963949457 
60.45189047036418 76.24082671201073 2461.4476347626705 0.014182414134629904 
56.76542919045359 72.78447419068044 2313.3256436689326 0.012987567752792849 
55.80314862431586 72.0729021000804 2276.6875933212395 0.010570549588656298 
57.644108577596825 74.13589426286097 2355.5688872785913 0.007380320386003577 
60.33002368771294 76.97948320207682 2467.901652816993 0.004304389239583888 
61.372644396603285 78.11728286449667 2510.6383894590285 0.0018559722066294692 
61.36195605840376 78.31114215690985 2495.182673054068 0.0018559722066294692 
epoch: 0, train time every whole data:204.32s
epoch: 0, total time:272.71s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.45s
test time on whole data:57.33s
3.860199898492634 4.643086449484271 150.20466758156056 0.6146585885684245 
3.885199756625951 4.68442325840962 151.4128983014204 0.5738181351985149 
3.9050729737044976 4.727567461913177 152.06100750280922 0.5244665403731147 
3.8924234540854536 4.739050090385063 150.9464230252213 0.48650095669344334 
3.895809543114333 4.768904493475854 150.41205135248276 0.4385018970915907 
3.9400804852263205 4.83154155654411 152.01536892565838 0.3883380535755437 
3.963382100437901 4.879280833563612 152.38463532149467 0.33907713234613535 
3.980996905838982 4.917813113164257 152.1835800923631 0.2983373781154774 
4.034409409538976 4.98172553148373 153.84045499234577 0.26068243865707136 
4.039721740093171 5.001747445235215 152.69726486192576 0.23363890322935235 
4.056121645242685 5.0290833439748175 151.98555267289842 0.2088535472491177 
4.086310476711197 5.071370262571431 151.80509376283135 0.1813530019927972 
3.9616440324260083 4.858280015039817 151.829129279782 0.1813530019927972 
epoch: 1, train time every whole data:205.15s
epoch: 1, total time:548.44s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.69s
test time on whole data:57.73s
2.6404046115503603 4.460343436221181 72.15597007546997 0.5855144284831555 
2.642831844556101 4.491939663558729 71.81538768015021 0.5659040751514596 
2.6713708609323064 4.564730854708028 71.84150885693154 0.5335318938138276 
2.704287272066499 4.623290546232994 72.41859596052804 0.49527171003750925 
2.7357610229432936 4.680388405964569 72.84654331704695 0.4556607901750226 
2.774736959101898 4.7376621205964655 73.54502411493455 0.4140267770583163 
2.8246842155760774 4.80441563651704 74.80962046082044 0.3746384081947378 
2.877325896477238 4.862731401882874 76.17741954853766 0.3377278903934801 
2.9154516121169465 4.8995205157201 76.89652541213647 0.3051434280579191 
2.9957493387545857 4.96560054776252 79.39520509041972 0.27179139214788434 
3.0727957519303475 5.021594114264625 81.77318105458747 0.2410568350737564 
3.1193431436241204 5.053931516102713 82.80748664321487 0.21938217408589367 
2.8312285441358145 4.767816886493306 75.5403869958906 0.21938217408589367 
epoch: 2, train time every whole data:204.55s
epoch: 2, total time:823.44s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.47s
test time on whole data:57.31s
2.763200503948189 4.377175639002897 86.07312267230459 0.63811497756559 
2.806793093501812 4.464661027821757 86.55534248443814 0.6070420435711057 
2.9201258532552137 4.729071048057239 86.04998461153576 0.5611855847561138 
2.949816686835495 4.794059171058583 86.08960911921417 0.5261530788882018 
2.984037130714616 4.861882600577086 86.11326507839378 0.4920304179815089 
3.0214455410044287 4.932782119055489 85.87635545335638 0.4605907643274216 
3.050465940830076 4.994149123524651 85.56955921064504 0.4313980334895947 
3.0690686461739243 5.034600994004169 85.33298478245923 0.4011707780548801 
3.085374370035671 5.062926148533848 85.27201730452921 0.3689955644419329 
3.154576525117226 5.128925063058577 87.5247676947911 0.33181348420082285 
3.227058140178965 5.193277451981101 89.95545531561541 0.2937742525699356 
3.2855875999437023 5.245911992291524 91.81737551284533 0.2606135149892678 
3.0264625026282768 4.908562011815121 86.85252133260268 0.2606135149892678 
epoch: 3, train time every whole data:204.77s
epoch: 3, total time:1099.26s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.60s
test time on whole data:57.50s
2.089089798538458 3.8214328499213894 57.35057120968477 0.7066618651001497 
2.1752003712674512 3.9824408478191033 59.38116136531909 0.6710028721290091 
2.342880782470728 4.27512702487487 63.19706688255457 0.5847984619144005 
2.361973578297932 4.311025433411588 63.46403143944038 0.5752504856568179 
2.397522345531377 4.355520553374084 63.9723441834146 0.5607826629580132 
2.4434012575226705 4.404038415084249 64.6977504798959 0.5503466754668978 
2.4752157725900235 4.423316621896996 65.4845715865753 0.547800813766712 
2.476788132523763 4.414205060768706 65.12434910100907 0.5490048443511552 
2.4967396813910456 4.43324610046578 65.06822595223639 0.5445284968021008 
2.5347924148992944 4.480980756432908 66.14697740590199 0.5326879561291632 
2.5695863417631815 4.5239133963882345 67.09085827023452 0.5113569037477096 
2.6128443115638302 4.574375407531025 68.37061367226079 0.4798270807716083 
2.4146695656966464 4.338446800810019 64.11253410948964 0.4798270807716083 
epoch: 4, train time every whole data:204.49s
epoch: 4, total time:1376.59s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.45s
test time on whole data:57.34s
2.145915249797826 3.692470145967678 62.04129644368853 0.7318061340414206 
2.209501713147653 3.7957796079218262 64.02324006509465 0.7043601559821878 
2.2940490231159187 3.966187414297429 66.39770339339172 0.6623363642372526 
2.3315841243959787 4.032539023566301 67.01300214602101 0.6471517624097516 
2.397866790478783 4.147878337620224 67.79852192035504 0.6232051222465919 
2.4689532222785058 4.250514735306796 69.49855611276438 0.605113541012634 
2.5307038457378566 4.317313574663925 71.21077678539788 0.5928170471906892 
2.5623575080901917 4.350097704809888 72.4487702970749 0.586076870417175 
2.5759947336378195 4.370427580073189 72.47881382716497 0.5780447808896138 
2.6018481551344017 4.412924547022306 73.13768631054019 0.5637172234355156 
2.644296428224072 4.477177934165389 74.500654777758 0.5430823633571341 
2.706055219122519 4.561376989010216 76.35775277918633 0.5039170890121663 
2.455760501096794 4.206091134050403 69.74247331323762 0.5039170890121663 
epoch: 5, train time every whole data:204.49s
epoch: 5, total time:1652.08s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.47s
test time on whole data:57.33s
1.877010875765855 3.2753400925557257 61.40992683946538 0.7372170591364107 
1.9299031849155823 3.337843787511088 63.67148729138239 0.721307925195606 
2.036727691042991 3.5373178707476858 65.29443839116267 0.6865354119954723 
2.0589033173738667 3.581557227444537 65.37518101072557 0.6823674823034572 
2.1168696837795986 3.7013844335927804 65.6144187893557 0.6663295298472508 
2.1738122524248347 3.8139015260143094 66.0196158277376 0.6518098413176572 
2.216249451495175 3.900613611531601 66.77655474487291 0.641539673604704 
2.239876256328431 3.936500276801449 67.48819756015797 0.6371907845047099 
2.251970486765905 3.9588549754505955 67.4585969999783 0.6384968093844768 
2.3268034449869854 4.070773421816455 69.20341197240822 0.6302496447368146 
2.360858492697988 4.125173884484548 69.71674968769315 0.6171221502522858 
2.4035508434753865 4.1962221792886 70.57666346613998 0.5912697777357666 
2.1660446650877168 3.7973149004549387 66.55057016490214 0.5912697777357666 
epoch: 6, train time every whole data:204.42s
epoch: 6, total time:1926.91s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.40s
test time on whole data:57.22s
1.9156538559264016 3.2844615752135273 60.962503771746555 0.7480689773126157 
1.9413848417526378 3.329946794586575 62.57174857713724 0.7323609373548388 
2.0200682934669865 3.4841000356488574 63.721173809975994 0.7114634882128993 
2.054103142809655 3.5552967404221456 64.17705806290523 0.700114718168677 
2.1390957410978597 3.708284829047199 65.25932244106731 0.6790245421062785 
2.239051988471654 3.887724552520491 66.57303701364053 0.6560102207158214 
2.337389558657383 4.0574796625155525 68.33976555730777 0.6365501680824253 
2.423513096528571 4.177090029684573 70.50534484486383 0.6208770008056835 
2.501363665170169 4.273853705804333 72.68276349083294 0.610246115454914 
2.581446956279111 4.376609521755054 75.07582057173305 0.6062589078078333 
2.6329551503232547 4.420916791377747 77.03238853119007 0.6064871297120065 
2.6375139993074392 4.427461466129246 76.84249250133675 0.6080074610078069 
2.2852950241492604 3.9370015989011833 68.64558439859879 0.6080074610078069 
epoch: 7, train time every whole data:204.05s
epoch: 7, total time:2201.54s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.48s
test time on whole data:57.36s
1.8624524432902358 3.115818109511258 59.27132594213902 0.747209009948249 
1.9114435305997197 3.200317688918672 61.59550758278848 0.7275944512881676 
1.9733971687581922 3.347591469152174 62.26980132934056 0.7023787841908148 
2.014948515967599 3.415424308987161 63.12677492107066 0.6875322826026183 
2.0599078033250713 3.4896811295939196 64.5231878599483 0.6686520527919768 
2.1038996042635825 3.56037235313431 65.8906082853837 0.6506112105409063 
2.131130077923781 3.623313767299017 65.95509344281116 0.6366839816343829 
2.14659219616279 3.6566971416707372 65.71937369936782 0.6286242646119623 
2.1689507801051118 3.6876703697911033 66.3750204530937 0.6204343085367473 
2.1839245989999423 3.7126397939967797 65.58277786401484 0.6168296669530209 
2.21718007718887 3.7762026534298685 65.22780192357665 0.6078098782215915 
2.2619138742350042 3.855431062493746 65.87395577109068 0.5879424853436002 
2.086311722568325 3.5435677893349906 64.28439360648781 0.5879424853436002 
epoch: 8, train time every whole data:204.71s
epoch: 8, total time:2474.50s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.54s
test time on whole data:57.45s
1.8494089155159004 2.952750019604944 59.836269742800155 0.7706440836353751 
1.8790581976392617 2.9991898603402225 60.75524305211325 0.761052989302138 
1.8940343521985092 3.0699237018146186 59.00385142790762 0.7558315330089322 
1.9171807265332235 3.0991673013115646 59.21134002318402 0.7508870829859429 
1.9507257065112216 3.191561532849218 58.47712488020327 0.7376257258456024 
1.989593683592975 3.301460449938891 57.85189271445079 0.7201949262914082 
2.0161406583232537 3.3931919882975907 56.07946040035422 0.7075403156051606 
2.0302776630873836 3.4254874377396964 55.73412157243073 0.7005226674301871 
2.039904552381131 3.449142567954604 55.572468811319965 0.6926725852235436 
2.061671219217813 3.490559289352846 55.38704902487801 0.6829119974488711 
2.075197419934241 3.5252159435804984 55.094530044203836 0.6799141632175246 
2.09275336498572 3.5628744004900916 55.26387051573385 0.6729182865469507 
1.9829955383267195 3.294925065135084 57.35548648012596 0.6729182865469507 
epoch: 9, train time every whole data:204.61s
epoch: 9, total time:2749.09s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.65s
test time on whole data:57.63s
1.8323061187249563 3.1475340923641792 59.7630160649154 0.7750487998727608 
1.8756444938825 3.212400608384236 61.28424970041909 0.7609481573693557 
1.9368335382284685 3.3538664605056203 61.97800514320333 0.7367542192937039 
1.953382277346438 3.3893983619789663 62.10278317079586 0.730005718352232 
1.9837205013509485 3.4439082696034466 61.84420095989939 0.7236070391393881 
2.0084900916185053 3.4835634527701624 61.58496578936875 0.7220277903607819 
2.022880413053912 3.515562977576098 61.07910594989622 0.7206353384148414 
2.002601413435613 3.4849389575361887 60.59343018801356 0.7218759279255599 
1.9735976665059902 3.4552187996359156 60.51077633763864 0.7209305648880544 
1.9359103824635524 3.389722591923152 60.663451689939244 0.7232625301181089 
1.928705801615048 3.3798461787042453 61.3156808135757 0.7213549081399667 
1.9612652959459949 3.4428936944152784 62.28401581493033 0.7051922605429657 
1.951278166180994 3.393231798923488 61.25030426312574 0.7051922605429657 
epoch: 10, train time every whole data:204.77s
epoch: 10, total time:3025.56s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.58s
test time on whole data:57.54s
1.6997747590111658 2.9260277245795816 55.08702027649984 0.7826471114232535 
1.7375759893040217 2.9747765428234594 56.17918997379833 0.7733317508679739 
1.7922189437716844 3.109821717006951 55.13088562910905 0.7610974038495107 
1.8053321255510228 3.1176839219875956 54.92240237831956 0.7624735383027021 
1.8351291945729227 3.182161705367451 55.27610536615357 0.7531076619648526 
1.8682351243812592 3.241775378995525 55.59896677514264 0.7469840307305669 
1.9022912449061515 3.3193040574619657 55.674257354713184 0.7413519214063123 
1.9104803923938778 3.3416810968779487 55.5756033424265 0.7420996334671294 
1.9065740711513375 3.341937124946696 55.942619325674016 0.7427074561883217 
1.9080639881179446 3.3449420891325556 56.76261608267139 0.7385123728735291 
1.9207941782191573 3.3605900759283993 58.26321752710095 0.7335262633963469 
1.9677709312151586 3.455315468377562 59.50046341189529 0.7123399945596228 
1.854520078549642 3.2302180338046025 56.15948811915699 0.7123399945596228 
epoch: 11, train time every whole data:204.64s
epoch: 11, total time:3302.43s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.58s
test time on whole data:57.53s
1.7586049195228233 2.923130425944679 56.652389430460246 0.7807433976820145 
1.7985305543105516 2.9981214974029364 57.62873779393646 0.7637474700195231 
1.8328110896668264 3.069785329740204 57.80227233636208 0.7522445766459839 
1.85482077085688 3.1108343386310318 58.11635025579609 0.7442572799677961 
1.8761655083034365 3.1388151899796175 59.10365096620882 0.7388155614586437 
1.883231673155246 3.1383804033502107 60.23935596802924 0.7375015841244936 
1.8870133200649704 3.1230402487314817 61.44904024677725 0.737760117232386 
1.8907609739085394 3.099222909752009 62.273250183356325 0.7403726743445276 
1.890046856650107 3.114322602463336 61.48516838888575 0.7377708895445285 
1.8959015227461322 3.1584117932762537 60.001446989620845 0.7333708322145875 
1.9145606982272474 3.240751736749559 58.6595745514513 0.7243995978151109 
1.961150822182319 3.3576568849342405 58.19610914719612 0.7075877123489634 
1.8702998924662566 3.124437996800216 59.30069436135293 0.7075877123489634 
epoch: 12, train time every whole data:204.53s
epoch: 12, total time:3580.42s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.61s
test time on whole data:57.60s
1.6989138622847164 2.8456217591070367 57.67442063395271 0.7944238100264709 
1.740193030070691 2.9122558374666965 58.74810472354363 0.7850542509291913 
1.775767698555209 3.00306352927713 58.14600517009012 0.7781536920407303 
1.8017466581252715 3.025512923432985 58.63600364081759 0.7738591915739989 
1.8290891971628935 3.066673630900255 59.39573441654261 0.7658863131674382 
1.8551856104386526 3.1003386225040135 60.20250933614896 0.7593849414476831 
1.8841382637001751 3.152440396461072 60.47558045897317 0.752283832812355 
1.9119920313005292 3.2032604442992105 60.48326122339821 0.7477003947466463 
1.9434391033341665 3.2669058049523705 60.99919125215533 0.7374381196590689 
1.9740741594405402 3.333503789507699 60.827426746134414 0.7290166012604002 
2.0084955761516023 3.4062611698092176 60.99711728807029 0.7164563393742118 
2.06537388396884 3.540071849877492 61.51585044205498 0.687804183261019 
1.8740340895444405 3.1607739801317676 59.84184050088448 0.687804183261019 
epoch: 13, train time every whole data:205.03s
epoch: 13, total time:3854.43s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.70s
test time on whole data:57.71s
1.6668430343913359 2.760335372228714 57.41815688728038 0.800669650265953 
1.7183176597902285 2.834565306507781 59.786625995171285 0.7871766072943215 
1.7488609096018508 2.890961716958775 60.24776193704425 0.7781568272799032 
1.7807584632013347 2.95467319172766 60.58972002733145 0.7678949273758442 
1.814619681765281 3.051212345587784 60.42689786393475 0.7524759933793203 
1.8363323441101682 3.120629771119704 59.37824435784157 0.7444088902543163 
1.8443774657414427 3.1649110598156023 57.60823867380154 0.7458171552573681 
1.8457050016874537 3.1795789948892947 56.239239981718256 0.7529224357675129 
1.8526468723642506 3.1969349342426407 55.810884661533144 0.7525808161045711 
1.8804105797614903 3.258641056694997 55.291637335665264 0.7494549387303111 
1.9214775079782342 3.339879208208959 55.557043230074854 0.742314904429697 
1.987529508639127 3.4660355116637427 56.12779356028437 0.7269849285024524 
1.8248232524193497 3.1081064239892657 57.87343030902314 0.7269849285024524 
epoch: 14, train time every whole data:204.86s
epoch: 14, total time:4132.06s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.59s
test time on whole data:57.60s
1.6937740167565645 2.801536675288031 58.43065913522387 0.8127547436117322 
1.7378405258433804 2.879168148175818 59.7653335452625 0.8013731745044483 
1.769649817626391 2.9805150124222997 59.599755388197664 0.7898340548884959 
1.786695711664678 3.018843042062804 59.67530495065719 0.7841329134444484 
1.8058598488573694 3.055605910016896 59.73624816989781 0.7793419849724261 
1.819551077162492 3.0601459798411907 60.30351639389778 0.7766745000624975 
1.8316497901388933 3.0711927981596987 61.162419762151984 0.7705489464035683 
1.84127485279597 3.0847935377219438 61.62328155163412 0.7669879117319744 
1.8528395870423742 3.1221436287156914 61.91864467639607 0.761320218045557 
1.8758781390479278 3.176366485766941 62.69821014910046 0.7545795872212168 
1.9084192483383453 3.253633360260429 63.561516710241875 0.7437739453528391 
1.9803189536836 3.3899498722360932 64.6934290406097 0.7258295797417857 
1.8253126307464989 3.0781416520220137 61.097450598604816 0.7258295797417857 
epoch: 15, train time every whole data:204.97s
epoch: 15, total time:4406.46s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.64s
test time on whole data:57.71s
1.744801101614322 2.7903992136473965 53.34154797882277 0.8182785613783976 
1.7697013473518726 2.8431491056937155 53.04218523971824 0.8129758592289665 
1.807656102270154 2.9300569663884537 52.74707191001643 0.8074149086736789 
1.811849573533716 2.939869317159199 52.341269978121794 0.8040175432209495 
1.8270291172009672 2.982844114772135 53.047642840906065 0.7941206046426942 
1.84069020493639 3.01552124563626 53.57011255815587 0.785540065767932 
1.850963397271665 3.0459340158495487 53.812551145173714 0.779162578394762 
1.8590364635413779 3.067435616122393 54.04780076126266 0.7743790708922776 
1.8829819951778544 3.136221635323033 54.69857631300783 0.7585386520935673 
1.9119803555259216 3.21065539018172 55.33757599287507 0.7430833726784546 
1.9397890196846177 3.2872219494268036 55.459383581784316 0.7314131962556851 
1.992817251289352 3.4163323037984354 55.87875107180664 0.7072364648003898 
1.8532746607831843 3.0604580465458624 53.94376152571556 0.7072364648003898 
epoch: 16, train time every whole data:204.79s
epoch: 16, total time:4682.37s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.67s
test time on whole data:57.71s
1.7119279397515845 2.6775689148870807 56.91062481889517 0.8229176067572785 
1.7350819604252243 2.728403450386304 57.6997709356193 0.8139260209077334 
1.7597059327186573 2.7847228637106816 58.41169549127466 0.8027679335897974 
1.7752654096129394 2.8145934137436885 58.304621092519014 0.7968739725284268 
1.8112799051730406 2.9041582501049827 58.664929395969544 0.7833611547135404 
1.8650424184957075 3.0324248456219864 59.15741805776901 0.7620087226537826 
1.9216845674657572 3.17783775045228 59.22688434898296 0.7358637877577753 
1.9500364615126913 3.2562085245789882 58.46299668253373 0.7222500479551587 
1.9585346043493954 3.2893150616849467 58.08296471765422 0.7138847152764434 
1.9679263010207741 3.2969825329471703 58.24283349749469 0.7098279785228523 
1.966484364008531 3.275499827010375 58.89794775377837 0.712119670824315 
1.994693201190925 3.324398290645622 59.6820887017959 0.6998464590517859 
1.8681385888104356 3.0563004735209063 58.47875809259008 0.6998464590517859 
epoch: 17, train time every whole data:205.15s
epoch: 17, total time:4956.74s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.68s
test time on whole data:57.70s
1.6258757635404666 2.676295570578725 56.39601265591608 0.8247506735551311 
1.6799684853771968 2.7809448354795205 57.58628786696885 0.8092197235310885 
1.722817753537691 2.878592803491952 57.566016597139416 0.7980262846889057 
1.7588806910404846 2.9436825705696026 58.06797779383008 0.7885510917342954 
1.8094918357109917 3.060501757919088 58.651528044833256 0.7706501100268601 
1.8496517697931045 3.152875247521156 58.9265965506127 0.7559182351378516 
1.8760096395600232 3.216333529559934 58.92975547556267 0.7464366369989495 
1.894572633415904 3.248221162746803 58.338005175327915 0.7448062736930785 
1.9064925291789252 3.2688430325564886 58.10561122523185 0.7452031016385938 
1.9277889499385796 3.290121380503738 58.304859508981 0.7492511828979733 
1.9533465784585902 3.333721312483772 58.84167266217054 0.746952066618382 
2.006592699165589 3.439852071599035 59.496381803970955 0.7307167241058323 
1.834290777393129 3.1159177985727466 58.267594660807944 0.7307167241058323 
epoch: 18, train time every whole data:204.84s
epoch: 18, total time:5232.72s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.59s
test time on whole data:57.63s
1.607986601026019 2.6061853202303165 58.11585238469483 0.8312256096541533 
1.6406404986846306 2.6743687487800343 59.47554500606238 0.8200246485729422 
1.6608059654240275 2.7056762746132614 60.02327288031933 0.8149472516376536 
1.6796652915933656 2.731491310093672 60.39231433738824 0.8106849087129675 
1.7019205195823064 2.768808385254904 60.53007613271985 0.8056183617140322 
1.7289521668962247 2.81145096562797 60.89748812399643 0.7998984004369133 
1.7465004177610612 2.8301557346003365 61.301364343199985 0.7974932963220606 
1.7603288550415919 2.839923054372668 61.77534667673312 0.7971298452304315 
1.7835611925221801 2.8982097479704487 62.565236692536 0.7886626192979418 
1.8119320439645754 2.9759557529809433 63.07355325048854 0.7784646077090215 
1.8544597584447335 3.0873129591542163 63.237751386520976 0.763884323228615 
1.9103642476975387 3.226453661027203 63.24795029910777 0.7437072667881189 
1.7405931298865212 2.8514487074211234 61.21973534626507 0.7437072667881189 
epoch: 19, train time every whole data:204.94s
epoch: 19, total time:5506.64s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.67s
test time on whole data:57.63s
1.654929474952409 2.6772464515034753 58.11637566583261 0.8270829714867725 
1.7086301603820175 2.7908581422197822 58.98037709530855 0.8157184290456798 
1.750726568939519 2.8817754779848763 59.57631604949278 0.8049189047989188 
1.775217422182095 2.9216622849074474 60.39506916825954 0.7984022863211296 
1.808899356074986 2.967511717242954 61.77922774299197 0.7885050787909587 
1.8174328714541736 2.962510554681117 62.62577114190443 0.7870791542029006 
1.8222468665927827 2.9678868213708296 63.48850888438244 0.7851918814232515 
1.8259952476527541 2.986696908256411 63.76773407239589 0.7838329417712584 
1.8357069555306365 3.024918298076783 64.10203581560138 0.7787343860249234 
1.861123319623105 3.0881387622718024 64.67185732950296 0.7704625622762009 
1.8929274213942922 3.16099911877243 65.03452228354158 0.762860770152896 
1.9505137332169604 3.2959995200796355 65.28376753894695 0.7463857827973426 
1.8086957831663109 2.9812028238029034 62.31860895573769 0.7463857827973426 
epoch: 20, train time every whole data:205.25s
epoch: 20, total time:5780.95s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.62s
test time on whole data:57.63s
1.6119416978841736 2.6315071623658497 55.077654897842855 0.8269621508352949 
1.638606157829985 2.66663794851335 56.30221792009238 0.821897857842776 
1.6746646985230702 2.770648367160172 56.351368266685675 0.8138201807842376 
1.705174137185106 2.837099559855653 56.480342188046805 0.8070886711131025 
1.7422657893870381 2.9203785018078734 56.78810618900432 0.7959341546189183 
1.7625801403972188 2.9674781886641277 57.14790348920992 0.7883942485549684 
1.7703185768492875 2.9861826624985035 57.62971080974882 0.7837356295962736 
1.765287381285358 2.9606325085419543 58.228286900539814 0.784787714701053 
1.7656618825041113 2.9532540550402944 59.211346176077114 0.7821376578357576 
1.7784883076831521 2.972293114279751 60.160272844959486 0.7774347063212939 
1.7941327970589378 3.008772800227716 60.524863630484646 0.7731893770063399 
1.8394493307222035 3.1126387081451106 60.66899654741687 0.7598473896432297 
1.737380908109137 2.9022412701381333 57.88101892809685 0.7598473896432297 
epoch: 21, train time every whole data:205.10s
epoch: 21, total time:6057.30s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.68s
test time on whole data:57.66s
1.6232287874572155 2.6871261145530285 52.93494235199315 0.8308182945824253 
1.6607002526353158 2.7748489264479628 53.37443721013654 0.8217909285528612 
1.7131934579929249 2.9027633596990476 52.64488852976255 0.8133956852833482 
1.7473310856054582 2.9609878577355166 53.133477627891715 0.8066826389572465 
1.7848336800895632 3.0525008980822177 53.468035772703324 0.7938453963154092 
1.8072509923653588 3.084498889335946 53.81575953086818 0.7883976327978872 
1.811097370720424 3.080033399463011 54.23724579190985 0.7853946212082524 
1.8119527523679038 3.0723558660259673 54.71248815230758 0.7842846018424658 
1.8269200234425564 3.105084281945989 55.66237725920744 0.7762639076191814 
1.8538245092958567 3.160011588548799 56.54365008443797 0.7676399536856614 
1.8833207591155632 3.221478255064019 57.20825664780912 0.759238156347926 
1.938088715403385 3.3499425553224516 58.00141692605847 0.7376936218081077 
1.788478532207627 3.042737910256858 54.644832591051184 0.7376936218081077 
epoch: 22, train time every whole data:204.92s
epoch: 22, total time:6333.82s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.70s
test time on whole data:57.63s
1.5787714271235855 2.5378174968401286 58.000985722993136 0.8330347946247109 
1.6067115216075132 2.6036671749605502 58.211401386237846 0.8248101413661483 
1.6266741994936018 2.6578083702239588 57.18651503235929 0.8190973491654575 
1.6541717875229993 2.721256235753114 56.7120891334688 0.8125333672019871 
1.6960152418270176 2.825873910403271 56.587998082703095 0.8002982173867756 
1.7192055737487084 2.8766854568163973 56.861144847554925 0.7922576604156663 
1.729604914617503 2.89678528279803 57.39015665483744 0.7865141463355683 
1.7346505196861746 2.903410775722927 58.06112998960587 0.7840394861885074 
1.747032255369283 2.9129756005330876 58.55052909921012 0.7823915100402213 
1.7675653904228517 2.9436761803530715 58.60693177578102 0.7790743195940237 
1.7965026860198094 3.013988985259896 58.266260637097865 0.7700053441192924 
1.8481583894896543 3.1471295292982213 58.50786302782372 0.7490667116868881 
1.7087553255773917 2.841801557439647 57.74527025528974 0.7490667116868881 
epoch: 23, train time every whole data:204.81s
epoch: 23, total time:6610.00s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.68s
test time on whole data:57.74s
1.6167664172632532 2.594638688496796 55.922495399156716 0.833290872448939 
1.6411782056147322 2.638537785500553 57.711167477757186 0.8245790773922542 
1.6675123733927806 2.6898224190331645 59.3292126563028 0.8160289607217515 
1.689326500613952 2.715452515338588 59.99759935138898 0.8128142643261801 
1.710072682765623 2.755274751212056 60.19272648484522 0.808089368405737 
1.7325956148099864 2.7954400853205157 60.18194833987855 0.8052943346680984 
1.770415317958487 2.874282852964457 60.29918751062846 0.7987164775265923 
1.8159202728584587 2.971782785363472 60.797806523433216 0.7884707328423296 
1.8667705306754048 3.0793013286710536 61.890520856479526 0.7740847412721198 
1.9108798355511611 3.1804044921113066 62.984279693129785 0.7597025760277077 
1.9459595180369382 3.25133820093147 63.95259763487824 0.7490728950452423 
1.9873638664660531 3.348436239855782 64.8843425469612 0.7304968608564976 
1.7795634280005692 2.918078311738667 60.67878150778806 0.7304968608564976 
epoch: 24, train time every whole data:204.91s
epoch: 24, total time:6885.00s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.73s
test time on whole data:57.69s
1.6815563699920617 2.8114052493717603 52.23882128451231 0.825084735248666 
1.6980176331200416 2.8340485477454647 53.27470715553416 0.8167095510476274 
1.7053677581194788 2.8341990790567992 53.70380976045337 0.8121383600254494 
1.7138930533584207 2.8361049182644913 54.30903883500271 0.8071633247368232 
1.7333012817580962 2.8776808798522717 54.819564784793364 0.7990473252336786 
1.7557300013345445 2.9221149671052786 55.05100083086162 0.7922768387039502 
1.7773305309821097 2.9670178355464842 55.11461522169399 0.7869687282573982 
1.7923058612979061 2.997858582538199 54.89927466861113 0.7858632917185542 
1.8123363952530283 3.0350870493480566 54.934578043334895 0.7833218137356189 
1.8338500202601509 3.0819284279069596 55.0382658032729 0.7761326801257733 
1.8624318831068065 3.1419273795906535 55.37596679454408 0.768183049118509 
1.918141858912827 3.2694653994982943 55.92617874606203 0.7494517194836826 
1.773688553957956 2.970596294067065 54.55720543559429 0.7494517194836826 
epoch: 25, train time every whole data:204.65s
epoch: 25, total time:7160.04s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.53s
test time on whole data:57.44s
1.6969147726653055 2.719388500379499 56.74441208342632 0.8342267055775536 
1.7323883601360555 2.7935350796933394 57.64441438906004 0.8254738121478588 
1.769503692587128 2.8784220364749835 57.46141636999096 0.8223534649279032 
1.8011014482538614 2.937971806535906 57.73467266777016 0.8186390967873474 
1.8422093119768514 3.020456129667003 58.06336988678331 0.8131999455626073 
1.8749201875792019 3.0728955474593582 58.52839084917224 0.8069734274428172 
1.8977177842332138 3.109896441263056 59.13822594569981 0.8004059955891396 
1.9234293503057922 3.1511833028493945 59.76687226370516 0.7979171018483152 
1.9730714925413153 3.251910172876927 60.7545936847099 0.7918992539053412 
2.0458883628208366 3.4029597987138156 62.42875177703513 0.7777430233205452 
2.1130529396988096 3.5323470250614606 63.85691531945271 0.7646034251210491 
2.1786401973921095 3.677187013284574 64.93828429154975 0.7440525347374725 
1.9040698250158734 3.1416210408822134 59.755149685503326 0.7440525347374725 
epoch: 26, train time every whole data:204.52s
epoch: 26, total time:7434.51s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.55s
test time on whole data:57.54s
1.6209283389248663 2.642191728136465 52.146149384553 0.8310905757946248 
1.6760962756521822 2.7865951902629607 52.38684172365554 0.8174098658626527 
1.7373516892423586 2.928953844784874 52.234702503626174 0.8090926161797525 
1.7731784831225161 3.001870391016419 52.547637047943255 0.8038174499229959 
1.8005200960793133 3.0535896966636167 52.99003304226855 0.7979436796598492 
1.8036542627468173 3.0391598308272316 53.692448474284916 0.7972985809171702 
1.7961040755324065 3.005143320416889 54.40601397304394 0.7954626313540604 
1.7859826595468358 2.9737624331633814 55.11172343557743 0.7938520217348832 
1.782469170497553 2.962754020388936 56.011804410317346 0.7901242532338253 
1.7949883835476246 2.9914164071659384 56.713714459086276 0.7822874591159276 
1.819698337910076 3.0601721287359838 57.10359682225108 0.7722690753876755 
1.8657183978898184 3.1833844920357537 57.470954871990024 0.754609577956796 
1.7713908475576974 2.972042115532873 54.40140915724804 0.754609577956796 
epoch: 27, train time every whole data:204.40s
epoch: 27, total time:7710.59s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.50s
test time on whole data:57.38s
1.6817130472278665 2.7533415840475106 56.83326076056411 0.8324908474961813 
1.7642715181532715 2.9548111739080936 57.59280583518825 0.81482702735134 
1.7960518179887108 3.024150930838338 57.88127281468969 0.8056802752410036 
1.807439229907734 3.0414732546969256 57.65421541180186 0.8059545364949312 
1.819565882777263 3.0552513419570277 57.726173385325 0.802834605233954 
1.8264098413456231 3.0393182264589105 57.97979727449602 0.8019781806507109 
1.8267420807679495 3.0301698606723364 58.333446830369176 0.7966207164714925 
1.8260287482157527 3.0171619132841863 58.80250367785148 0.7928705774592548 
1.8314811403229834 3.020935582144411 59.46784235079987 0.7890480584545271 
1.8487505948690786 3.0551294267720044 60.31189407346166 0.7835147919445241 
1.8758904624516588 3.109384990702197 61.034707194024016 0.7779373849477468 
1.9298347492107146 3.221962388538146 61.622585334666546 0.764967977757229 
1.8195149261032173 3.0286719182739255 58.77011194833085 0.764967977757229 
epoch: 28, train time every whole data:204.66s
epoch: 28, total time:7985.66s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.52s
test time on whole data:57.46s
1.7219369529012944 2.7234967652616655 58.15399582737969 0.8356917208940815 
1.7595445749269178 2.805260469983375 59.27657894799364 0.8258109007063326 
1.8108024619967633 2.9080104336501527 60.14264528629912 0.815958154550824 
1.8528294805284766 2.988795396712096 61.51112774980398 0.8042460431025497 
1.9046228393096298 3.106821404159719 62.433705899443815 0.7891627098192995 
1.9484570227801976 3.193938882716278 63.28586672420244 0.7766558113886733 
1.9666454354408418 3.231634310048387 63.70264717540584 0.7673737363239747 
1.9695690021427081 3.2273945433226126 64.0077487496189 0.76513701365591 
1.9640232741038004 3.220700551026502 64.15009992607818 0.762827589701343 
1.972889862902374 3.233434269626597 64.88122709276514 0.7576254814953052 
1.9944222271970162 3.2699034609322823 65.74788996166635 0.752120766643151 
2.0400488101671495 3.358939385901392 66.6130507546243 0.7397437541898093 
1.9088159953664308 3.1117012478953887 62.825693580918305 0.7397437541898093 
epoch: 29, train time every whole data:204.90s
epoch: 29, total time:8261.59s
fine tune the model ... 
epoch: 30, train time every whole data:423.54s
epoch: 30, total time:8685.14s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.76s
test time on whole data:57.79s
1.5316466807585565 2.488308490905811 55.67271655048576 0.845492057677 
1.5544609114492223 2.5446085442227546 56.13035028770965 0.8391508553917068 
1.5803643557493177 2.6043056362219037 56.128910163612034 0.8338697704132555 
1.6044525807555765 2.6593093643914547 56.11617003871405 0.8306256447301004 
1.6374390323838839 2.738533264818879 56.1634430591725 0.8232837196774233 
1.6663419859339261 2.794748709065596 56.4861946579057 0.8168155440108965 
1.687950596563252 2.839384409169168 56.86032093759148 0.8089808106861799 
1.699422603359446 2.847300887399918 57.36457731509061 0.806719751847569 
1.7164341455465626 2.8817783418369833 58.07777532237735 0.7991146273220018 
1.7394244087553095 2.931430186327057 59.03132519425981 0.7899070575593808 
1.7677470939727056 2.9850234183452637 59.91890948078108 0.7823966752642479 
1.8106427105545466 3.0783609281776654 60.43353161080911 0.7706179768323038 
1.6663605921485254 2.788145396860281 57.365425540127916 0.7706179768323038 
epoch: 31, train time every whole data:423.71s
epoch: 31, total time:9179.71s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.92s
test time on whole data:57.97s
1.5284634598866105 2.485585674257167 55.731472635868315 0.8463067628108825 
1.5518518552993026 2.543189776682342 56.01175579685861 0.8401106038360915 
1.5783602165623258 2.605368007796711 55.85358973060984 0.8347843016505933 
1.6019042180848442 2.656376132530897 55.82531073661797 0.8322588882048365 
1.634660904205005 2.7317350619761944 55.84092684232472 0.8258628683536982 
1.6633032116053537 2.7844836405116986 56.1577364807887 0.819932644669058 
1.6853606334245277 2.8327481053947228 56.54484125252643 0.8112575234914946 
1.6974190651900356 2.8492312456645554 57.07620132025324 0.8072691547874367 
1.7147199788890957 2.883106027769318 57.851430542914095 0.7996209387397782 
1.7387132819577875 2.935893369107725 58.78453531391257 0.7903106030285039 
1.768629163667471 2.9915200689300034 59.636715076729175 0.7833674778986263 
1.8098760116100312 3.08222590861707 60.20392026380382 0.7718108728582718 
1.6644385000318658 2.7873166939357077 57.126606393453194 0.7718108728582718 
epoch: 32, train time every whole data:423.05s
epoch: 32, total time:9679.94s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.79s
test time on whole data:57.87s
1.528148577229519 2.4711343435143687 54.88287214511261 0.8469401651127413 
1.5509166675607364 2.5271619127334146 55.18659716830937 0.840422202574905 
1.5748783960113568 2.5847430570483687 55.09476906281515 0.8346586045942872 
1.5929940754174299 2.625754925518628 55.10379606867922 0.8318386493946207 
1.6204414741883852 2.6921374398419076 55.13842893700481 0.8255653339499605 
1.643601232255055 2.7319198251855488 55.433607349062086 0.8212376217495443 
1.6648113558695963 2.77260434168633 55.88261297148214 0.8136775051244314 
1.6761789677607872 2.7901271724381997 56.44986354205057 0.8093503362858958 
1.6931789196562022 2.8208839212331562 57.259595503379025 0.8030386221117525 
1.7137019789309373 2.868676729095611 58.02612536488338 0.7953061511329391 
1.7387352459627603 2.9211276796465215 58.63423207730389 0.7889607439559179 
1.7747970913912923 3.004745922604929 58.88224203624658 0.7786853366241093 
1.6476986651861716 2.7386104981998085 56.3312982578215 0.7786853366241093 
epoch: 33, train time every whole data:423.64s
epoch: 33, total time:10174.93s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.79s
test time on whole data:57.82s
1.5214578157168415 2.4639101364518625 55.978951529124885 0.846764751984915 
1.5461904008085174 2.5271209661332477 56.02947101276654 0.8403241508089352 
1.5727194564902178 2.592358951377171 55.849395769682154 0.834070408066002 
1.593391416789609 2.6418030257145295 55.7898365241147 0.831202956969704 
1.6236576355308117 2.714109289524928 55.82590107000354 0.8248910050803447 
1.6481887168776954 2.75788564501475 56.05791034263985 0.8201162136756547 
1.6723295976255266 2.803639307531511 56.50982955557071 0.8120242809331057 
1.6854506579578101 2.825770615897008 56.964811450332874 0.8074436461752935 
1.7044625076364193 2.863109001714602 57.82048340191016 0.7999299095324826 
1.7296484505456118 2.921458023585432 58.79642916019918 0.7899560913484344 
1.7591740644515625 2.9781606731418506 59.71763275222484 0.7816411665819236 
1.7950462341231428 3.055117492241889 60.39359775221341 0.7700637859743042 
1.6543097462128138 2.7675174062992034 57.144589232757134 0.7700637859743042 
epoch: 34, train time every whole data:423.21s
epoch: 34, total time:10668.51s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.86s
test time on whole data:57.90s
1.5280176943656767 2.4967271304202896 54.665790152594475 0.847402786901507 
1.5529413983956688 2.556395641786315 54.942905186172 0.8415649918632775 
1.5799054474990282 2.617164789215423 54.98063089163271 0.8357059513403348 
1.6016650386458884 2.6630344176261884 55.17342428437647 0.8326807157219941 
1.6303874375215244 2.7298435266696894 55.275580261174994 0.8267603987200376 
1.655597525030818 2.7756394336112713 55.57796329509489 0.8218390120906539 
1.6793913418796977 2.822532814816582 55.9997301635677 0.8139756405543267 
1.694645189176624 2.8500473882502724 56.44564654460507 0.8085421435167585 
1.7141702366633607 2.8901154851845883 57.235445739852196 0.8002863932844223 
1.73836709306123 2.9426868146772978 58.222397981713414 0.7910687533417669 
1.7686445574118268 2.995137958077894 59.20523005507108 0.7836480894239362 
1.8060231179740458 3.0712721360983153 59.904909420416764 0.7729633705942636 
1.6624796731354492 2.7894373322241686 56.46921916608295 0.7729633705942636 
epoch: 35, train time every whole data:423.69s
epoch: 35, total time:11163.07s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.78s
test time on whole data:57.76s
1.5224850596601942 2.4792890760804056 55.50384961686614 0.8471626753615847 
1.5472731772192 2.5385574767377683 55.79705692289959 0.8412682341362989 
1.5755423104487183 2.604488399254003 55.80301895794183 0.8354802393708634 
1.600820607233615 2.6602994937595414 55.985647184340735 0.8320403387822014 
1.6335098126460577 2.7378499518320365 56.18987651156117 0.8248715992979435 
1.6608580536993132 2.785553637989888 56.569451728581534 0.8193440440489376 
1.6840279578679196 2.8338735189459263 57.04934095847442 0.8105135863783038 
1.697494809545133 2.856853947562629 57.59980169805567 0.8048369451650609 
1.7119212736430622 2.882490095349035 58.46279507642676 0.7986046204989309 
1.7325172208396806 2.9287756860022713 59.47017225371113 0.7898446994848561 
1.7600655874128321 2.974267259200938 60.38050606132161 0.7832608844068314 
1.7963085449227973 3.047891420692476 61.068246356578 0.7727559049997788 
1.6602353679282102 2.7827095704540707 57.49007120033893 0.7727559049997788 
epoch: 36, train time every whole data:422.61s
epoch: 36, total time:11656.59s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.72s
test time on whole data:57.74s
1.52990266551716 2.4502299911195298 55.44930077357023 0.8478827302631439 
1.551428584434092 2.499752845379334 55.68448409834552 0.8423403797492864 
1.5749323317458233 2.557767741896731 55.339941213828816 0.8368469630862397 
1.5965013258786251 2.6070895375978154 55.304529332064256 0.8334071911564723 
1.6269367310204321 2.6819893779263886 55.214390993166326 0.8265780014081903 
1.653738320964699 2.732471474032206 55.32576552269345 0.8206434629746062 
1.6784857873409278 2.779628670762571 55.62856516644437 0.8121748712919744 
1.6909886623829427 2.802951975431728 55.930648443801765 0.8065432971487908 
1.7067462114691734 2.833178112664985 56.55498363629925 0.7997835057081415 
1.7279352068122298 2.884688111676339 57.359984319470925 0.7905614630952982 
1.752940019494721 2.933762893140918 58.05806253689757 0.7833849830978709 
1.7853167402073742 3.005978667843078 58.49284794557617 0.7733570887943453 
1.6563210489390168 2.73594928473866 56.195335515188106 0.7733570887943453 
epoch: 37, train time every whole data:421.81s
epoch: 37, total time:12148.90s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.70s
test time on whole data:57.73s
1.5267160093337298 2.4601302552357702 54.93633442670702 0.8478956035041294 
1.5493345731278616 2.5133637316919155 55.24275923687823 0.8422025333915998 
1.5732285588983268 2.5724608396895414 55.10328872133924 0.8366480850896211 
1.5944658613422265 2.6227312277859225 55.20346455601579 0.832975778231878 
1.6236446524714785 2.6905383849562567 55.38751108026617 0.8261849941617277 
1.6486526993740174 2.734996241059267 55.696880054346856 0.8205303031945954 
1.6726774206689248 2.786156290122528 56.06294829799646 0.8120813515883964 
1.6858448229192624 2.809213313211686 56.347958281876195 0.8078321589994623 
1.7028652315497221 2.84536389807336 57.01282172860256 0.8008858677661712 
1.7263843953984657 2.9020023881881034 57.93292620218091 0.7908083229579234 
1.7536391597564023 2.9587271980218492 58.86252759024483 0.7814877333200067 
1.7876059606556913 3.0261340719374252 59.53621745340235 0.7715378918078688 
1.6537549454580092 2.7487622542437227 56.44387298178337 0.7715378918078688 
epoch: 38, train time every whole data:421.55s
epoch: 38, total time:12641.34s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.66s
test time on whole data:57.68s
1.5168262943803732 2.464260199262999 55.798841226947935 0.8477500558569658 
1.5414400921484366 2.526174990365436 55.88307068909869 0.8417427117010229 
1.5693956253566734 2.5924549007065787 55.74262451738985 0.8359684005148468 
1.5916196507133897 2.6435774443980247 55.874275083184614 0.8323910458038691 
1.6202038916260713 2.7091088627729456 56.08029445763303 0.8259208979775012 
1.6430120495692606 2.747338223896177 56.42341466499754 0.820583567914734 
1.6626248748537509 2.784092951571133 56.85506459681905 0.8132527974143364 
1.673904907694796 2.8037073415795746 57.28707333099058 0.8083929302988215 
1.6893358858103553 2.8357699661279967 58.09429700762472 0.8011036035108765 
1.7118301656403179 2.8887789694001476 59.15641516636142 0.7912073391696588 
1.7393397526403978 2.9408117914767167 60.085595159431904 0.7830373785382003 
1.7740338233368738 3.010435078553663 60.629275748122055 0.7735366444862142 
1.6444639178142246 2.7501557180960288 57.32593328579634 0.7735366444862142 
epoch: 39, train time every whole data:421.89s
epoch: 39, total time:13135.30s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.79s
test time on whole data:57.93s
1.5175383788200008 2.473343252550689 55.372858741300135 0.8482359939498497 
1.5446232112493543 2.5431002652523107 55.608060244978965 0.8414972799167599 
1.5741275754218833 2.6061276545331644 55.71288707515062 0.8358956262528239 
1.6000209619342571 2.664378533407118 56.00622404724322 0.8312887991933201 
1.6341901597417003 2.7438761342565203 56.26715973911567 0.8236258531467033 
1.6627937172838443 2.7932629193935834 56.70217581943783 0.8173441547025991 
1.6863202426903658 2.8447016211111866 57.16711083288622 0.8078587837836163 
1.7003699112469775 2.870661865690413 57.64918083511199 0.8016972027739026 
1.7170778584034139 2.9009546602968332 58.42636849809062 0.7942749690019978 
1.741323457047698 2.9582041650251294 59.536983903792375 0.7828845999216997 
1.7670134939657791 3.0045257025331544 60.43616991194219 0.7750435415012603 
1.7966488633193962 3.0648689305562575 61.10115263993649 0.7657738158498875 
1.6618373192603892 2.7947407012114196 57.49895598560801 0.7657738158498875 
epoch: 40, train time every whole data:423.80s
epoch: 40, total time:13631.89s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.87s
test time on whole data:57.93s
1.5171928554034482 2.461739482345893 55.196261520538194 0.8485336534319586 
1.54042720289963 2.519636097026153 55.430455336844865 0.8427511122020982 
1.5664065160965104 2.581410402154638 55.397399747405885 0.8368405531088318 
1.5875878014261169 2.6300834831913855 55.50876474842036 0.8331566767040622 
1.6156639391830456 2.694864211895626 55.64560984846567 0.8269743495378342 
1.6387635449824767 2.7332655199536338 55.982521675413345 0.8219985818529945 
1.6583478980522957 2.776668599481558 56.414489799713785 0.8138937730447298 
1.668335397608667 2.7928092928079016 56.94228929884685 0.8095586998702805 
1.6849385692540202 2.8231489206968643 57.854834443847416 0.802801521236756 
1.7055975495715225 2.869682622854392 58.83411416528255 0.7947446337866428 
1.7303638227196145 2.914449026910563 59.487789109138845 0.7887331827087127 
1.7640291292815513 2.991393675170351 59.75190099230048 0.7791248591984122 
1.6398045188732415 2.736788710459819 56.87061608809203 0.7791248591984122 
epoch: 41, train time every whole data:423.44s
epoch: 41, total time:14126.83s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.74s
test time on whole data:57.81s
1.5134532412585935 2.4498220555777954 55.841542421008526 0.8487753777754402 
1.5373407324066475 2.5125313848039745 55.8748238213175 0.8424051907947878 
1.563959612112581 2.57438082096643 55.63790053873207 0.836810991879494 
1.5860870338278334 2.628687444527137 55.65620227117575 0.8326756499683955 
1.6155497945902781 2.697640931426034 55.775183450364764 0.8262628498353178 
1.6410471756903544 2.742562895621656 56.13034100577391 0.8207645405636153 
1.6647481244380276 2.789510025269887 56.606434453696195 0.8127127578805649 
1.6791130350598444 2.817523762172747 57.01644686997661 0.8072696704775275 
1.6980917524259005 2.854483448929511 57.70591005609524 0.8003655634207658 
1.7249618869853163 2.9201670994467874 58.540818187626286 0.7897486158503009 
1.756837077784751 2.984856628769851 59.240399071065966 0.7803411304615271 
1.794885448639769 3.0676397311431423 59.78444963693396 0.7688853397512232 
1.6480062429349913 2.759315053627939 56.98427109387894 0.7688853397512232 
epoch: 42, train time every whole data:423.00s
epoch: 42, total time:14620.39s
predicting testing set batch 1 / 168, time: 0.34s
predicting testing set batch 101 / 168, time: 34.80s
test time on whole data:57.84s
1.5131310275388616 2.4515242978182616 56.053148830699385 0.8487323494453001 
1.5373043232769483 2.514699842696713 56.16713677815657 0.8422402933267927 
1.5663023001497522 2.5799467282613495 56.017001998509265 0.8369794712640495 
1.593160772144351 2.6468933445260108 56.10091277647777 0.8322044397385902 
1.631183701238462 2.738429477267995 56.229005901907534 0.8242566789077953 
1.6638428370930431 2.80022330737476 56.56431176996824 0.8178546345935825 
1.688374111337676 2.8521407268828205 56.89639648590342 0.8092594418331446 
1.700244702355049 2.872425573757773 57.262161157519344 0.8038920664552788 
1.7129212840069972 2.894262533068244 57.97157529738178 0.7977218587743696 
1.7356500448622696 2.947610565721835 59.00169860427985 0.7871400836490025 
1.7621268708398121 2.9990959610113803 59.929183026925934 0.7782465793984481 
1.7956953006237746 3.065394536564949 60.63091844288696 0.7684378926006233 
1.6583281062889164 2.7865453830431015 57.40202476554036 0.7684378926006233 
epoch: 43, train time every whole data:423.17s
epoch: 43, total time:15111.66s
