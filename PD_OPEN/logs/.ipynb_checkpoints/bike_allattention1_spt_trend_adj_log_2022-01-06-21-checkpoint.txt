total training epoch, fine tune epoch: 40 , 40
batch_size: 4
folder_dir: MAE_ASTGNN_h1d0w0_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1_spt_trend_adj
load file: data/bike.npz
ori length: 3001 , percent: 1.0 , scale: 3001
train: torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12]) torch.Size([3001, 250, 2, 12])
val: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
test: torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12]) torch.Size([672, 250, 2, 12])
TemporalPositionalEncoding max_len: 12
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): EncoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DecoderLayer(
        (self_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_kc(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (conv1Ds_aware_temporal_context): ModuleList(
              (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
              (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            )
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (src_attn): ModuleList(
          (0): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): MultiHeadAttentionAwareTemporalContex_qc_k1d(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Linear(in_features=64, out_features=64, bias=True)
            )
            (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (feed_forward_gcn): ModuleList(
          (0): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (1): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn_cross): spatialAttentionScaledGCN_cross(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
          (SAt_cross): Spatial_Attention_layer_cross(
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (Gate): Linear(in_features=64, out_features=1, bias=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (3): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (4): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (6): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (7): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (8): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (9): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (sublayer2): SublayerConnection2(
          (dropout): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (src_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (trg_embed2): Sequential(
    (0): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (1): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(250, 64)
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
  (prediction_generator2): Linear(in_features=64, out_features=1, bias=True)
)
delete the old one and create params directory experiments/bike/MAE_ASTGNN_h1d0w0_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1_spt_trend_adj
Net's state_dict:
encoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.2.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.2.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.3.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.3.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
encoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
encoder.norm2.weight 	 torch.Size([64])
encoder.norm2.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.0.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.1.sublayer2.norm2.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.0.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.1.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.0.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.0.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.0.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.0.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.1.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.1.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.1.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.1.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.2.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.2.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.2.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.2.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.3.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.3.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.3.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.3.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.0.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn.1.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn_cross.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.feed_forward_gcn_cross.Gate.weight 	 torch.Size([1, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.3.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.4.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.5.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.6.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.7.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.8.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.9.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.weight 	 torch.Size([64])
decoder.layers.2.sublayer2.norm2.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
decoder.norm2.weight 	 torch.Size([64])
decoder.norm2.bias 	 torch.Size([64])
src_embed.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed.1.embedding.weight 	 torch.Size([250, 64])
src_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
src_embed2.1.embedding.weight 	 torch.Size([250, 64])
trg_embed.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.1.embedding.weight 	 torch.Size([250, 64])
trg_embed2.0.pe 	 torch.Size([1, 1, 12, 64])
trg_embed2.1.embedding.weight 	 torch.Size([250, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
prediction_generator2.weight 	 torch.Size([1, 64])
prediction_generator2.bias 	 torch.Size([1])
Net's total params: 1140994
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411]}]
predicting testing set batch 1 / 168, time: 0.71s
predicting testing set batch 101 / 168, time: 34.77s
test time on whole data:57.72s
23.24256776794667 28.60098230302205 913.6495123050878 0.09562220367925703 
24.948976462732645 29.783144998731558 979.3125211242984 0.08002498377499487 
27.32327088557698 32.25847546688048 1073.6592083812122 0.0584568185209984 
28.83687157831687 34.2614654230452 1134.9081276911988 0.041308225072582395 
29.13910104361957 35.19949084874393 1148.262999431669 0.028916839850717316 
30.430903533132746 37.13009656560811 1201.5115179039815 0.01988647475795454 
33.42788154435752 40.448111479389716 1323.2290536850235 0.013588510499947965 
35.72584423362109 42.494035048647696 1417.2696731499723 0.00917914948729588 
35.12139471119217 41.184939440990036 1393.8606594841203 0.005237326918339978 
31.880013180663365 37.19545739911157 1262.4654373647604 0.0005273276680784171 
28.657007051737004 33.60448261732125 1130.5215614557294 -0.004928782948897305 
27.981514505920313 32.92302095388053 1102.0647587940934 -0.01010428631994726 
29.72627887490141 35.67426474322533 1173.4003207160263 -0.01010428631994726 
epoch: 0, train time every whole data:211.34s
epoch: 0, total time:285.65s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.85s
test time on whole data:59.59s
2.8733068386898153 4.128679828652222 90.81159667995856 0.4521209514928687 
2.91016131556584 4.170026383191264 93.15183049074149 0.4335929230282358 
2.9250762032794633 4.202503714098954 93.76847144353721 0.416717959073769 
2.9347014506782982 4.259109143850007 93.27227479455053 0.37894928146973467 
2.9657426840243417 4.310553279949398 94.0237809519029 0.34880813304586683 
2.9920999989711814 4.366618159703666 94.19604967513695 0.3134097784379414 
2.999954510568508 4.4189585839295535 92.93599062705246 0.2780111438002947 
3.0216189935958635 4.4794203029565995 92.23719760214453 0.24152738331980098 
3.0403408073095517 4.532321335202753 91.22518428135645 0.2108300535159885 
3.0821087230396946 4.585766967399787 91.58045129775442 0.18363632970277496 
3.1392616011819667 4.627569049389549 92.93882086725127 0.16223696682812527 
3.1504006574678103 4.6525912111853165 91.29688656438162 0.1477918282452866 
3.0028978153643613 4.39800130674274 92.61986219491175 0.1477918282452866 
epoch: 1, train time every whole data:213.16s
epoch: 1, total time:570.67s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.81s
test time on whole data:59.54s
2.7329311121060025 4.328440245028591 78.70871338470288 0.4462365793137576 
2.7657318770726698 4.3820054729582685 79.7731474226003 0.426014012798266 
2.7909557515474126 4.441368741996134 80.15718956513747 0.4003073278428364 
2.842189194138916 4.514900395441058 81.72803328813868 0.36995434749091427 
2.8707432059563165 4.565910596495792 82.33674998343929 0.34112213206732755 
2.9058095081006843 4.6229212018812404 83.00956114296663 0.3113047673193616 
2.938424923145168 4.676199935257847 83.51880657329396 0.2822520690024384 
2.981259171741704 4.727282231759531 84.61054987656632 0.2523915412728258 
3.004858288377169 4.762582539922969 84.52489621004288 0.22940763084283863 
3.0314304481932806 4.787546671645918 84.63551591169906 0.20956533448400483 
3.0692512333504856 4.812734988396348 85.32517426075175 0.18981913534367617 
3.1115102602959213 4.857478632806729 85.5651643728663 0.17326726125393566 
2.920424581168811 4.626380373013366 82.82459146801307 0.17326726125393566 
epoch: 2, train time every whole data:213.26s
epoch: 2, total time:857.47s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.78s
test time on whole data:59.57s
2.22407833685859 3.803477425846508 63.48378042989662 0.616235432425993 
2.260913822801695 3.871550178333488 64.788959176173 0.5893356766944469 
2.395145322551064 4.125977390668339 64.12071802669068 0.5255846860605037 
2.4311122461684227 4.181717576933042 65.0712372206049 0.503386587751298 
2.469620016497427 4.232540181660489 65.47626401553597 0.482109943473967 
2.5168321221598555 4.28688124011916 66.13749842040383 0.4590774600263473 
2.563580539633387 4.3462945300992795 65.95260319298917 0.4386896737977404 
2.6166115947185706 4.395929961356619 65.52150798020155 0.42012786578468986 
2.6574544727201794 4.460432991361114 65.26146561577646 0.3939895143939577 
2.6855938448984 4.49079919397883 65.44970596459201 0.3711338209128394 
2.721407208364191 4.520545562678127 66.15436976967547 0.3423779941978772 
2.7650501192868466 4.574954349928264 66.82077313031634 0.31306010767815146 
2.5256166372215523 4.2807738349979365 65.35328211394955 0.31306010767815146 
epoch: 3, train time every whole data:213.24s
epoch: 3, total time:1144.16s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.77s
test time on whole data:59.54s
2.236862703771907 3.4829056262473093 65.71192511615034 0.724200055584202 
2.3003895844143063 3.6303108393524304 66.85057704880964 0.6748636493070034 
2.403432449758319 3.8616838430978127 66.52513892323971 0.6081159228514749 
2.43596353752166 3.92909551465716 67.28019773375853 0.575609854486781 
2.4728769522422835 3.999286079198159 67.23226542973995 0.5482572350195924 
2.499956143407151 4.048269415687331 66.59218994878172 0.534467654139001 
2.5105122404367264 4.064997072615612 66.25614151417695 0.5266909741217424 
2.5191284341728757 4.064475101632986 65.96618974190181 0.5274238358380491 
2.532918883707463 4.0809085787874055 64.73188671150851 0.5330328546947021 
2.5387274958816843 4.100597013431239 63.75641782862732 0.5316203370756725 
2.5563025223334277 4.116397468805467 64.37257669113514 0.5209643453255215 
2.5768168570729237 4.155163489283722 64.43430739390391 0.49716521063270835 
2.4653239837267273 3.966168784810671 65.80910562146398 0.49716521063270835 
epoch: 4, train time every whole data:213.24s
epoch: 4, total time:1430.26s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.83s
test time on whole data:59.57s
1.932812659240372 3.4735176711831643 58.15130823734693 0.7250196597073219 
1.9721855057473516 3.56174027478285 60.075363736960085 0.7003306667392344 
2.0647701515841876 3.772473134945723 61.45426845526153 0.6627904812136798 
2.1070034652732845 3.847885293644223 62.64855193156657 0.6405783829440839 
2.1676194101251838 3.9587162092310346 63.65503352009551 0.6154907860412713 
2.2265507093851586 4.0647712923578965 64.2611406007699 0.5938043538682909 
2.2813586429179247 4.158394927780389 64.34917626722864 0.5783041255897305 
2.306599193405005 4.196232643071286 64.55952452000155 0.5686115869249623 
2.323465716743132 4.217421086365483 64.64750493696107 0.565458240798678 
2.3605755721831785 4.263337941584705 64.84109761606845 0.5643868677892081 
2.3798926883242313 4.283029015800785 64.99765426105168 0.5591261935095051 
2.4396097182463854 4.3480074398522 66.74484461143257 0.5376605197862454 
2.2135369527646165 4.021771037217437 63.36558554338272 0.5376605197862454 
epoch: 5, train time every whole data:212.91s
epoch: 5, total time:1716.62s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 35.77s
test time on whole data:59.47s
1.759816707505268 3.0663181257302448 54.70583542855979 0.765732138237901 
1.8152473455357054 3.1969504663291315 56.873744094504985 0.7403771254024488 
1.8936910843352477 3.4183444420248743 58.19582281840666 0.7063659673008498 
1.948864624477568 3.51014721168922 59.61222911182112 0.6867640463101704 
1.9998923973516516 3.6118205316280894 59.56817657404072 0.6681019627576736 
2.0322047744286027 3.677536948166862 59.92806493291853 0.6513972790971705 
2.0551225554239716 3.7246846455597162 59.264843719041004 0.641244191881525 
2.0617553125775996 3.737474610601064 58.813615023636146 0.6372238790938715 
2.081886244871166 3.7499181361753524 58.62440441883429 0.6267803626502891 
2.099747219267612 3.754088265139653 57.579225269505855 0.6281685763695789 
2.129646517539042 3.7965085252593687 57.55169019062105 0.6190726335542996 
2.1811158365872467 3.8814999822349288 59.04718922300907 0.5946236357752742 
2.0049158849917235 3.601794840318124 58.31377591124496 0.5946236357752742 
epoch: 6, train time every whole data:213.15s
epoch: 6, total time:2002.05s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.82s
test time on whole data:59.59s
1.7405982061299894 2.969039756696114 56.86939716489685 0.7693797700097033 
1.8001497079210267 3.097526904952798 59.58451870213244 0.742209337367757 
1.856136360762286 3.2560888084469077 59.2001256624105 0.7162964879851385 
1.8977910662638466 3.326504398409899 60.547362954235325 0.702885536370773 
1.9427022648350825 3.4015051223608426 60.97620493339756 0.6884825018967138 
1.9747469664624049 3.4555433988364688 60.5136558073649 0.6804559806873458 
2.003152704063448 3.5112353056723653 59.53101205967336 0.6737355497205502 
2.029783159009935 3.5482208255573178 59.818553303575904 0.6674269044383239 
2.0603198136579066 3.5973496459455014 59.817450673598415 0.657224301546907 
2.081831881405697 3.642448867519681 58.892367052827666 0.6537121750300908 
2.111006539145396 3.707422393956879 59.15438275988 0.644763515798924 
2.1712257582830885 3.8172198324758497 60.77324272986782 0.616271401478637 
1.9724537023283424 3.4524361423810808 59.63987862693124 0.616271401478637 
epoch: 7, train time every whole data:212.50s
epoch: 7, total time:2287.31s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.88s
test time on whole data:59.62s
1.8029375702082402 3.0786857029291235 51.94382569791116 0.7750019313012294 
1.8576132412983903 3.1987840438063797 53.998937367745015 0.7460510049016442 
1.9114351710913968 3.3227499932436486 54.27179810030312 0.725823540434911 
1.9564723460984728 3.3854820175357085 55.06173313889676 0.7131100325051377 
1.9818086751890147 3.433588144184817 55.3669999316776 0.7032508795604487 
2.029072347448163 3.510751485144157 54.83394910641769 0.6878689579984707 
2.0673855967517234 3.580711089347997 54.46183064663368 0.6751314743923972 
2.093352586452095 3.6289001806423706 53.799106320320476 0.6669298766900824 
2.112142714154951 3.6591215675186954 53.66858118335074 0.6602666382804058 
2.1264117030764798 3.675804255632405 53.21520725271767 0.659752473962529 
2.1578173980957693 3.7287529735040166 53.63859371661059 0.6509451587387031 
2.205461866581635 3.815919382633514 54.80415925089599 0.6266251443153807 
2.025159268037194 3.5081279436657513 54.08873297361586 0.6266251443153807 
epoch: 8, train time every whole data:213.30s
epoch: 8, total time:2576.12s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.80s
test time on whole data:59.57s
1.856120770665684 3.1542287324433214 54.20842483745635 0.7692490777012403 
1.876917337523862 3.2015956965144823 55.52326673415582 0.75340330138892 
1.9284381977877625 3.2967481340984626 55.52722465886573 0.7382656589937501 
1.964357733038121 3.34895416750828 56.40474996826145 0.7288379741809451 
2.019926457974971 3.4410157565220496 57.650445929132296 0.7127383566435369 
2.054611324566372 3.50711065130799 57.87587215715282 0.7060158202390371 
2.0847585048614334 3.5539628079598735 58.028225433794574 0.7039020678828766 
2.122368250118214 3.6048929783414776 58.81726463688025 0.7027463035376159 
2.1638411717594024 3.6725751114299965 60.08597659075069 0.6928035735913716 
2.208081639095875 3.7514331759285398 60.940725049636534 0.6797420652597846 
2.2489461425533963 3.824814029252419 61.99227087693882 0.6659443371554312 
2.3096946933259743 3.9403723666296457 63.60959740253826 0.6376759037875976 
2.0698385186059225 3.5327813797781866 58.388816912297294 0.6376759037875976 
epoch: 9, train time every whole data:213.30s
epoch: 9, total time:2862.21s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.76s
test time on whole data:59.54s
1.9333983355686954 3.2559589156068394 61.28190314313231 0.770664438852525 
2.0033279466491547 3.386906935066141 62.58815608453212 0.7480492905272789 
2.083032980072711 3.5316247293305483 63.393404790924336 0.7238493502402078 
2.1082274904062173 3.5355216184243616 64.51617777538135 0.7257093179522444 
2.1479642342319267 3.5657762153869546 65.88457659160125 0.7244373655026337 
2.1831374266701085 3.6108524721876267 66.77758097237302 0.7216606640497479 
2.2503711883269606 3.7202208950746267 68.61134638512432 0.7071782498952317 
2.336644605386293 3.8563524410401917 70.98799278482393 0.6901872925377937 
2.4188845498790696 3.986183251734931 73.18426211187314 0.6719383178071301 
2.4563593030379463 4.0542298030127855 74.0767661766176 0.660287699358236 
2.4741614503668 4.084521049084639 74.93046057467438 0.6505163397390917 
2.4974192203893546 4.140197518896811 75.532184834928 0.6294461959096584 
2.24107739424877 3.7378713923593048 68.48068471254656 0.6294461959096584 
epoch: 10, train time every whole data:213.17s
epoch: 10, total time:3149.48s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.80s
test time on whole data:59.48s
1.6825220101378149 2.8310421344223973 57.734586740510274 0.7893480947303222 
1.727993707054073 2.901604382075168 59.28242150213138 0.7759208280825634 
1.7620438858063092 2.9537413673231345 59.81372446833861 0.7659672039314647 
1.779953624424569 2.953913874184543 59.80051455344393 0.7660782249314446 
1.8013672183944534 2.9893067802914035 59.444756370687834 0.7600437742867848 
1.820036849903475 3.031965369763892 58.52262515454877 0.7537309854457409 
1.827185959302155 3.0623794122409924 57.72249631465334 0.7501588351295029 
1.8304783375523097 3.0806712509323195 56.99107425843124 0.7512542541166044 
1.8440510337544338 3.1276954024976735 56.699697189853836 0.7481996579399839 
1.8705407525844695 3.20446254736671 56.99574905809841 0.7391129639297945 
1.9092737783824227 3.290506257443906 57.43950823024942 0.7267992699365655 
1.9578747547792181 3.399576582982257 58.31226126917206 0.7058376948425589 
1.8177768260063085 3.0729652613720204 58.229903848425934 0.7058376948425589 
epoch: 11, train time every whole data:212.85s
epoch: 11, total time:3434.99s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.76s
test time on whole data:59.49s
1.6887571394569463 2.8622597306268687 59.16169135692938 0.7884067988020467 
1.75268021143148 3.0024477402954077 60.13123938421513 0.7711007373700245 
1.8287726672244746 3.177649475612987 59.84171871185973 0.7530796731912275 
1.8756394150795503 3.2431596755096272 60.9320903643338 0.7471960851063286 
1.9082166694260778 3.2956338086609223 62.05911761310745 0.7381509886047006 
1.9294728807593209 3.3125682004986756 63.41896353331988 0.7336999343581292 
1.9486015266292684 3.3291925323958576 64.51557799585238 0.727543244275431 
1.9834729930949175 3.3711386299390216 66.01902045203701 0.7187448323456608 
2.0216057435809858 3.4282111099007344 67.44191426152622 0.708084201865104 
2.0718420733719 3.503678500457847 69.0114546835143 0.6957198547870593 
2.11624843908669 3.575134445442211 69.96686259479601 0.6835172772924417 
2.145496489930277 3.6420862185082314 70.33991490053437 0.6698718395445747 
1.939233854089324 3.3188846734497024 64.40351879381134 0.6698718395445747 
epoch: 12, train time every whole data:212.93s
epoch: 12, total time:3721.64s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.74s
test time on whole data:59.42s
1.7854021593198357 2.943107071886335 55.56716770670601 0.7894708474357058 
1.8238833053251284 3.0291116195676278 54.68018028241758 0.7828374970169502 
1.87983839716851 3.1851077835361497 53.30536785426523 0.7659506143285904 
1.9037078739400244 3.242388287022762 52.338779648221426 0.7597428362542075 
1.931543703999193 3.324419335074899 52.14632812794753 0.7434768291473004 
1.9363910600283138 3.329973774779681 52.13403200368189 0.7417502899843491 
1.9383054115952303 3.3160341234028934 52.13621017473081 0.7417353085645587 
1.9574534963154722 3.3418321307987675 52.52314532660759 0.7378299089617154 
1.9874550750722133 3.404878764647127 52.995321339880896 0.7285389483528255 
2.0209126121206653 3.487706849116135 53.473131618289656 0.7154579269792559 
2.0540621368655314 3.5653031889249323 53.87613258853789 0.7029130614156751 
2.0897525372694115 3.6528242320177013 54.37078729296678 0.6862800318295641 
1.9423923140849608 3.3242995948694998 53.2955247715336 0.6862800318295641 
epoch: 13, train time every whole data:212.59s
epoch: 13, total time:4008.00s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.78s
test time on whole data:59.53s
1.677652427629967 2.80199149976186 56.80223136273197 0.8042461780779011 
1.721224875385474 2.861778294432001 57.9158236862828 0.7956053350328304 
1.7752696221328916 2.991902359865343 57.27666048072878 0.7816903980999931 
1.812302642302765 3.0684678356112296 57.126849116696995 0.7748253482863435 
1.8640359004250773 3.193923403074496 57.04510541553749 0.760313543126623 
1.9145195689580092 3.297284313550917 56.938506418049414 0.7471290772711968 
1.9644862291748149 3.3974087455609694 57.15269784367369 0.7326341106069921 
2.019702616916792 3.500230912040157 57.56800119911455 0.7192694305345312 
2.076989239124315 3.6090568290930256 57.69081612230905 0.7072069641797076 
2.1378255886966806 3.725975083706266 58.54059661737131 0.6953641458847578 
2.1818038057296403 3.8147566966961444 58.83559163962859 0.6871996657048264 
2.223690462641595 3.899465947315386 59.488508058474196 0.6756908247717142 
1.9474585815931684 3.3657778197216555 57.69847522877498 0.6756908247717142 
epoch: 14, train time every whole data:212.60s
epoch: 14, total time:4294.41s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.81s
test time on whole data:59.63s
1.6351371054126038 2.757698654481181 57.33927473891038 0.8079537593631712 
1.6783076901968037 2.811966716275535 59.14872175199205 0.7987111697277359 
1.7262348092109674 2.9107300376777716 60.73369793290823 0.7835951441841866 
1.7631134960968047 2.9848233906719894 61.27230694806588 0.7736259804083501 
1.823634689758044 3.119286614629161 61.77489989191748 0.7557303098806213 
1.8850427002818988 3.2471539905671833 62.24120953301392 0.7339930975802014 
1.9338466011255624 3.3698503335860357 62.55631680962267 0.712966524908495 
1.966719104165566 3.449893256362491 62.484548316278996 0.7011577416942962 
1.9907205981785165 3.4965512736718725 62.29825294559064 0.6965537665763769 
1.9909289234476608 3.492242366880845 62.03467855326228 0.7039703194099708 
2.005312072299953 3.5218521193159855 62.31897870732358 0.7009442427701252 
2.051289201480026 3.6222619171399955 63.419844069591235 0.6795358597509716 
1.870857249304534 3.245137937322345 61.46864708384634 0.6795358597509716 
epoch: 15, train time every whole data:213.06s
epoch: 15, total time:4581.78s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.84s
test time on whole data:59.56s
1.6166737938423952 2.62746098047155 56.42986555152806 0.8211394906631575 
1.6567608864035102 2.71158820441646 57.67353749253942 0.8083691492648629 
1.692121540218769 2.797139072696171 57.04023567542005 0.7986536104104923 
1.7160557508000305 2.8599160359546985 56.49846516032494 0.7938637752904626 
1.7580510140384238 2.9611278111065245 55.84589007348214 0.7857743419001797 
1.7897935556317015 3.0270114390809764 55.10957458913591 0.7828470575728388 
1.808064069872456 3.044328302212693 55.32845535499271 0.7782493899228216 
1.8022454548627138 3.008360477843308 55.610936729856874 0.7789312011212156 
1.8049327753660758 3.0061889047941555 56.213817792813394 0.7744883491211056 
1.8123307885323607 3.019568717618903 57.027624485154824 0.7690934910898092 
1.8289411945441472 3.059289656107311 57.82129650671609 0.7616922368988888 
1.8693823149465911 3.1713526565553267 58.4071614540488 0.745021379944755 
1.7629460949215978 2.945056735520554 56.58390024349017 0.745021379944755 
epoch: 16, train time every whole data:212.86s
epoch: 16, total time:4869.07s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.67s
test time on whole data:59.42s
1.6751421137413098 2.713640094202874 56.74300609951156 0.8147794822199165 
1.7147399497774563 2.801516979150478 56.80650108816101 0.8022860198156997 
1.7671251662152687 2.9343670678183624 56.04043253122077 0.7905276289700782 
1.7963740443762923 3.0065609333966097 55.541741111292986 0.785786783906338 
1.8293163069502583 3.0663997863110066 55.78712285895 0.7790422973134953 
1.8454291755447962 3.086047630548719 56.30526162597835 0.7769616897941349 
1.850279420851863 3.086874610075593 56.97837556823855 0.77201197783953 
1.8638101652536896 3.1069785213137227 57.65646301577267 0.7679047414332294 
1.8795794821846343 3.1479274348981305 57.9931624234423 0.7633553926897519 
1.9134266534209075 3.219235612435402 58.53897399714286 0.7575914780670234 
1.949503050407661 3.3079302497500365 59.1624742366055 0.7449028457579985 
2.0095810854767584 3.4561542726466 60.00759894121429 0.7209406839467959 
1.8411922178500746 3.0839800671547857 57.29681646741835 0.7209406839467959 
epoch: 17, train time every whole data:212.74s
epoch: 17, total time:5154.65s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.77s
test time on whole data:59.51s
1.6486415001998345 2.647693353160814 56.91336426389086 0.8212970439614974 
1.695480814018065 2.776616229600876 56.417200729779225 0.8077038706071987 
1.7465959736856498 2.9159071151605263 55.535174039866774 0.7943139554721144 
1.7965287083800705 3.0159001042951585 55.149540155010065 0.7834415741779696 
1.8432831267883025 3.1291295812503495 54.763571482170626 0.7679885620642373 
1.8657618706393988 3.1591920357993675 54.74772072768277 0.7621115888021623 
1.858428919993607 3.110496083238257 54.9602467441211 0.7631026908484837 
1.8535765610489816 3.0703413684397343 55.348265689594164 0.7628856575750221 
1.8624705390704885 3.091821487714307 55.71028972281739 0.7546004906440833 
1.878564787199632 3.134813328185407 56.00658569649308 0.746428894960056 
1.8989594881743903 3.192973722883101 56.14318083611413 0.7383530552179518 
1.9461133644524076 3.316894646869138 56.49492303380084 0.7185202671062554 
1.8245338044709023 3.0519865086530813 55.68249788222106 0.7185202671062554 
epoch: 18, train time every whole data:213.04s
epoch: 18, total time:5441.56s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.74s
test time on whole data:59.52s
1.6215108125966398 2.6053389883547986 59.69119333299881 0.8243343614186154 
1.6569083995874083 2.6933680641515467 59.767218406120584 0.8133692423307235 
1.6916547830475583 2.792276453912127 58.658655675674076 0.8035836742615047 
1.7121612914138074 2.8349038379135187 58.42439477866181 0.7991487811587825 
1.739460262302664 2.900331185164328 58.75387384581375 0.7890616785712102 
1.7736446597387217 2.976894684949543 59.23308686501856 0.7781494971743801 
1.796482666889649 3.017970694056415 59.839279476718986 0.7735032429944619 
1.8276710386776498 3.0723848684121333 60.164011235207546 0.7663108201419395 
1.87126778799597 3.1828074325041324 60.800184466542476 0.7496464632237341 
1.928027630892183 3.3049858059798143 61.305468010837814 0.7342616757237496 
1.976656364200016 3.4181157137859524 61.429769426904855 0.7213133168527849 
2.0414327504936427 3.5734894963582944 61.91037713118747 0.6962871866803724 
1.8030732039863258 3.0441688208882214 59.998174067969 0.6962871866803724 
epoch: 19, train time every whole data:213.18s
epoch: 19, total time:5729.08s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.77s
test time on whole data:59.54s
1.669957652010201 2.6166407819030293 60.657724310934036 0.8205089195256194 
1.7016043394293991 2.6888174158423395 61.94688537855227 0.8095261310051186 
1.7410546143799133 2.784430157068497 62.61719520863595 0.7954364316306677 
1.7569598871756877 2.8032427341458606 62.69105576368611 0.7939493112446991 
1.7835851900522552 2.8531706486918926 62.99810392820392 0.785814701593828 
1.813674719685866 2.9222852438901006 62.968166540476965 0.7731936873491025 
1.8434412862169778 3.009298965799641 62.62474516942452 0.7580077833559118 
1.8769173079986303 3.0943237256103755 61.96027648214074 0.7440488451631748 
1.9066012195308826 3.180121370934405 61.111490566948575 0.7304370889559518 
1.923589581887903 3.246411106312462 59.73669959086399 0.7226364798034943 
1.9390200514981435 3.2869234247499746 58.640052678740176 0.7206470096449884 
1.9638168727143535 3.352648756538387 57.82627828785668 0.7152615103581179 
1.8266852268816844 2.9957650507980027 61.31484869375411 0.7152615103581179 
epoch: 20, train time every whole data:212.86s
epoch: 20, total time:6014.76s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.78s
test time on whole data:59.50s
1.6071379121031968 2.641980679427638 56.863629941218775 0.8257374728993745 
1.6520434422555956 2.7421291927728104 57.739959876357915 0.8138746868561938 
1.7044307210895986 2.876683946404387 57.349403294704004 0.8010351687515916 
1.738467705643248 2.943586561268635 57.66648052853015 0.7945003473717233 
1.7635725548862524 3.0035934907497466 57.46743865449664 0.7901505001776534 
1.7743004514583873 3.0135451036817313 57.50465438951442 0.7894192779584362 
1.7777738838499146 2.999584041112752 58.192623183772255 0.7874769854304102 
1.800449068523766 3.0307408813173944 58.851494952042685 0.7791959941166022 
1.8423824952047851 3.1185203222366877 59.788814383910264 0.7668141814177868 
1.9028868076224768 3.257517549196816 60.487225029376546 0.7484599803957169 
1.9565848264361598 3.38409390454151 61.05129533254969 0.7304706339061476 
2.0173763855516555 3.5219081101462826 61.44710941824872 0.7078378893703482 
1.7947838545520864 3.0539289901847573 58.7009171166769 0.7078378893703482 
epoch: 21, train time every whole data:213.34s
epoch: 21, total time:6302.55s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.79s
test time on whole data:59.51s
1.5919606894503924 2.5962415773023944 55.132375413972625 0.8321468094969021 
1.615736522929743 2.6503204708804295 55.7710716189323 0.8256353850613022 
1.639457422213895 2.708242058297644 56.03523245718769 0.819707165081715 
1.6687772876130682 2.7768797649473766 56.80013337470998 0.8102059637310834 
1.7115854997522242 2.869480513285098 57.317014174333735 0.7988798132063968 
1.7579139786510773 2.9797555209500457 57.86522691444102 0.7834926011132598 
1.8003468708159136 3.094282172767879 58.31686009319563 0.7642387474414998 
1.8308766025456467 3.1544966970265897 58.25960716172466 0.7530420038318836 
1.8326261476148806 3.1655685135979694 57.841715351058234 0.7486550364576958 
1.8300915020548163 3.151945280610753 57.93323494753106 0.7472451834460844 
1.8196246465957235 3.0975943411971385 59.152767064176146 0.7517029461997724 
1.85319518084558 3.1573649754768054 60.62217746997594 0.7376760744071665 
1.7460160292569133 2.957592285340043 57.58736094570166 0.7376760744071665 
epoch: 22, train time every whole data:213.22s
epoch: 22, total time:6587.82s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.89s
test time on whole data:59.66s
1.6704784534060884 2.6241432691719635 58.612111471106296 0.8292973162001653 
1.6984936745065664 2.6953243852139344 59.42576782671577 0.8197930373040834 
1.7345906050239823 2.7847555822760297 59.28232293055628 0.8115266678700573 
1.7682158726625854 2.8426612288770503 59.56798705499351 0.8074399853667994 
1.8065019644665576 2.921985883678624 59.41784909001736 0.7988108469827879 
1.8260357268576821 2.969757427872614 59.00781046238487 0.7928821074424328 
1.8310052257285765 2.9787385268450395 59.00213244481206 0.7898490309931137 
1.8304398843820784 2.9655170342926565 59.043633943815834 0.7891950331484336 
1.8469300280027978 2.992773090452152 59.78711665560542 0.7839292221246243 
1.8830179568249732 3.0617576150448054 60.596735200368315 0.775406512715504 
1.9328934787502956 3.1522826545495226 61.5165654783678 0.7649152680008346 
2.0017108233459293 3.294908167593395 62.50640721155233 0.7450935836843686 
1.8191928078298427 2.9458371668328716 59.81390401524902 0.7450935836843686 
epoch: 23, train time every whole data:213.23s
epoch: 23, total time:6874.64s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.86s
test time on whole data:59.63s
1.7643836239404267 2.93255801173404 56.4855556838931 0.8248631422485686 
1.7786002214421475 2.943255390120457 57.46859347976342 0.8181489637034359 
1.8091628247479066 3.0113453889645303 58.349320387045346 0.8126205911663114 
1.837358353086969 3.057003703578732 59.36699466118364 0.8099161133546876 
1.8695789850113824 3.1226844709797517 60.00895736621297 0.8045468258500853 
1.8965458247222537 3.176786881862669 60.75770078344714 0.7986387881042037 
1.9184235441784951 3.218292404928633 61.56930399584499 0.7908166415479466 
1.9351303294469557 3.245266388695148 62.19833309684171 0.7824990456192368 
1.9584124123149862 3.2923311348673963 63.11850486819834 0.7699427434227087 
1.9905207629662363 3.3448366609260454 64.08147496289922 0.7585033424838121 
2.0130971578725925 3.3746921771618315 64.87053425704977 0.7517255481488605 
2.049349488621489 3.4600506289672213 65.38360873960968 0.7379505874804786 
1.9017136273626534 3.1858579280128136 61.138400798945966 0.7379505874804786 
epoch: 24, train time every whole data:213.35s
epoch: 24, total time:7161.07s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.75s
test time on whole data:59.44s
1.6446133879173015 2.629926870910644 53.589180494024355 0.829015493164674 
1.6809946526720942 2.735536839840595 53.443409440618495 0.8148009434994912 
1.7128599688997048 2.807449346514265 53.5415516247972 0.8062684687554352 
1.719882591975498 2.821948420332167 53.499236909147044 0.8032670583206516 
1.7322262942100033 2.8352359548337662 53.84496842542564 0.8014904461721568 
1.741127311690666 2.854864952237471 54.34560469411611 0.7968125577669962 
1.7550808129458733 2.895554984421004 54.84990529822865 0.7893172812179776 
1.773319576963605 2.937374561757212 55.394654648033594 0.7823123314909268 
1.7968228971710696 2.9879363712920366 55.97139782155035 0.7761982195442463 
1.8353809139113872 3.0819437575743382 56.66451105013206 0.7647227171387787 
1.8675010926381996 3.156572793451604 57.27590740955605 0.7556162115344749 
1.9089985741680222 3.2547534506207363 57.99007259015902 0.7380071169787025 
1.7640673395969522 2.9216088543829883 55.03427963993289 0.7380071169787025 
epoch: 25, train time every whole data:213.36s
epoch: 25, total time:7449.68s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 35.89s
test time on whole data:59.68s
1.5941944682127132 2.5960330280118042 56.20294014304529 0.8299072911881362 
1.643785804607861 2.709418905970801 57.009598322837405 0.8171040172682059 
1.6805933677312874 2.788625705452966 57.10249857554779 0.8090247191184586 
1.695621049867588 2.8151896546284036 56.84647216787899 0.8071461836894663 
1.713401605914125 2.8573377277908314 56.51014344083757 0.8026657368458506 
1.7262418718436467 2.887256581060984 56.538665930745935 0.7971698037441141 
1.7334254146674204 2.898328871653174 56.547577873773655 0.7945078517336328 
1.746783512154328 2.9265942586659324 56.51632170449389 0.7896175697633535 
1.772931692718484 2.994313517035834 56.86215515160424 0.7802714732593262 
1.795555273466788 3.0494095901295246 57.2322319086813 0.7746662986744678 
1.8186073025699172 3.1154039644340097 57.60959968865047 0.7665946795781751 
1.8626805512505982 3.2301316009112124 57.726036038614595 0.7521311840697316 
1.7319851595837297 2.9105019952656694 56.89203022656656 0.7521311840697316 
epoch: 26, train time every whole data:213.26s
epoch: 26, total time:7735.37s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.04s
test time on whole data:59.84s
1.586041959143022 2.6054690895140253 55.29001571627546 0.8329727451433828 
1.6083199939945092 2.6520588158397413 55.88043505821398 0.8241372765451493 
1.6223379449515294 2.659762329564041 57.00407646970655 0.8213350497537197 
1.6296517066243326 2.672248699482854 57.285475791476145 0.8193926870068473 
1.6463996998772379 2.7083688550594367 57.649306845367974 0.8151395300054856 
1.6685302536913327 2.753051680607203 58.00649682139121 0.8092797294493452 
1.6828267107332746 2.7824638973706377 58.32538750835947 0.8060561839231822 
1.6974638705944554 2.8130304072432453 58.11992428402301 0.8023557098052106 
1.7194314423899744 2.862904579403127 58.214116577268186 0.7958486019180008 
1.7453130869415723 2.911968645592601 58.63030828187693 0.7902233004718612 
1.76060533533813 2.944863032223975 59.330955454261606 0.7858942025322239 
1.7989157878064683 3.0391620622970055 59.696863893558735 0.7724001984057985 
1.68048648267382 2.786746057490781 57.78618183240609 0.7724001984057985 
epoch: 27, train time every whole data:213.41s
epoch: 27, total time:8023.06s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.86s
test time on whole data:59.66s
1.5754563741483505 2.5724256129606156 58.6471798190061 0.8304759177574524 
1.6058065929813754 2.6587691872162993 58.15830164076762 0.8210846925384846 
1.6306991702473412 2.723264583141967 57.758527597580944 0.8160516912015238 
1.6537845582061759 2.7759511796772354 57.65583148640437 0.8116387048367245 
1.681187984621507 2.830759781854026 57.711938691974986 0.8067025122984779 
1.7087488289483423 2.882895376999066 57.59628711170183 0.8007194274904073 
1.7309339647280673 2.9296287153141343 57.41467907234217 0.7966437948395781 
1.7513044067239831 2.97175702295714 57.16450965858677 0.7920683663783021 
1.777494240082623 3.029716916232762 56.95799587536935 0.7869173974150071 
1.8104636775162959 3.095186800682363 57.08297903423875 0.780247041185791 
1.8447657274406049 3.156110588861507 57.711438667110514 0.7740074679704685 
1.8958875876966685 3.266012752677693 58.07010975725257 0.7635998761697609 
1.7222110927784446 2.914533871402493 57.66079518921923 0.7635998761697609 
epoch: 28, train time every whole data:213.19s
epoch: 28, total time:8310.60s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.83s
test time on whole data:59.62s
1.8137286848050675 2.6485513651133905 68.95685522408913 0.8221031316163354 
1.822098586002276 2.738570541040772 66.82341952199806 0.8075396061621182 
1.8381173868496858 2.818936250420658 65.31596169532939 0.7955454494390495 
1.863215224378432 2.8815763078511334 64.56247732589038 0.7854702794035132 
1.8852737865086113 2.9356639788066916 63.83989358334608 0.7758829785283512 
1.9134378219836703 2.9974815668758814 63.7837868614229 0.7620556194867043 
1.9362825490316997 3.069651734609815 63.839855949512916 0.7471497895561711 
1.9592883426727994 3.134263587223146 63.860453640212214 0.7348780253249217 
1.9728952352897753 3.192962986664932 63.12823726972263 0.7258877432020031 
1.9900406761643965 3.2549015292179413 62.273981988434635 0.7185082198949782 
2.0094321111412277 3.3097170619776186 61.711972856950005 0.7136216379759035 
2.050131280106714 3.411873249110592 61.77232283441032 0.6943042828394888 
1.9211618070778629 3.041358906633056 64.15566261068582 0.6943042828394888 
epoch: 29, train time every whole data:213.26s
epoch: 29, total time:8598.78s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.92s
test time on whole data:59.84s
1.6674791405340212 2.8088254491657265 53.75243143958588 0.8375776873138158 
1.6949925987680576 2.8558670896427363 54.50221350177395 0.8319560722141776 
1.7311959975532123 2.909420828286772 55.48350865299827 0.8262954087286887 
1.746449405397599 2.928466234166769 56.2058016005026 0.8212518786194075 
1.7806139814925512 2.9995207699917557 57.00546071836064 0.8127156503826022 
1.8269476402754052 3.100097499887658 57.754629680680495 0.801719845907472 
1.8817282212993929 3.2080443802582694 58.76225425068636 0.7909137849460646 
1.923895680089082 3.2872518691433736 59.460367981493654 0.7812768332085681 
1.9495914505874472 3.330386388947136 60.104799302587764 0.7734331447204409 
1.9616393110761863 3.339215845608458 60.637565859531975 0.766874458677329 
1.9603172492057617 3.3160750741854557 61.38345802621045 0.7630203071937142 
1.9813855823069102 3.3715739315002518 62.08666193651909 0.7464369074592728 
1.8421863548821356 3.127793749461928 58.09508381226151 0.7464369074592728 
epoch: 30, train time every whole data:214.04s
epoch: 30, total time:8887.41s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.09s
test time on whole data:60.07s
1.5616701651918037 2.5286466858332735 56.767318907158135 0.8364011170704774 
1.5857562437957773 2.5887551490182252 57.04084918045557 0.8285725819941957 
1.6070854145963454 2.6505799300758146 57.04261912653428 0.822353013733847 
1.6223581873602455 2.6885985809088835 56.85432558378438 0.8201565948310051 
1.6562349854377763 2.7789575933172403 56.952961126882705 0.8121179568229381 
1.6921765865082188 2.8522617557380654 57.388974619029256 0.8044521547593466 
1.7370882482303396 2.949067743459522 58.21165484860727 0.7918332198921051 
1.7820262426412 3.0446642781191704 58.755437591007734 0.7778631654617653 
1.8217646181700484 3.136416442723548 58.988287874916686 0.7639920751783812 
1.860537677403983 3.219540262887909 58.93015927405625 0.7511678477137173 
1.8755639718831294 3.2478155873586076 58.90583238575297 0.745444241758629 
1.9007693602883566 3.3016973601032813 59.09029328010684 0.7330628038776662 
1.7252526417922687 2.927269579356976 57.91078023022933 0.7330628038776662 
epoch: 31, train time every whole data:213.88s
epoch: 31, total time:9175.70s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.23s
test time on whole data:60.29s
1.5745730030631557 2.5473409391019595 58.22910272066347 0.834797003301061 
1.6033698557164697 2.616367460690266 58.43523410772351 0.827343577267824 
1.6348948817839402 2.687142622932852 59.0960993195423 0.8194761242631772 
1.655678550179338 2.718145830432773 59.68296550866744 0.8159097844932346 
1.686604400877619 2.7684656584420546 60.70910040056605 0.8087935439129432 
1.7159291753535646 2.8209604249430202 61.49069492209958 0.8019094150246248 
1.755053469754844 2.909120656198778 62.262166924818665 0.7909611919750457 
1.8010781914628156 3.0106401963511127 62.88494616733774 0.7780647517870066 
1.8613743381196899 3.1506170814288583 63.592442573379635 0.7577266559736607 
1.920417836965993 3.2843464875447212 64.04200562441955 0.737263397185227 
1.9600798088544535 3.3671718170316165 64.37351061186473 0.723313820852345 
1.9977312170767358 3.4551770367948667 64.49234378805096 0.7081042700473426 
1.7638987274340516 2.95919398135125 61.60768654571416 0.7081042700473426 
epoch: 32, train time every whole data:215.04s
epoch: 32, total time:9465.45s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 36.02s
test time on whole data:59.92s
1.6089562508550783 2.5273090990683467 57.35258471779314 0.8342407204592102 
1.6319300107484949 2.575091526449619 57.819999073762375 0.8276871119590139 
1.650792972826532 2.614042916234904 58.26154121442505 0.8224605014943218 
1.6672972324423136 2.6613965164449613 57.9768917445315 0.8168745884596789 
1.69664592398029 2.734550512285627 58.03600023864941 0.8080157833855268 
1.7196009324364188 2.78287820745251 57.999122986962426 0.80183948789324 
1.7391659352763422 2.827503597714168 58.092557249915664 0.7948693128671432 
1.759947473567334 2.8713571176446804 58.12043582026617 0.7873328140450854 
1.7851609567927995 2.9319419351410834 57.884251721711244 0.7783863957499618 
1.8134319345843757 2.999106304424817 57.51566806727525 0.7690060917912396 
1.835793471577533 3.050894456354229 57.4323404504448 0.7616872934537128 
1.8728800021045442 3.1425604980819246 57.53589271305985 0.7481480472269042 
1.731800258099338 2.8161861171268776 57.83560723133556 0.7481480472269042 
epoch: 33, train time every whole data:215.10s
epoch: 33, total time:9754.71s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.49s
test time on whole data:60.69s
1.6207192011145608 2.611554365835722 54.99575251372192 0.8397289748846667 
1.661282143816884 2.6981701043512865 55.91329253702714 0.8299286909369721 
1.702841179224618 2.7936262793342532 56.54973432210978 0.8217496066786864 
1.7400023361113632 2.863480180408139 57.137320145351545 0.8156889102320697 
1.7890881825740494 2.9686904805996246 58.01937452593775 0.8058419350132667 
1.833770172771155 3.0553815797720367 58.91946268624745 0.7959269892507433 
1.8670923711089862 3.1260492748648456 59.941473585157524 0.784783881203032 
1.894396786951593 3.181631210237326 60.95100870357245 0.77342229267388 
1.9182933729050593 3.242047444936879 61.88602490381284 0.7606916477943471 
1.9431254499099617 3.2867252934738627 62.7370025064109 0.7525540151976141 
1.9575757369658067 3.2980927755016154 63.64162031144814 0.7485684219375297 
1.9834163214683178 3.353404849242378 64.1402156602826 0.7391389313853604 
1.8259669379101964 3.049369066887458 59.569529157802414 0.7391389313853604 
epoch: 34, train time every whole data:215.39s
epoch: 34, total time:10045.50s
predicting testing set batch 1 / 168, time: 0.38s
predicting testing set batch 101 / 168, time: 36.69s
test time on whole data:61.16s
1.5549448608875807 2.502763386950966 59.25576127207315 0.8383504019592934 
1.5753823705241972 2.561095865538475 59.0127379886042 0.8307029381842614 
1.592438956742929 2.6201481509802225 58.359587396993476 0.8240114256691473 
1.613165905762996 2.653539698139409 58.07063283629921 0.8222273794229473 
1.650023479058895 2.747413390356915 57.85943575369957 0.8130363367910216 
1.6818969086672046 2.818909512219356 57.91705265970188 0.8049948140550541 
1.7057109036952967 2.87360205600144 58.30418565789357 0.7949427261068928 
1.7213126099038691 2.8966077608112015 58.701385200949005 0.7892735479172382 
1.7396167440480952 2.936774022932379 58.951669012413724 0.7824625242143962 
1.7564947570165885 2.965791154057747 59.06683396759327 0.7784068749541354 
1.7660444939615472 2.9835307524858368 59.36294583250971 0.776486194546754 
1.7988182665028032 3.059371566510472 59.96648379575493 0.7655129945786951 
1.6796541880643334 2.8070404239804954 58.73573586845663 0.7655129945786951 
epoch: 35, train time every whole data:215.90s
epoch: 35, total time:10338.22s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 36.62s
test time on whole data:61.00s
1.6216371431519232 2.5099139424557353 56.334452269905924 0.8363556523111642 
1.6487026827403655 2.569728925141869 56.94725691281971 0.8278559123537733 
1.6744468383067952 2.6398361024598653 56.7951379149236 0.8178930099467521 
1.6890006108814406 2.6743943408245694 56.45857565746007 0.8138604617993742 
1.7141609128848427 2.736168481383326 56.30592348217414 0.8064857108627168 
1.7366520678679502 2.80764639451 56.27782766818873 0.7973062393705007 
1.75764600178147 2.8642858058044403 56.44068938911986 0.7902419327082237 
1.7767728897098096 2.9088879196582034 56.581806575081174 0.7843010674147791 
1.8052000781380173 2.976721768356913 56.830546497501814 0.774434073961699 
1.8379089111370877 3.061917101628082 57.11533884327196 0.7604962624953037 
1.8596296375945565 3.0976144163003267 57.552348292185926 0.7536133644115645 
1.8844866751943317 3.14960347358344 58.01950505262159 0.7441982912789825 
1.7505203707823827 2.8403488454068615 56.804964257329104 0.7441982912789825 
epoch: 36, train time every whole data:215.67s
epoch: 36, total time:10630.19s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.79s
test time on whole data:61.24s
1.5814757001094875 2.5367586766399848 54.446870692819616 0.8393161405290392 
1.593864456159462 2.5631228156682555 55.851038725720144 0.8320799733039222 
1.6070614508487995 2.5750260697564222 57.30120124294535 0.8283152486800407 
1.6175176947119512 2.5977861210963 57.678988951013096 0.825139339129719 
1.6343518597753275 2.639956415253194 57.871586744849836 0.8200117910185928 
1.6595726886976156 2.694986198573212 58.34953052005915 0.8128572356289464 
1.6860418445407634 2.7553963763601863 58.95808433460189 0.8048756339897332 
1.710617497280861 2.8095570450486287 59.33828209667639 0.796949135531563 
1.7372551153517728 2.861721755004769 59.54361140750045 0.7884064953866339 
1.758437369345377 2.896848394443872 59.64359704066682 0.7819515023018365 
1.775255394438725 2.9216708002499394 60.49573653693473 0.777060293997183 
1.8011988498116178 2.975574478362212 61.496066688309526 0.7670637840606552 
1.68022082675598 2.7397311512346167 58.414654292934664 0.7670637840606552 
epoch: 37, train time every whole data:214.82s
epoch: 37, total time:10922.16s
predicting testing set batch 1 / 168, time: 0.39s
predicting testing set batch 101 / 168, time: 36.77s
test time on whole data:61.02s
1.5616325308730206 2.543609997725282 55.091851387357806 0.8408498052284916 
1.5992217561704594 2.6386920856122194 55.26829575517096 0.8335363288061737 
1.6456937878270235 2.7487199411745897 55.61337319472213 0.8250442604858688 
1.6647767747222846 2.77687315563776 56.02512531657729 0.8216670860819041 
1.6932419038442452 2.8242793712105603 56.75732872653657 0.8149680424014993 
1.7224784817452587 2.8718129645917467 57.41512842845137 0.8089037336724442 
1.750207328981587 2.923843970382058 58.12308134815438 0.8025403824278039 
1.7741985073204019 2.9778653617831963 58.8320352593546 0.7949573595104821 
1.806424277419047 3.042903824978923 59.74682658588544 0.7853184727313977 
1.8333058118650778 3.0892120248695174 60.54679374753283 0.7785094418967039 
1.8599786380824765 3.1199109484171057 61.52517737180728 0.773753553039912 
1.9001864462477998 3.192944627713627 62.31027909033414 0.7636679391639362 
1.7342788537582234 2.902165531566779 58.10473990305904 0.7636679391639362 
epoch: 38, train time every whole data:215.59s
epoch: 38, total time:11213.01s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.07s
test time on whole data:60.00s
1.5545541099589317 2.5467986844614017 55.041595737570624 0.8378308876732667 
1.5882112113802382 2.6185633805378377 55.556620322423065 0.8289408554218914 
1.6233599071995843 2.7110460489111565 55.73871519532907 0.8204045622229693 
1.6554892287986973 2.783887280209959 56.118758010132765 0.8148499812216716 
1.7085293551930891 2.91746562586783 56.37764539055045 0.8049062583145987 
1.7620561164076485 3.043917795347376 56.884141932864665 0.7926653637828991 
1.8186121629599836 3.1690741594481464 57.57028999068168 0.7798977417017962 
1.871643555522231 3.281086833270971 58.25367890447727 0.7654882300289403 
1.9198031129758983 3.377021890550122 58.770590704541625 0.7518769010522739 
1.959533358570543 3.4522682677788796 59.1739077645109 0.7403130083739463 
1.9758893189124231 3.4657416697983234 59.602543935195676 0.7350667475189876 
1.9963695354105107 3.506694300433114 60.31238707999277 0.7206912305373483 
1.7861709144408149 3.0911050225368824 57.45016783592612 0.7206912305373483 
epoch: 39, train time every whole data:215.04s
epoch: 39, total time:11502.66s
fine tune the model ... 
epoch: 40, train time every whole data:447.57s
epoch: 40, total time:11950.23s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.18s
test time on whole data:60.32s
1.5228356673419476 2.4886930931364355 55.21303862959231 0.845743277737424 
1.5481173498909921 2.5442213645457255 55.88654570989918 0.8393444249853481 
1.5734329289791307 2.6053569194734716 56.42171742069154 0.8327592618555107 
1.5929085278360262 2.645161314704608 56.596974004275445 0.8293103747407103 
1.6217413988915228 2.7060916981407894 56.93681875089999 0.8242952204139025 
1.6534196273463646 2.7736723016129545 57.401071741652956 0.8168654392757063 
1.6848575437516862 2.8410480284452895 58.032724970745534 0.8088645179735567 
1.708950666732554 2.8945792274757878 58.56873817790375 0.7998033226651837 
1.736736740800419 2.954677039269953 59.12467227795777 0.7896944020489086 
1.7611138002954068 3.011257544867139 59.52577285629087 0.779994855431831 
1.7767895902271071 3.0316916112108556 59.96806541179915 0.7761091641717048 
1.806139181421715 3.101391655987858 60.39439061729425 0.7648738856284945 
1.6655869186262393 2.8066190288772184 57.83930401297251 0.7648738856284945 
epoch: 41, train time every whole data:446.55s
epoch: 41, total time:12470.76s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 36.14s
test time on whole data:60.04s
1.5172718273757824 2.4691826627111944 55.58378691742926 0.8471616657206944 
1.5436213640291243 2.529313884941403 56.083231485219976 0.8402194454660751 
1.568984579420693 2.5915999382054338 56.42839674102782 0.8333477826823563 
1.5884561630130878 2.6315926690714355 56.526714334335715 0.8299837295041518 
1.6155292961500762 2.6877871124566024 56.76795646829493 0.8251871795799073 
1.6443694177481036 2.7479899985501572 57.155203593419856 0.8189617878021419 
1.6734142253622413 2.810994571792783 57.742339097431596 0.811856280335604 
1.698527999964587 2.8669540440276196 58.2975317594168 0.8030203452622843 
1.7288773742162047 2.9324800917534337 58.950410692657144 0.7930761031100817 
1.7577226316315964 2.999298347068533 59.50652155911197 0.7825598636586002 
1.7812137293610721 3.036813137964629 60.191435336256646 0.7770687527106328 
1.8167037206505914 3.116120550672579 60.77056482014065 0.7658252776442405 
1.6612243607435966 2.792257120409624 57.83376240092078 0.7658252776442405 
epoch: 42, train time every whole data:448.06s
epoch: 42, total time:12992.69s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 36.24s
test time on whole data:60.42s
1.5152640598601705 2.4501575924410326 56.5645168956842 0.8477902133859827 
1.5419708447120197 2.51096411237985 57.05447290904989 0.8405303327401195 
1.5648521351185405 2.5668870404553563 57.51399764941366 0.8337620649605397 
1.582307409819925 2.601647847473557 57.65780447877377 0.8302953673492981 
1.6064182626170416 2.6483167327266077 57.94685200333275 0.8260539012102552 
1.6360125970603632 2.7160022516901945 58.466618068576956 0.8180268111060739 
1.6676064499216599 2.785825871495348 59.19210067040586 0.809313309234331 
1.6980354698114097 2.8542703212856115 59.88078231887606 0.7989151986141357 
1.732425619738886 2.932259812408965 60.53417440634771 0.7872957158255119 
1.763682993253693 3.0072569230761874 60.910605400721394 0.7761551645129748 
1.7873697355793168 3.0464275602560287 61.332227030234854 0.7710866037004509 
1.8191831059501995 3.1188069671618095 61.71665710620333 0.7607096362597721 
1.659594056953602 2.778084588804411 59.06433153392603 0.7607096362597721 
epoch: 43, train time every whole data:448.31s
epoch: 43, total time:13515.94s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.01s
test time on whole data:60.85s
1.5130935993837635 2.4478239314255905 56.323543249135014 0.8478772969364281 
1.5396629684956833 2.5094935087985064 56.83234924896227 0.8404746660173668 
1.5617368693918523 2.5633158080507257 57.29708491157632 0.8337752683509448 
1.578558013015471 2.597325063960988 57.360001465949715 0.8302813230617636 
1.6026740411647729 2.643206824350798 57.63338617920567 0.8261126097563508 
1.6307292410555694 2.7063876110410674 58.054643092198724 0.8187864599983586 
1.6586452905240336 2.7683502798468855 58.6722261261655 0.8110710576233048 
1.6854918878630158 2.829092863424851 59.28641881769242 0.8014789666599478 
1.7152202124631122 2.897843398545354 59.841813054821834 0.7907190511120741 
1.7401572419664866 2.958284839575485 60.173618915281764 0.7811165991498313 
1.7586296361613842 2.987868991562406 60.611480567607714 0.7766244243290226 
1.7844316411175365 3.0462228171028634 60.96159675241187 0.7679847188588975 
1.64741922021689 2.7528844134482924 58.58743237505206 0.7679847188588975 
epoch: 44, train time every whole data:445.33s
epoch: 44, total time:14037.44s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.46s
test time on whole data:61.30s
1.5117913130298257 2.447719533538344 56.45869841968422 0.8479712934675818 
1.5396638396486109 2.511033370083921 57.0459442206105 0.8401809091170401 
1.5621336713779186 2.5666331724431353 57.60040632939132 0.8332054702125989 
1.5791422399063373 2.5986957676646574 57.73501121993618 0.8299206658117485 
1.6032313887085765 2.6448497208602233 57.98336743467423 0.8256683711246211 
1.6334702882105927 2.7143235009835474 58.3969069001104 0.8176710241563252 
1.6635310588366397 2.7806982068310697 58.904903673771805 0.8098194668604463 
1.6914067114986302 2.8446730125904485 59.36617461429835 0.8003945441193177 
1.7231889084461367 2.9174266876220383 59.85028266062654 0.7896488896717268 
1.7489895821337011 2.981551814520385 60.09995391161203 0.7801036547460983 
1.7672877290338456 3.0123214115879553 60.464277249291584 0.77621833636847 
1.7942482243688511 3.0738030884334804 60.82752464858358 0.7675311203845901 
1.6515070795999722 2.765080614086405 58.727866132958376 0.7675311203845901 
epoch: 45, train time every whole data:440.74s
epoch: 45, total time:14554.05s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.83s
test time on whole data:59.59s
1.5097499101349996 2.4503034438686657 55.746176657788574 0.8484345602277349 
1.5383979606259437 2.518307923991469 56.16265010276235 0.8408018146734332 
1.5625035159975467 2.5800405976854788 56.46972287628047 0.8341354370479032 
1.5801706452948885 2.6143305060660795 56.64141080670595 0.8308601593594009 
1.603955567357973 2.6618106472659675 56.99355767072357 0.8264825649478704 
1.6300478850902014 2.7153046239212832 57.53101295438705 0.8202526544110732 
1.6562529670086112 2.770772248728694 58.20389893285003 0.8131294782970757 
1.681708664251048 2.8242783757021153 58.87625652259143 0.8043391088848245 
1.7132731971626303 2.8943802896867705 59.555708140517524 0.7932049369212746 
1.7398568713428186 2.9602367823886646 60.051546928838604 0.7824852519572001 
1.7594270089741442 2.9902673386053307 60.59433209330759 0.7780227656724915 
1.7858686632007539 3.049829125483797 61.05230015609134 0.7693623700571838 
1.6467677380367967 2.758813650348783 58.15664663294805 0.7693623700571838 
epoch: 46, train time every whole data:442.97s
epoch: 46, total time:15071.08s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.98s
test time on whole data:59.82s
1.5134576630581702 2.466578568249303 55.31306776122915 0.848701273741315 
1.5450327930488579 2.5375163010771247 55.75532882376636 0.8412693774233311 
1.5711988228670366 2.6005959353411057 56.26340267930898 0.8343494480634489 
1.5896201028638475 2.632341510964589 56.54153324556243 0.8316057715387566 
1.6155520416187743 2.6813058418207305 57.0103987251096 0.8272793816858313 
1.6429726330721308 2.7389899230861285 57.62062420470278 0.820541113096 
1.6692613367223668 2.793504027353332 58.33988977391098 0.8133631931182466 
1.6936096520356478 2.847341292556418 59.010087253980615 0.8041255660496015 
1.7254614510039488 2.915817993275156 59.662146713352534 0.7931822031688667 
1.7523777226315191 2.9845153877970163 60.14242601565787 0.7822221102650428 
1.7728645771513027 3.0177831392249663 60.65901017644445 0.7777631355488372 
1.802806410584244 3.078666076340095 61.132441706724606 0.7694566911930905 
1.6578512672214871 2.7810718919604027 58.12097316613583 0.7694566911930905 
epoch: 47, train time every whole data:438.76s
epoch: 47, total time:15584.87s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.82s
test time on whole data:59.58s
1.5086986484661522 2.4400311356613265 56.46323396345263 0.848701476826554 
1.5346120923368172 2.495039476205617 56.86941286922058 0.8421398055533745 
1.5567491228612407 2.5511990628458174 57.11241130497443 0.835946837654894 
1.5751211859206005 2.590802190980048 57.10055287289496 0.832254941163378 
1.5995034959174337 2.640534906451186 57.317403463215264 0.8278499042665947 
1.627380733641574 2.7031139979494836 57.80500044680691 0.8205956221929881 
1.6546948309282639 2.7613183739582294 58.43305172874981 0.8134997875844416 
1.6805010075488438 2.8210720891834535 59.10865409170791 0.8040995694190675 
1.712105954369796 2.8881197001899324 59.83020490296247 0.793677665959017 
1.7395017607011611 2.9584250178243012 60.365477252661684 0.7823753824736498 
1.7604441655717258 2.99234988676951 60.90093149039608 0.7774706996952507 
1.7873514935566732 3.0518026057328487 61.352491052012624 0.7687105997683062 
1.6447220409850236 2.7480840952170906 58.55499090808027 0.7687105997683062 
epoch: 48, train time every whole data:437.74s
epoch: 48, total time:16097.66s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.83s
test time on whole data:59.58s
1.5210872695787498 2.471731153068394 55.40103625292067 0.8486822408181239 
1.5535044105960323 2.5396882782197516 55.869205644105676 0.8412394536316402 
1.5787741713088361 2.5959455511503506 56.35780803888167 0.8350225731768974 
1.5965834895885063 2.626090464756187 56.64459690256087 0.8322815851368198 
1.620804955423233 2.6725742587851076 57.111547498820734 0.8280475212768549 
1.6465683751053044 2.7232114820973097 57.74149620870163 0.8222326012860439 
1.6732650107804332 2.778050524769764 58.468551791967535 0.8153357200359208 
1.6979284820443994 2.8341400456972186 59.15188626068385 0.8063032622064895 
1.7288313684722498 2.9024413425066196 59.82337698375231 0.7955799169493456 
1.7537500651198483 2.963938620560038 60.3122809973642 0.7857756668000131 
1.772629281711543 2.9979388884050224 60.8489766571526 0.7811279974717229 
1.8002530563688348 3.054795514773291 61.378294279753874 0.7735163637271353 
1.6619983280081643 2.769351841252314 58.259200207139116 0.7735163637271353 
epoch: 49, train time every whole data:437.13s
epoch: 49, total time:16609.13s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.99s
test time on whole data:59.75s
1.5091504258820343 2.451049177099354 55.541463842706904 0.8493508599516678 
1.5391684199100626 2.519724866962852 56.09640660640177 0.8413379414572066 
1.5634146082103253 2.5804119075261904 56.65017237973565 0.8340637495134524 
1.5818187129301506 2.613291116991663 56.836070274884435 0.8310763615949588 
1.6067014247730729 2.6608339572521835 57.26604978626507 0.8265437665194193 
1.6344491088152642 2.7207654819577693 57.89579345051612 0.8191048503374774 
1.660389779905833 2.7794073407039597 58.52563010955315 0.8113884253726383 
1.6841777296180704 2.831683737527812 59.126853757790386 0.8027899084789732 
1.7120417125268528 2.8908305808255155 59.67584828963878 0.7933977505523135 
1.735439059801311 2.9502638730885056 60.12489128370019 0.7837088299039577 
1.7542715822660497 2.980600675202819 60.63631418194251 0.7794001362574682 
1.780352829809079 3.037524425242172 61.069121335038425 0.7712297712397217 
1.6467812828706754 2.7574460563788024 58.287153755940246 0.7712297712397217 
epoch: 50, train time every whole data:438.18s
epoch: 50, total time:17121.89s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.62s
test time on whole data:59.27s
1.5082985429148235 2.448271711529672 55.86848282298183 0.8488251308383431 
1.537406550921145 2.5096869458473403 56.537598487593975 0.8410984992332607 
1.557154652553921 2.5602379773400106 56.96492598139703 0.834930453174562 
1.573261960798431 2.5878319725943877 57.078290985378864 0.832316457032234 
1.594876952309605 2.6314916479935118 57.39356691813789 0.8281447652619767 
1.6207874494658872 2.6893719408938037 57.92030757501405 0.8212730027593731 
1.646100136183291 2.744384762163036 58.50623747201576 0.8145844069884424 
1.670756035105281 2.7995348371975513 59.1477761455445 0.8062457443617165 
1.7023181385726092 2.866533143354119 59.830639233482245 0.7961819997975192 
1.7303211357449846 2.936557205332645 60.38995474246818 0.785426299984722 
1.7544976577744598 2.9800448960754977 60.97970807362682 0.7795004486256732 
1.7856680940084515 3.0479370118080547 61.540827215253856 0.7698344890720556 
1.6401206088627407 2.7398560882028167 58.513291826211514 0.7698344890720556 
epoch: 51, train time every whole data:437.59s
epoch: 51, total time:17631.71s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.60s
test time on whole data:59.32s
1.5085392698767106 2.449176495520816 55.756951578728 0.8495081914939835 
1.53817701966759 2.5157242712740264 56.32923006304982 0.84152847108332 
1.5616836657328088 2.5751256549680615 56.72252678493728 0.8347163375910996 
1.5804638432614682 2.609304628765739 56.898764370739876 0.8316005641390918 
1.6033365940456057 2.6527745280108372 57.239623258602904 0.8275411979163614 
1.6292165983394675 2.7031331895816515 57.78988656656762 0.8217193163476746 
1.6542991769579019 2.7558819371764187 58.39462315771282 0.8151745200038704 
1.677564461582562 2.8091162086245998 58.98803186002557 0.8066323927598235 
1.7070443307934773 2.8742011860477654 59.563404662670784 0.796371562727061 
1.7292488090139593 2.9280000266878052 59.999870634487884 0.7878337212579124 
1.7459569169574727 2.955353022452235 60.55324700362489 0.7840368629713246 
1.7706071434302166 3.0048150530184707 61.07999308530777 0.7772216136238416 
1.6421781524716035 2.7415701001202692 58.27644145855707 0.7772216136238416 
epoch: 52, train time every whole data:436.97s
epoch: 52, total time:18142.07s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.77s
test time on whole data:59.49s
1.506014016714302 2.446237007147113 55.72391945386443 0.8494347882190582 
1.5363202692882292 2.5150647023569612 56.07584359950479 0.8417756119140262 
1.559747920131311 2.5725979504772485 56.36259637550302 0.8354453868037033 
1.5772928280119918 2.604694733531904 56.48665648747091 0.8325697729928018 
1.6016519514141339 2.6529817494075343 56.73042025683208 0.828605468240564 
1.6292325096735287 2.7139508956063345 57.211232007225874 0.8218807790072289 
1.6560979837845302 2.7720519932963104 57.76794356887519 0.8150877147600399 
1.6799282644441618 2.827131245439072 58.34376704367109 0.8062259031632335 
1.7090088887912709 2.8884537289876238 58.946226144634394 0.7962515776785588 
1.7333636146037883 2.948345783587147 59.48103830415823 0.7861456094362052 
1.7515312819422355 2.9774775305629184 60.12338558139165 0.781577711337604 
1.7754967299402646 3.0236810074390155 60.68904306410899 0.7748537474734305 
1.642973854894979 2.7513183637522975 57.82859313949085 0.7748537474734305 
epoch: 53, train time every whole data:439.87s
epoch: 53, total time:18655.07s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 36.47s
test time on whole data:60.28s
1.508242810395502 2.45425451999034 55.25947953651289 0.8493841106745152 
1.5407602498214692 2.531842688955328 55.58866193934833 0.8411971659608009 
1.5660423515645698 2.5933635482305 55.979161635903395 0.8348616818661105 
1.5838022933502105 2.6239736121986375 56.21750159390329 0.8322885311267907 
1.6086314927053948 2.672595828627644 56.68453850016261 0.827833080531794 
1.6351206896193327 2.7278736352630792 57.36270821221434 0.8212724969528401 
1.6607183102366647 2.782809677040973 58.11788732504368 0.81396631989351 
1.683380728079627 2.8346494383410428 58.83892775353188 0.8049642273257579 
1.7131717848142698 2.8953919014453753 59.57081313576127 0.795141343963561 
1.7371847740120832 2.9543360101447096 60.138904022596805 0.7855518445594534 
1.7550072287000007 2.982700489306476 60.741960813781304 0.7816056792541529 
1.7797742858604484 3.030685005372214 61.27197234327162 0.7751277346856265 
1.6476530832632976 2.7628584860292595 57.98115721538636 0.7751277346856265 
epoch: 54, train time every whole data:439.81s
epoch: 54, total time:19170.13s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.72s
test time on whole data:59.38s
1.512195320709032 2.4648421866908734 54.94412006941386 0.8488629288780469 
1.5432810303112936 2.530609193342393 55.37419041394626 0.8416079329564637 
1.5675876211105357 2.590216681728653 55.74561578931391 0.8347833091836332 
1.5846941797507128 2.620990841527754 55.97168504932559 0.8318110838127192 
1.6078007440587416 2.6647930791384393 56.43540514986688 0.827441141018487 
1.6306987836061135 2.7119638927246816 57.100818402327604 0.8213279357299278 
1.6509432922382616 2.7533675991747124 57.784437839158734 0.8154689843930338 
1.6705141201720883 2.7980875026357546 58.47109502800751 0.8072943890997295 
1.6982838460076601 2.854855418979447 59.10917267597869 0.7980238537319633 
1.7213867292998448 2.911342865376408 59.603303573455094 0.7891117734745792 
1.740916286335726 2.945900967632986 60.09434801964735 0.7845940546720945 
1.7669859901301208 3.000770434975976 60.58051386539112 0.7773173709271572 
1.6412739953108442 2.742163302343567 57.60133275373713 0.7773173709271572 
epoch: 55, train time every whole data:441.79s
epoch: 55, total time:19684.07s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 35.80s
test time on whole data:59.48s
1.5058965555384223 2.4467512062051395 55.90298162969988 0.8492899185103064 
1.5357719468058397 2.5130590987877928 56.563007787772705 0.840991316196915 
1.555762060749655 2.561369855200507 57.030540731380476 0.8349547345962036 
1.5705078789232565 2.582421778390241 57.15011571894271 0.833181652403415 
1.5938789868885208 2.6297597230441547 57.44578549304155 0.8285473311763156 
1.6197551683694833 2.686276380344681 57.91023195323072 0.8222776365889928 
1.6464812987085786 2.7475149950192512 58.448156643770844 0.8149935531933014 
1.6712988530907424 2.80538104194002 59.00777918046198 0.8062578634025173 
1.70248078810397 2.870620067600897 59.54762370289064 0.796423944933267 
1.728164880291071 2.9347315122144138 60.01821908438908 0.7863933851659044 
1.7490783989761203 2.971365439854241 60.531989780978165 0.7813070162543378 
1.7747449669473405 3.023095438231993 60.981769092075325 0.7740921522895126 
1.6378184819494168 2.7371445116777595 58.37827231263948 0.7740921522895126 
epoch: 56, train time every whole data:443.55s
epoch: 56, total time:20199.13s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.70s
test time on whole data:59.41s
1.5044575626898025 2.436393820476551 56.5406895408148 0.8490890670804532 
1.5334577172441142 2.5028637687372624 56.849491725466365 0.8412890265817976 
1.5549657477736474 2.5550301773424087 57.121886020997636 0.8352181251364774 
1.5727965919262774 2.5897363899664527 57.162468113363786 0.831834076696904 
1.5968836303876623 2.6372275050567797 57.37911525061955 0.8277145661331038 
1.6256343807501807 2.7024484585597848 57.84001342391342 0.8203125609054794 
1.6542888841902217 2.76727877114981 58.39404464288046 0.8124790545282555 
1.6803943597345303 2.8270154373165655 58.985073372800024 0.8031918281328442 
1.710312373437697 2.894035519049254 59.57867502970865 0.7924901471305498 
1.7331988910968814 2.948658652964435 60.13997958653851 0.7835081065179721 
1.751092831687647 2.977602988378695 60.75281705769324 0.7791469300932333 
1.7754262589069882 3.0244047990149627 61.355918963151645 0.7722711042744249 
1.6410757691521376 2.745093709945237 58.50843159876563 0.7722711042744249 
epoch: 57, train time every whole data:439.68s
epoch: 57, total time:20709.99s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.74s
test time on whole data:59.50s
1.506459020600255 2.450545964987852 55.31996042855336 0.8496761200929246 
1.538428808029475 2.5230018243994814 55.71884502747384 0.8416853291432757 
1.5627895788959272 2.582406153458888 56.06207948841221 0.8354816105052184 
1.582226015999736 2.6167657905289716 56.23201800744743 0.832833055858883 
1.6074153199416719 2.668521147702477 56.60042788192511 0.8283633350004597 
1.6346675565189548 2.7257187443864686 57.17382421569961 0.8221066516832033 
1.6613592136500492 2.7860679164868736 57.838038514884225 0.8145385923417361 
1.6852909867615395 2.8421777001768156 58.51047262683048 0.8052592650989647 
1.7141524571966202 2.902623765075556 59.10644150795355 0.795520086653027 
1.737530940182507 2.960410807014838 59.578184206966476 0.7861091308499726 
1.7575068536666887 2.998318896756105 60.08892653223155 0.7810870128019222 
1.7833901457759065 3.047904222169871 60.54659332606781 0.7751948143015676 
1.6476014081016108 2.7650585889253585 57.731415846070746 0.7751948143015676 
epoch: 58, train time every whole data:439.73s
epoch: 58, total time:21220.85s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.82s
test time on whole data:59.61s
1.50814276591715 2.4613277707635794 55.32006635537161 0.8490482184557278 
1.5377570715751498 2.524513663589488 55.90836717465023 0.8414342367685548 
1.5594668534249068 2.5748467900492 56.46896996067395 0.8353323673743823 
1.576315829544816 2.603942077720948 56.693009669267866 0.8326781004328733 
1.6002397464358558 2.6513107911068685 57.119586353067945 0.8281237816208528 
1.6276318090507496 2.7086645769854876 57.69727004663253 0.821808193536847 
1.6545601423874143 2.766145579099704 58.31381324248777 0.8150562585999211 
1.67883299548241 2.824477174524639 58.93283736426156 0.8057972207044928 
1.7090147296237272 2.8866440276791683 59.581759636232924 0.795925795350992 
1.7333768624035375 2.9442893939900383 60.172256118709136 0.7864975833770773 
1.7532630632011486 2.980311189801836 60.76886581208092 0.7812320229841855 
1.7788193020575813 3.0298876289607404 61.340862418999464 0.7742813741681003 
1.6431184309253706 2.752290363728287 58.19324599802981 0.7742813741681003 
epoch: 59, train time every whole data:438.91s
epoch: 59, total time:21732.14s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.76s
test time on whole data:59.45s
1.5035552589047168 2.4360531162941608 56.04636048250018 0.8497950454962175 
1.5338273186199367 2.510326381565744 56.194754752966745 0.8416427712398818 
1.5591715705776144 2.5724863720921554 56.460286257640966 0.8351449487264595 
1.5774600421470546 2.6044593703573637 56.530852442680214 0.8328002624673927 
1.602174968195813 2.6565024438238614 56.88249138598837 0.8279117821047323 
1.6284159545328114 2.7098856230226467 57.44357250657014 0.8220660861463787 
1.6560195134535787 2.7705075330592206 58.08636567234362 0.8146080263843194 
1.680266335762505 2.8287559107808855 58.72152369659528 0.8053408606846019 
1.7101071106524517 2.89237417451743 59.375114562141604 0.7950525146822615 
1.7331667663515324 2.945877047550086 59.959688285451506 0.786133489737387 
1.7515958502572029 2.9725044317208296 60.576753289514315 0.7823373177274703 
1.7775217766472626 3.023961504919399 61.18166524655012 0.7748689489781702 
1.6427735388418734 2.749836224150274 58.121713655819086 0.7748689489781702 
epoch: 60, train time every whole data:441.62s
epoch: 60, total time:22246.93s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.76s
test time on whole data:59.49s
1.506800433476766 2.4555189817170073 55.20404420192933 0.8494712807702313 
1.539531882207131 2.5313595928619277 55.758158902222846 0.8404480818306367 
1.5642996340006412 2.5907468713327693 56.313646982342 0.833523745275161 
1.5812833711746193 2.617656905126 56.52910643399951 0.8313324986264974 
1.6057263498846441 2.6655484868923423 57.01701084000648 0.8265649017575537 
1.6318782098980709 2.7185242113197217 57.659048160399905 0.8202599184958861 
1.6574893612776485 2.7757673753256014 58.30130819355384 0.8130217742447582 
1.6819052247892001 2.834647628659871 58.907869686943734 0.8036344648409255 
1.712719014126098 2.900818514001548 59.49477075336971 0.7932559885420034 
1.7367022709766669 2.956869880969356 59.98501562609927 0.7842688286195951 
1.7562382859439545 2.989609509412941 60.49501662369096 0.7795537958765724 
1.78026796841852 3.034845570525931 61.05238436541044 0.7730003067713328 
1.6462368338478301 2.7619885108104416 58.05988905396329 0.7730003067713328 
epoch: 61, train time every whole data:441.00s
epoch: 61, total time:22760.91s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.73s
test time on whole data:59.41s
1.506150896664975 2.4413832764588896 56.03852367863518 0.848943214015457 
1.537343560674272 2.5164746765769967 56.30066092374991 0.840209923389096 
1.5615295675362326 2.5744638647671056 56.55823317206765 0.833720868640145 
1.57811374551998 2.605179064824597 56.64431971608472 0.8309158265885916 
1.6008855760894005 2.649547347862856 57.001192699506355 0.8266803907271202 
1.6259001179145027 2.7013605902319493 57.531617821881 0.8208207000363883 
1.6520689703127636 2.756796330616407 58.118785219595125 0.8143494483707017 
1.6765081998428242 2.815730183623213 58.676369664697795 0.8051100987863752 
1.7061339018223363 2.881421846933498 59.25142708375461 0.7945507235001965 
1.7288173808332177 2.937557350456767 59.8131284927391 0.7851692897402356 
1.7477163004619736 2.969486897698399 60.400088178829826 0.780404306120681 
1.7716045410709367 3.015804437047174 60.954091959997726 0.7738915556648222 
1.6410643965619514 2.7446659754123517 58.10745920811778 0.7738915556648222 
epoch: 62, train time every whole data:440.34s
epoch: 62, total time:23274.78s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.76s
test time on whole data:59.51s
1.5065953106019823 2.438678751889982 55.966524781191026 0.8493394930579692 
1.5364672045193257 2.504363762389095 56.39856763313552 0.8413277202724418 
1.558238326083248 2.555924970612621 56.71828022937828 0.8351468727562031 
1.5735316020539474 2.5792430763389156 56.75948493886794 0.8332922497771114 
1.5960901133906806 2.626001128612853 57.06822684263694 0.8287809515331218 
1.621166382396239 2.6823788476879944 57.58147043726335 0.8223474541901031 
1.6464824504323659 2.736672912922213 58.15233299891055 0.816014283378779 
1.6705818031079003 2.798568207492877 58.74138624134944 0.8066150100958688 
1.7007656480316073 2.864178004280499 59.33938341414243 0.7967536053017263 
1.7255656424026404 2.924899853717814 59.88349418560015 0.7874429933609154 
1.7468453216135857 2.9615539551195194 60.4206477232544 0.7825450903820291 
1.7731465415572305 3.0141019925688695 60.92138497554998 0.7754595611541549 
1.6379563621825628 2.7299390638958876 58.16268755477672 0.7754595611541549 
epoch: 63, train time every whole data:439.15s
epoch: 63, total time:23785.16s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.72s
test time on whole data:59.40s
1.503243206107989 2.4405833028170427 55.69107721598384 0.8496959747158993 
1.5341163797717363 2.512810416246481 56.09054744913906 0.8413211212542984 
1.5582064320241056 2.57035620691325 56.437854180887726 0.8348846534095209 
1.5764431722108274 2.603602638311384 56.51339788182095 0.832276012796478 
1.6029496796740486 2.659180859727035 56.880038397864894 0.8270062027131806 
1.6314849007477363 2.7213937204539387 57.43738048353143 0.8200887751713877 
1.6597206311218795 2.7833193669518326 58.072338684114534 0.8125890252300682 
1.6854719736498027 2.842851488211185 58.74119630801419 0.8029000794274531 
1.7154818052603376 2.90731586293615 59.35844719296682 0.7923917021914882 
1.737731254993716 2.9605498280133644 59.88170612395768 0.7837642094140821 
1.7566648767722681 2.9920378924567927 60.43331222608924 0.7794609967905165 
1.7808618143209745 3.041757940392616 60.96376143550578 0.7724420569876302 
1.6451980105546185 2.7595887618211195 58.041851404561385 0.7724420569876302 
epoch: 64, train time every whole data:438.40s
epoch: 64, total time:24295.91s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.72s
test time on whole data:60.89s
1.503373315078428 2.44272765756077 56.04854175396805 0.849242929730454 
1.5341555237542128 2.5116594747438916 56.469173438243224 0.841162946649735 
1.560891276410293 2.576662759352686 56.722196028946094 0.834173161962914 
1.5804722602782504 2.6152545950863075 56.795122770369744 0.8309351964939113 
1.6064756494466925 2.667845147440159 57.16916287269876 0.8259617631213687 
1.633271403601748 2.723804871424165 57.72272371308043 0.8195844667389829 
1.6583357117482949 2.775831657526994 58.37199508882669 0.8128011053553482 
1.6814052198930156 2.83115593047966 59.08216609051239 0.8030898867370387 
1.7092287035818612 2.8892866363917262 59.76635774116609 0.7931899460913222 
1.7307997642897424 2.941527991411228 60.33708331668438 0.7843838400638876 
1.7488507449691673 2.9706843994098064 60.93378248757173 0.7801202106240596 
1.7714396900067195 3.013797703897277 61.51680166904428 0.7738185653839879 
1.643224938588202 2.7525715536108573 58.41135815308595 0.7738185653839879 
epoch: 65, train time every whole data:436.95s
epoch: 65, total time:24804.95s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.35s
test time on whole data:60.86s
1.5032713667846151 2.4448739928953023 55.58693078860969 0.8497669655202071 
1.5352301770058416 2.5174978587206627 56.07739827500308 0.8412462062223364 
1.5591528188871842 2.5730924680606324 56.454849897311256 0.8347064746495604 
1.5762574550588393 2.6017774191689407 56.5096761097137 0.8326276045505394 
1.6000722575286137 2.6502153766155123 56.82536999534806 0.8280547528705993 
1.625666690965671 2.705913859757096 57.31636835716352 0.8216592788488939 
1.6508952489603488 2.7612325497769126 57.890253691730834 0.8148345291864239 
1.6731381228990143 2.814232428137651 58.50148979563017 0.8058472046110001 
1.7005508083035017 2.8697886943946234 59.14475048839132 0.7967948980118397 
1.7233660177527261 2.921685261803233 59.712551609808905 0.7882384955061777 
1.7427995240204037 2.9594279143539053 60.31920233924335 0.7827048637627875 
1.7669225317248631 3.0045869338534166 60.88833937807142 0.7765789357157115 
1.6381102516576351 2.740937548740452 57.93569117724623 0.7765789357157115 
epoch: 66, train time every whole data:435.59s
epoch: 66, total time:25313.36s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.81s
test time on whole data:59.57s
1.5040670071374624 2.4428486642964597 55.93977803472663 0.8493905479399584 
1.5340991017081376 2.5093143814071377 56.249739010129815 0.8418021110622301 
1.5582489038864593 2.5682455447397623 56.558913341974105 0.8349938752645985 
1.5758785959951402 2.5966308984770756 56.64127979747072 0.8329679922819251 
1.599695300184545 2.6440186288768945 56.957390314875326 0.8289407686853986 
1.6263579388150857 2.703409333278179 57.44694686066523 0.8222345670046802 
1.65160195860586 2.7588697531855813 57.95731177937652 0.8154889671646656 
1.6726058808838327 2.810765982281436 58.45274996590547 0.8070624889596757 
1.699272794217492 2.8696413258870486 58.95579363141532 0.7979052174452094 
1.720274369820509 2.9192433706214613 59.40374474322787 0.7899457040921843 
1.739101190434148 2.9506775496780797 59.98437448023732 0.7855549804704844 
1.7641718817315108 3.0001444725914728 60.53086641929869 0.778911918263551 
1.637114576951682 2.736752299296272 57.923322307294626 0.778911918263551 
epoch: 67, train time every whole data:437.17s
epoch: 67, total time:25821.87s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 35.88s
test time on whole data:59.56s
1.5046872113323992 2.440808339524223 55.61695959120387 0.8499343292561958 
1.5362900166631277 2.51504472465228 55.95536658066445 0.8414727911263813 
1.5604778927725163 2.5721934701009217 56.32435876711492 0.8349825464366668 
1.57835160394962 2.603165960809594 56.42060541856888 0.8324748117784871 
1.6024614302099993 2.652040971415925 56.78073241545065 0.8279330457001318 
1.6280879728841995 2.7070918295074353 57.28536125188942 0.8217018890948398 
1.6529805859858613 2.762872095219836 57.83535452566354 0.8147198331431132 
1.6758558394383816 2.820633113393418 58.43610119269261 0.8052216997571491 
1.7037354811759045 2.8790221837799015 59.05961377090504 0.7958509487572968 
1.7262150896446158 2.9319586598278016 59.65773153151815 0.7871400245150576 
1.7460335007156467 2.965818437399486 60.30464974581484 0.7821669650027593 
1.7697044250858682 3.0087595086348458 60.91485695376085 0.7761759484433716 
1.640406754154845 2.7440938326532254 57.88273451621016 0.7761759484433716 
epoch: 68, train time every whole data:440.18s
epoch: 68, total time:26333.90s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.85s
test time on whole data:60.33s
1.5039739653397175 2.444405151396233 55.796341019997065 0.8487323977656971 
1.5352768136690416 2.514048503854892 56.14805708492408 0.8407910671518488 
1.5584997189110588 2.566733511101376 56.60612305455219 0.8344568131058899 
1.5744830470515327 2.5950243696981006 56.72835983598641 0.8319476839203476 
1.5985039760486355 2.6446148490703107 57.10442703559734 0.8270291770833182 
1.6249867850423214 2.7033413007267106 57.66567593920555 0.8203077384861844 
1.652273940311124 2.7602054176387285 58.28449732413934 0.8136731371593219 
1.6802660123932043 2.8260836638810645 58.944225888200684 0.8035947580596945 
1.7129814761521382 2.899607215414801 59.57776654514061 0.792200305998957 
1.73885404727741 2.9614105943915834 60.17188748400469 0.7822832277223244 
1.7600517898957644 2.9984068502836645 60.74534366096286 0.7768920901324832 
1.7829982655737548 3.039786933542645 61.28140227081318 0.7711136872482788 
1.6435958198054752 2.752783083908382 58.254609122406805 0.7711136872482788 
epoch: 69, train time every whole data:440.27s
epoch: 69, total time:26848.10s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 35.81s
test time on whole data:59.47s
1.508722019359558 2.4474923139749074 55.7211805442283 0.8500583754536859 
1.5396728559343056 2.513173012248318 56.23005637870258 0.8423386553020504 
1.5624067116807259 2.565620439984969 56.75342053153878 0.83575555835749 
1.5796300316280907 2.597446982836175 56.93962354388407 0.8327334056511557 
1.605657860165728 2.650697993400785 57.343650373530444 0.8274395246633837 
1.6337768902829184 2.712506255543159 57.87154984565327 0.8205451775223783 
1.6634510332067454 2.775386225788198 58.46249417124126 0.8133781667221983 
1.6923112945043082 2.842837894795953 59.08933080953233 0.8033170324042231 
1.7246156451616081 2.9132783681505505 59.66036339811858 0.7928701799272903 
1.7503585060215776 2.972963437799541 60.1801260670511 0.7835573450396333 
1.769230385755854 3.002018757624024 60.70798459443106 0.7797994655340557 
1.7917188366909644 3.0445184343432095 61.30472732821054 0.7737954937543461 
1.651796005866032 2.7599923595175704 58.35547546191398 0.7737954937543461 
epoch: 70, train time every whole data:439.65s
epoch: 70, total time:27360.46s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.80s
test time on whole data:59.76s
1.5064195038314376 2.4469888447582795 55.68573138495343 0.8489505062579998 
1.5375057125282252 2.516050339425781 55.916764265573526 0.8415571423146205 
1.5628806255117413 2.5791073013945542 56.23729991044944 0.833963024058227 
1.5785901776287883 2.6058947479608934 56.321220103137414 0.8317860264054995 
1.602436620221695 2.655633881563258 56.759736375359836 0.8266670378220022 
1.6274426844255732 2.709146188569726 57.313596203471384 0.8204430544954038 
1.6535486096441745 2.7664140143157177 57.97964259624224 0.8131163701959164 
1.6799588955502425 2.826120313286118 58.686998710738855 0.8032218194358521 
1.7106415075084993 2.8910675744286403 59.33733726427446 0.792761889895067 
1.73516643070944 2.9496339541117833 59.9297528968712 0.7829684683534445 
1.7553358642051795 2.9855949976423224 60.50660423215356 0.7776086074999377 
1.7777910027338104 3.0284529643566325 61.093038717341095 0.7714395926421488 
1.6439764695415673 2.752824754753463 57.98074420892269 0.7714395926421488 
epoch: 71, train time every whole data:439.78s
epoch: 71, total time:27872.18s
predicting testing set batch 1 / 168, time: 0.36s
predicting testing set batch 101 / 168, time: 36.26s
test time on whole data:60.30s
1.5039958846400536 2.454935045956796 55.228388992318536 0.8491371150235704 
1.536111568646328 2.5255184813495006 55.62976668214424 0.8412546910040903 
1.561069176954439 2.585255497037201 56.07634737169319 0.8345477003471492 
1.5790084695110895 2.6167404279883617 56.26140365392636 0.8321243205130029 
1.6047712997986625 2.66864746775698 56.773734168010925 0.8266819591311115 
1.6304784110698494 2.7240644235507943 57.381967576835166 0.8199013512365022 
1.6550719800291671 2.7749849104237128 57.9940732209264 0.8134379435592308 
1.6791203785699216 2.83305866097385 58.59063342774067 0.8038984175399067 
1.7073958207159525 2.8932847958519328 59.17514884577971 0.7943324420082162 
1.7299285601180578 2.944264705884114 59.72942269379132 0.786058960627396 
1.749006612853014 2.976214151641171 60.32730340857121 0.7817325917490563 
1.7719000821873723 3.0208260746714335 60.9125647613425 0.7754165597064884 
1.6423215204244923 2.757230305191855 57.84016689076461 0.7754165597064884 
epoch: 72, train time every whole data:440.44s
epoch: 72, total time:28386.56s
predicting testing set batch 1 / 168, time: 0.37s
predicting testing set batch 101 / 168, time: 35.98s
test time on whole data:59.68s
1.5039815110518109 2.4365940403368826 55.81232515188037 0.8499450601483165 
1.5363077078904246 2.510398626084485 56.104093084262985 0.8416874850987686 
1.5609180987118079 2.5711889442989686 56.44977263670216 0.8344896102930438 
1.5785776218157261 2.6039596856427787 56.5278069643122 0.8316707849616511 
1.6035235647323232 2.654628128271705 56.84452395649079 0.8267175559857418 
1.6299887266770183 2.7087801465597554 57.34260424611451 0.820288544492013 
1.6557643666016382 2.7632826108498643 57.90311002309717 0.8133068477899774 
1.6812305020608362 2.8238150960220074 58.50371034431126 0.8033823828378055 
1.7112931091880337 2.887361756506284 59.05482820207737 0.7934130573358789 
1.7360625123389597 2.9440156209194828 59.55808207856298 0.7846714413121681 
1.7577321552258163 2.982792554878082 60.04150519298048 0.7796684019110004 
1.7832981639060945 3.036914957905964 60.573987300453204 0.7721423722049809 
1.6448898366833742 2.7499833795389717 57.89311605927415 0.7721423722049809 
epoch: 73, train time every whole data:439.68s
epoch: 73, total time:28896.98s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 36.33s
test time on whole data:60.07s
1.5054720087318372 2.451636735520471 55.423443871802206 0.8496569715740052 
1.5380752617160656 2.527291056777147 55.754623764102185 0.8414714889113332 
1.563664742604519 2.5873915990798326 56.166297891474024 0.8345919219513135 
1.581877938569479 2.622599167036149 56.33753700084031 0.8314536290350686 
1.608835739330078 2.6790739841697073 56.85553377182513 0.8257076446070667 
1.6374820643068246 2.7413171829632863 57.485616102588445 0.8183797401744409 
1.6640339320045674 2.7954529485036415 58.17310651339107 0.811483042354638 
1.6886388151688234 2.858231820901481 58.86154924570253 0.8004347446744541 
1.716743272616661 2.9187609293140957 59.50002557985299 0.7902032205008885 
1.7382419571115502 2.963779675818153 60.03994345962212 0.7826297339948308 
1.756032438250702 2.991505791321249 60.594659101891345 0.7787064315235572 
1.777691916221132 3.0345596508301873 61.109285666816895 0.7724695319285095 
1.6480658405526867 2.7704912709566134 58.02524300062187 0.7724695319285095 
epoch: 74, train time every whole data:439.86s
epoch: 74, total time:29407.28s
predicting testing set batch 1 / 168, time: 0.40s
predicting testing set batch 101 / 168, time: 37.41s
test time on whole data:62.08s
1.503769993417852 2.4489251044863796 55.98045029955555 0.8489373925422875 
1.5352247205523863 2.518877482035377 56.40505468109554 0.840882345157442 
1.5594925327820792 2.577665165047888 56.765677183930876 0.8338226973179441 
1.5757832106541665 2.6050499283814266 56.93158762169717 0.8313972893392915 
1.59976353575715 2.651124270116541 57.35049053536307 0.8267903448167754 
1.6262244689005116 2.7108016808308055 57.89109161778375 0.8194156773023256 
1.6512169244098698 2.764121206641155 58.44480981261074 0.8124730941356858 
1.6738971990945617 2.819900552698363 59.00235076993473 0.8029215977506731 
1.6992873841588874 2.8778925948918435 59.48414107889243 0.7932627730120513 
1.717239275572112 2.913873593526863 59.9316075899899 0.7874491391289616 
1.7307355462252385 2.9330456341377578 60.37996742833685 0.7849062489709829 
1.7496903730610474 2.9705290385100147 60.85527487370014 0.7795910013974234 
1.6351937637154885 2.7377219264260897 58.28529893792405 0.7795910013974234 
epoch: 75, train time every whole data:438.84s
epoch: 75, total time:29921.04s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.75s
test time on whole data:59.52s
1.5028033456612555 2.443718300087465 55.42287867875568 0.8495532090547084 
1.5356379392418478 2.5204970292666027 55.65907649665941 0.8409450640316857 
1.558992459268531 2.575930085448562 55.97870691247573 0.8345640483636069 
1.574651849244322 2.6024885592364755 56.02056998796506 0.832824686855266 
1.5962099803171697 2.6458897619998747 56.36439967051595 0.8289031636414961 
1.6201379941703662 2.698661176581285 56.89189668874316 0.8227925551594992 
1.6435578875296881 2.747318852691008 57.51213213568243 0.8168873397927301 
1.6665826645002124 2.801961879807309 58.18209618322455 0.8080658274212366 
1.6955658098136386 2.8637539203762756 58.854576773298525 0.7985099756052504 
1.7216551314266841 2.923860599404682 59.52828925014727 0.7890728849359926 
1.7457479579358228 2.9685070009119774 60.205924968200144 0.783036224478347 
1.774291172766171 3.025911261096661 60.915100713465854 0.7749727924012126 
1.6363195159896424 2.740643218896556 57.628067823758556 0.7749727924012126 
epoch: 76, train time every whole data:439.39s
epoch: 76, total time:30431.99s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.74s
test time on whole data:59.52s
1.5076787315494424 2.4451446927020033 55.489809762393435 0.849197554546378 
1.537775971641764 2.511978766246789 55.96376249924312 0.8411105259501042 
1.5606615433205984 2.5649217133882942 56.53955795332137 0.8344936641571893 
1.5765892607206036 2.5933443033896983 56.67346420525347 0.8320448983961347 
1.6009813277735596 2.6430577393721517 57.09799673875763 0.8269424789465403 
1.6270019704495513 2.698856374119879 57.612935825658184 0.8204148689259677 
1.6526174995713823 2.75249071640481 58.18849300584483 0.8139183499230885 
1.6777317705134907 2.810830654633366 58.80761858051366 0.8043654957099573 
1.7057378830158463 2.8717708714147485 59.40256663363345 0.7943908264038503 
1.7275793503153891 2.925911142865004 59.9318267505337 0.7854132119214792 
1.7457595590415986 2.9568114542019086 60.39512415159676 0.7811369838818532 
1.7691023776848755 3.0040639843768955 60.87439061641773 0.7745603452966643 
1.6407681037998418 2.737315296181944 58.081560683476866 0.7745603452966643 
epoch: 77, train time every whole data:440.09s
epoch: 77, total time:30944.60s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.70s
test time on whole data:59.41s
1.5058500840671005 2.449887592017188 55.9758511538398 0.8487398802622138 
1.5364388649478733 2.5157282336972733 56.367241756985955 0.8409715768964746 
1.5601420149684306 2.5708426241229168 56.77336533614648 0.8346103965280163 
1.5773848323511581 2.604992392958957 56.81194192237686 0.8317439252564135 
1.6026003629577656 2.6554686993648153 57.168954370879554 0.8268489325544259 
1.6306124706853713 2.7158316226527592 57.65767865353257 0.8198197354124209 
1.6567633344734176 2.768119051489273 58.21180336688134 0.8136544349561378 
1.682634151857701 2.8282364247975416 58.857972231318136 0.8037371484789568 
1.7109018855549927 2.8903608966831253 59.46670840327071 0.7935866940738439 
1.7340378465068838 2.9416777703650485 60.064316839290285 0.7851197514688084 
1.7523034633409587 2.97340490896068 60.57366187970179 0.7805197332829324 
1.7745481416503233 3.014010613033356 61.10927985091013 0.7748355217379587 
1.6436847877801648 2.749966145916511 58.25332361139909 0.7748355217379587 
epoch: 78, train time every whole data:436.68s
epoch: 78, total time:31452.09s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.65s
test time on whole data:59.50s
1.5036221530308858 2.4489644799949324 55.34312915636595 0.8494109794333017 
1.5368878216917317 2.5243014592662316 55.654239819032604 0.8408962445476591 
1.5628138341390128 2.586794301360797 56.11471780785601 0.8339310396468532 
1.5807589511203446 2.6201742239310577 56.26775459583623 0.8312596410555942 
1.606664392573404 2.6703239393779885 56.76462245432001 0.8263572692649975 
1.633876037858604 2.7302356375596455 57.35020505629079 0.8194294048305393 
1.6602046830513115 2.7828563513100546 57.988224586209924 0.8132278850327066 
1.6861764109949804 2.8440272877829473 58.6596055282327 0.8033359302235664 
1.7162050971201666 2.9092779436560288 59.332689945825415 0.792803390760221 
1.740853340570327 2.9638922081325774 59.944584411059175 0.7838765163590866 
1.7612698480305926 3.0023566299369593 60.546398748693676 0.7782229930624811 
1.785644750044548 3.0464230337025793 61.136270262771966 0.7721105653817458 
1.6479147766854925 2.7671786726255596 57.92531036096066 0.7721105653817458 
epoch: 79, train time every whole data:437.81s
epoch: 79, total time:31962.82s
predicting testing set batch 1 / 168, time: 0.35s
predicting testing set batch 101 / 168, time: 35.82s
test time on whole data:60.58s
1.5085663660330964 2.4516349812752964 55.514355271588926 0.8483577143257757 
1.542289792609268 2.5306866204092273 55.69790005429852 0.8399194677878431 
1.5663337703586688 2.5864315998992007 56.069537531742995 0.8337252661630833 
1.5832916474661656 2.6178862301983385 56.16778759662301 0.8313266473501608 
1.6083036686823304 2.666694793229125 56.58224174732701 0.8268088432886646 
1.6353215366180631 2.7264168041888994 57.095664092217966 0.8197773815432713 
1.661114483654765 2.779090603979822 57.67723537284073 0.8133967025826491 
1.6861554883023102 2.839433819671097 58.31163881651745 0.803352955873246 
1.714144808030643 2.9012354146737906 58.87715974751352 0.7931739749905452 
1.735610374653446 2.949470281860176 59.40476064010154 0.7851789869202501 
1.7537698878888042 2.982761587321282 59.9376475336854 0.7802431860056522 
1.7751871443655165 3.0249366121627452 60.48416053357324 0.774115401146161 
1.6475074140552564 2.7606133733964793 57.651766590170226 0.774115401146161 
save parameters to file: experiments/bike/MAE_ASTGNN_h1d0w0_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TEallattention1_spt_trend_adj/epoch_79.params
